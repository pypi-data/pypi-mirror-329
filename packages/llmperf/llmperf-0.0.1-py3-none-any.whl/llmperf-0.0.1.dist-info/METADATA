Metadata-Version: 2.2
Name: llmperf
Version: 0.0.1
Summary: A CLI tool for LLM performance testing
Home-page: https://github.com/theodoreniu/llmperf
Author: Theodoree
Author-email: theodore.niu@gmail.com
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: click
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# LLM Perf

`llmperf` is a benchmarking tool for stress testing self-hosted LLMs, Azure OpenAI, and OpenAI APIs. It provides efficient concurrent request management, detailed performance analysis, and helps developers optimize inference latency and throughput. With customizable test parameters, it is suitable for various LLM workload evaluation scenarios.

## 1. Config your env

Copy and set up the environment variables from `.env.example`.

```bash
cp .env.example .env
```

## 2. Launch your App

Copy and set up the environment variables.

```bash
./start.sh
```
