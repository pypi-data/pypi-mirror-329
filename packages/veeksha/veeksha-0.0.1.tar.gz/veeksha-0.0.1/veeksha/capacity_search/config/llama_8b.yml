servers:
  - openai_server_engine: vllm
    openai_api_url: "http://localhost:8000/v1"
    openai_api_key: token-abc123
  - openai_server_engine: sarathi
    openai_api_url: "http://localhost:8001/v1"
    openai_api_key: token-abc123

models:
  - name: llama-3-8b-instruct
    identifier: meta-llama/Meta-Llama-3-8B-Instruct

request_generator_configs:
  - start_qps: 1
    request_interval_generator_provider: "poisson"
    request_length_generator_provider: "trace"
    request_generator_max_tokens: 8192
    trace_request_length_generator_trace_file: "data/processed_traces/sharegpt_8k_filtered_stats_llama2_tokenizer.csv"
    trace_file_name: "sharegpt"
  - start_qps: 1
    request_interval_generator_provider: "poisson"
    request_length_generator_provider: "trace"
    request_generator_max_tokens: 16384
    trace_request_length_generator_trace_file: "data/processed_traces/arxiv_summarization_filtered_stats_llama2_tokenizer.csv"
    trace_file_name: "arxiv"

client_configs:
  - num_clients: 2
    num_concurrent_requests_per_client: 5
    timeout: 1200
    max_num_completed_requests: 20
    additional_sampling_params: {}
    llm_api: "openai"
