{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a **comprehensive set of rules** for the `MarkdownChunkingStrategy` class, meticulously designed to handle the intricacies of Markdown document structures. These rules encompass **chunk size management**, **content type preservation**, **structural integrity**, **duplicate prevention**, and **automatic detection of headers and footers**. Additionally, they incorporate your specific requirements regarding **hard length limits**, **table splitting with full headers**, and **automatic identification of repeating headers and footers**.\n",
    "\n",
    "# Rules\n",
    "---\n",
    "\n",
    "## **1. Chunk Size Management**\n",
    "\n",
    "### **1.1. Minimum Length (`min_chunk_len`)**\n",
    "- **Definition:** The minimum number of characters a chunk must contain.\n",
    "- **Default:** `512`\n",
    "- **Rule:** \n",
    "  - Ensure each chunk is at least `min_chunk_len` characters long.\n",
    "  - Merge smaller chunks with adjacent ones to meet this requirement.\n",
    "\n",
    "### **1.2. Soft Maximum Length (`soft_max_len`)**\n",
    "- **Definition:** The preferred upper limit for chunk size.\n",
    "- **Default:** `1024`\n",
    "- **Rule:** \n",
    "  - Aim to keep chunks below `soft_max_len`.\n",
    "  - Allow slight exceedances if necessary to preserve structural elements.\n",
    "\n",
    "### **1.3. Hard Maximum Length (`hard_max_len`)**\n",
    "- **Definition:** The absolute maximum number of characters a chunk can contain.\n",
    "- **Default:** `2048`\n",
    "- **Rule:** \n",
    "  - Strictly enforce that no chunk exceeds `hard_max_len`.\n",
    "  - If a chunk exceeds this limit, split it at logical boundaries (e.g., sentence endings, newlines) regardless of content type.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Content Type Preservation**\n",
    "\n",
    "### **2.1. Headings**\n",
    "- **Identification:** Detect headings using various Markdown syntaxes (e.g., `#`, `##`, numbered headings).\n",
    "- **Rule:** \n",
    "  - Preserve all heading levels and formats.\n",
    "  - When splitting, ensure that headings remain intact and appropriately formatted within chunks.\n",
    "\n",
    "### **2.2. Tables**\n",
    "- **Identification:** Detect Markdown tables by the presence of pipe characters (`|`) and separator lines (e.g., `---`).\n",
    "- **Rules:** \n",
    "  - **No Split Rule:** Do not split tables across chunks unless they exceed `hard_max_len`.\n",
    "  - **Splitting with Full Headers:** If a table must be split due to the hard length limit, ensure that each resulting chunk includes the full table header to maintain clarity and structure.\n",
    "\n",
    "### **2.3. Footnotes and Citations**\n",
    "- **Identification:** Detect footnote definitions (e.g., `[^1]: Footnote text`) and citations (e.g., `[@citation]`).\n",
    "- **Rules:** \n",
    "  - Keep footnote markers and their corresponding definitions within the same chunk.\n",
    "  - Avoid splitting footnote definitions across multiple chunks.\n",
    "\n",
    "### **2.4. Images and Links**\n",
    "- **Identification:** Detect Markdown image syntax (`![Alt Text](url)`) and links (`[Text](url)`).\n",
    "- **Rules:** \n",
    "  - Ensure that images and links are not split across chunks.\n",
    "  - Maintain the integrity of image and link syntax within chunks.\n",
    "\n",
    "### **2.5. Code Blocks and Inline Code**\n",
    "- **Identification:** Detect fenced code blocks (e.g., ```` ```python ````) and inline code (e.g., `` `code` ``).\n",
    "- **Rules:** \n",
    "  - **Code Blocks:** Never split within a fenced code block. Include both opening and closing fences within the same chunk.\n",
    "  - **Inline Code:** Avoid splitting inline code snippets across chunks. Ensure that both backticks are within the same chunk.\n",
    "\n",
    "### **2.6. Lists**\n",
    "- **Identification:** Detect ordered (`1.`, `2.`, ...) and unordered lists (`-`, `*`, `+`).\n",
    "- **Rules:** \n",
    "  - Do not split lists across chunks.\n",
    "  - Preserve list hierarchies and indentation levels.\n",
    "  - If a list exceeds `hard_max_len`, consider splitting between major sections or logical breaks, ensuring individual list items remain intact.\n",
    "\n",
    "### **2.7. Blockquotes**\n",
    "- **Identification:** Detect blockquotes using the `>` symbol.\n",
    "- **Rules:** \n",
    "  - Keep entire blockquotes within the same chunk.\n",
    "  - Preserve nesting and indentation within blockquotes.\n",
    "\n",
    "### **2.8. Embedded HTML**\n",
    "- **Identification:** Detect embedded HTML tags within the Markdown.\n",
    "- **Rules:** \n",
    "  - Do not split embedded HTML elements across chunks.\n",
    "  - Ensure that opening and closing HTML tags are contained within the same chunk.\n",
    "\n",
    "### **2.9. Tables of Contents (TOC)**\n",
    "- **Identification:** Detect TOCs, typically represented as nested lists with links to headings.\n",
    "- **Rules:** \n",
    "  - Optionally exclude TOCs from chunking or place them in a separate chunk.\n",
    "  - Ensure that TOC links correspond to headings within the same or subsequent chunks.\n",
    "\n",
    "### **2.10. YAML Front Matter**\n",
    "- **Identification:** Detect YAML front matter enclosed within `---` at the beginning of the document.\n",
    "- **Rules:** \n",
    "  - Keep YAML front matter intact within a single chunk, preferably the first chunk.\n",
    "  - Do not split YAML front matter across multiple chunks.\n",
    "\n",
    "### **2.11. Emphasis and Formatting**\n",
    "- **Identification:** Detect bold (`**bold**`), italics (`*italics*`), strikethrough (`~~text~~`), and other formatting.\n",
    "- **Rules:** \n",
    "  - Avoid splitting emphasis markers across chunks.\n",
    "  - Ensure that formatting syntax remains intact within chunks.\n",
    "\n",
    "### **2.12. Horizontal Rules and Page Breaks**\n",
    "- **Identification:** Detect horizontal rules (`---`, `***`, `___`) and page breaks.\n",
    "- **Rules:** \n",
    "  - Preserve horizontal rules and page breaks within the same chunk.\n",
    "  - Use them as natural split points for chunking.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Logical Split Points**\n",
    "\n",
    "### **3.1. Natural Language Boundaries**\n",
    "- **Rule:** \n",
    "  - Prefer splitting at sentence endings (e.g., `.` followed by a space) or newline characters (`\\n`) to maintain readability and coherence.\n",
    "\n",
    "### **3.2. Exclusion of Key Elements**\n",
    "- **Rule:** \n",
    "  - Do not split within structural elements such as tables, code blocks, lists, blockquotes, images, links, and embedded HTML to preserve their functionality and appearance.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Markdown Integrity**\n",
    "\n",
    "### **4.1. Consistent Formatting**\n",
    "- **Rule:** \n",
    "  - Ensure that all Markdown syntax within each chunk is correctly formatted.\n",
    "  - Maintain proper heading levels, list structures, and other formatting conventions.\n",
    "\n",
    "### **4.2. Content Type Classification**\n",
    "- **Rule:** \n",
    "  - Classify blocks of text (e.g., paragraphs, tables, headings) to apply appropriate formatting and prevent structural issues post-chunking.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Duplicate Prevention**\n",
    "\n",
    "### **5.1. Exact Duplicates**\n",
    "- **Rule:** \n",
    "  - Utilize hashing (e.g., MD5) to identify and exclude exact duplicate content blocks, ensuring each chunk contains unique information.\n",
    "\n",
    "### **5.2. Fuzzy Duplicates**\n",
    "- **Rule:** \n",
    "  - (Optional) Implement fuzzy duplicate detection using similarity metrics to identify and manage near-duplicate content.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Header and Footer Management**\n",
    "\n",
    "### **6.1. Automatic Identification of Headers and Footers**\n",
    "- **Rule:** \n",
    "  - Automatically detect and remove repeating headers and footers without relying on predefined templates.\n",
    "  - Use heuristics such as consistent phrases, patterns (e.g., page numbers), and positional information (e.g., top and bottom lines) to identify headers and footers.\n",
    "\n",
    "### **6.2. Removal of Identified Headers and Footers**\n",
    "- **Rule:** \n",
    "  - Exclude detected headers and footers from chunk content to avoid redundancy and irrelevant information within chunks.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Table Splitting with Full Headers**\n",
    "\n",
    "### **7.1. Handling Oversized Tables**\n",
    "- **Rule:** \n",
    "  - If a table exceeds `hard_max_len`, split it into smaller tables.\n",
    "  - Ensure that each split table includes the full header row to maintain context and readability.\n",
    "\n",
    "### **7.2. Preserving Table Integrity**\n",
    "- **Rule:** \n",
    "  - Maintain proper Markdown table syntax within each split to ensure tables render correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Additional Enhancements**\n",
    "\n",
    "### **8.1. Embedded Media Handling**\n",
    "- **Rule:** \n",
    "  - Place large media embeds (e.g., videos, high-resolution images) in separate chunks to prevent inflating the size of other content.\n",
    "\n",
    "### **8.2. Cross-Chunk References**\n",
    "- **Rule:** \n",
    "  - Maintain functional references between chunks (e.g., links pointing to sections in other chunks).\n",
    "  - Include identifiers or anchors in links that correspond to target chunks.\n",
    "\n",
    "### **8.3. Metadata Preservation**\n",
    "- **Rule:** \n",
    "  - Retain essential metadata (e.g., titles, authors, dates) within chunks or alongside them for processing and rendering purposes.\n",
    "\n",
    "### **8.4. Accessibility Considerations**\n",
    "- **Rule:** \n",
    "  - Preserve accessibility features such as alt text for images, proper heading hierarchies, and descriptive link texts within chunks.\n",
    "\n",
    "### **8.5. Handling Custom Markdown Extensions**\n",
    "- **Rule:** \n",
    "  - Detect and preserve syntax introduced by Markdown extensions or plugins (e.g., task lists, diagrams, LaTeX equations).\n",
    "  - Ensure that extended syntax is not split across chunks to maintain functionality.\n",
    "\n",
    "### **8.6. Consistent Line Endings and Whitespace**\n",
    "- **Rule:** \n",
    "  - Normalize line endings and manage whitespace to ensure consistent formatting across chunks.\n",
    "  - Remove excessive trailing spaces or blank lines that could disrupt Markdown structure.\n",
    "\n",
    "### **8.7. Error Detection and Recovery**\n",
    "- **Rule:** \n",
    "  - Implement syntax validation within each chunk to identify and rectify potential Markdown errors.\n",
    "  - Provide fallback mechanisms to handle or skip malformed sections without halting the entire chunking process.\n",
    "\n",
    "### **8.8. Concurrency and Performance Optimization**\n",
    "- **Rule:** \n",
    "  - Utilize parallel processing (e.g., `ProcessPoolExecutor`) to handle multiple pages or large documents efficiently.\n",
    "  - Ensure that the chunking process scales effectively with document size.\n",
    "\n",
    "### **8.9. Testing and Validation**\n",
    "- **Rule:** \n",
    "  - Develop comprehensive test cases covering various Markdown structures and edge cases.\n",
    "  - Regularly validate that chunks render correctly across different Markdown parsers and platforms.\n",
    "\n",
    "### **8.10. Customizable Chunking Rules**\n",
    "- **Rule:** \n",
    "  - Allow users to define or adjust chunking behavior based on specific requirements or preferences.\n",
    "  - Enable rule extensions to accommodate additional or customized chunking strategies.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Summary of Comprehensive Rules**\n",
    "\n",
    "### **9.1. Chunk Size Constraints**\n",
    "- **Always** adhere to `hard_max_len`, never exceeding it under any circumstances.\n",
    "- **Aim** to keep chunks between `min_chunk_len` and `soft_max_len`.\n",
    "- **Merge** smaller chunks to meet `min_chunk_len`.\n",
    "- **Split** oversized chunks at logical boundaries, ensuring no chunk exceeds `hard_max_len`.\n",
    "\n",
    "### **9.2. Structural Integrity**\n",
    "- **Preserve** all Markdown structural elements (headings, tables, lists, code blocks, etc.) within chunks.\n",
    "- **Never split** critical elements unless absolutely necessary (as per `hard_max_len`).\n",
    "- **Ensure** that any split tables include full headers for clarity.\n",
    "\n",
    "### **9.3. Automatic Header and Footer Detection**\n",
    "- **Identify** repeating headers and footers automatically using heuristics (patterns, consistent phrases, positions).\n",
    "- **Remove** detected headers and footers from chunk content to avoid redundancy.\n",
    "\n",
    "### **9.4. Markdown Syntax Preservation**\n",
    "- **Maintain** proper Markdown formatting within each chunk.\n",
    "- **Avoid breaking** Markdown syntax across chunks to ensure correct rendering.\n",
    "\n",
    "### **9.5. Duplicate Content Management**\n",
    "- **Detect and exclude** exact duplicates using hashing techniques.\n",
    "- **Optionally** implement fuzzy duplicate detection for near-duplicate content.\n",
    "\n",
    "### **9.6. Metadata and References**\n",
    "- **Preserve** essential metadata within or alongside chunks.\n",
    "- **Maintain** functional cross-chunk references and links.\n",
    "\n",
    "### **9.7. Accessibility and Usability**\n",
    "- **Ensure** accessibility features are intact within chunks.\n",
    "- **Preserve** logical heading hierarchies to support assistive technologies.\n",
    "\n",
    "### **9.8. Performance and Scalability**\n",
    "- **Optimize** chunking processes for efficiency, especially for large documents.\n",
    "- **Leverage** concurrency to enhance performance.\n",
    "\n",
    "### **9.9. Customization and Flexibility**\n",
    "- **Allow** users to customize chunking behavior based on specific needs.\n",
    "- **Support** multiple Markdown flavors and custom extensions.\n",
    "\n",
    "### **9.10. Robust Error Handling**\n",
    "- **Implement** mechanisms to detect and recover from malformed Markdown.\n",
    "- **Log** errors with sufficient context for troubleshooting.\n",
    "\n",
    "---\n",
    "\n",
    "## **Implementation Considerations**\n",
    "\n",
    "To effectively implement these comprehensive rules within the `MarkdownChunkingStrategy` class, consider the following strategies:\n",
    "\n",
    "1. **Enhanced Pattern Recognition:**\n",
    "   - Utilize advanced regular expressions and heuristics to automatically detect headers, footers, and other structural elements without relying on predefined templates.\n",
    "\n",
    "2. **Context-Aware Splitting:**\n",
    "   - Develop logic that is aware of the current parsing context (e.g., inside a table, code block) to prevent inappropriate splits.\n",
    "\n",
    "3. **Dynamic Header Insertion:**\n",
    "   - When splitting tables, dynamically insert header rows into each new table chunk to maintain clarity and structure.\n",
    "\n",
    "4. **Concurrency Optimization:**\n",
    "   - Implement parallel processing judiciously, ensuring thread safety and efficient resource utilization.\n",
    "\n",
    "5. **Comprehensive Testing:**\n",
    "   - Create extensive test suites that cover a wide range of Markdown features and edge cases to validate the robustness of the chunking strategy.\n",
    "\n",
    "6. **Configurability:**\n",
    "   - Design the class to accept configurable parameters and extension points, allowing users to tailor the chunking behavior to their specific needs.\n",
    "\n",
    "7. **Logging and Reporting:**\n",
    "   - Incorporate detailed logging to track the chunking process, identify issues, and provide insights for debugging and optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By integrating these comprehensive rules, the `MarkdownChunkingStrategy` class will robustly handle the complexities of Markdown document structures. It ensures that **chunk size constraints** are strictly respected, **structural integrity** is maintained, **headers and footers** are automatically identified and managed, and **Markdown syntax** remains unbroken across chunks. Additionally, the strategy emphasizes **performance optimization**, **accessibility**, and **flexibility**, making it well-suited for a wide range of applications involving Markdown processing.\n",
    "\n",
    "Implementing these rules will result in well-organized, coherent, and structurally sound chunks, facilitating efficient processing, rendering, and analysis of Markdown documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kph/kph/document_processing/chunking/chunker.py\n",
    "\n",
    "import re\n",
    "import hashlib\n",
    "from typing import List, Set, Tuple, Optional, Generator\n",
    "from abc import ABC, abstractmethod\n",
    "from unstructured.documents.elements import Element\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import logging\n",
    "\n",
    "\n",
    "class ChunkingStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def chunk(self, items: List[Element]) -> List[Tuple[int, List[int], str]]:\n",
    "        \"\"\"\n",
    "        Splits a list of elements into chunks and returns a list of tuples.\n",
    "        Args:\n",
    "            items (List[Element]): The list of elements to be chunked.\n",
    "        Returns:\n",
    "            List[Tuple[int, List[int], str]]: A list of tuples where each tuple contains:\n",
    "            - An integer representing the chunk number.\n",
    "            - A list of integers representing the page numbers of the chunk.\n",
    "            - A string representing the content of the chunk.\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "\n",
    "class DocumentChunker:\n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        strategy_name = self.config.chunking.strategy\n",
    "        if strategy_name == \"by_page\":\n",
    "            self.strategy = PageChunkingStrategy()\n",
    "            logger.info(f\"Using {strategy_name} chunking strategy\")\n",
    "        elif strategy_name == \"by_character\":\n",
    "            max_len = self.config.chunking.by_character.max_len\n",
    "            overlap = self.config.chunking.by_character.overlap\n",
    "            self.strategy = CharachterChunkingStrategy(max_len, overlap)\n",
    "            logger.info(f\"Using {strategy_name} chunking strategy\")\n",
    "        elif strategy_name == \"by_title\":\n",
    "            self.strategy = TitleChunkingStrategy(\n",
    "                combine_text_under_n_chars=self.config.chunking.by_title.combine_text_under_n_chars,\n",
    "                include_orig_elements=self.config.chunking.by_title.include_orig_elements,\n",
    "                max_characters=self.config.chunking.by_title.max_characters,\n",
    "                multipage_sections=self.config.chunking.by_title.multipage_sections,\n",
    "                new_after_n_chars=self.config.chunking.by_title.new_after_n_chars,\n",
    "                overlap=self.config.chunking.by_title.overlap,\n",
    "                overlap_all=self.config.chunking.by_title.overlap_all,\n",
    "            )\n",
    "            logger.info(f\"Using {strategy_name} chunking strategy\")\n",
    "        elif strategy_name == \"markdown\":\n",
    "            self.strategy = MarkdownChunkingStrategy(\n",
    "                min_chunk_len=self.config.chunking.markdown.min_chunk_len,\n",
    "                soft_max_len=self.config.chunking.markdown.soft_max_len,\n",
    "                hard_max_len=self.config.chunking.markdown.hard_max_len,\n",
    "            )\n",
    "            logger.info(f\"Using {strategy_name} chunking strategy\")\n",
    "        elif strategy_name == \"custom\":\n",
    "            self.strategy = CustomChunkingStrategy()\n",
    "            logger.info(f\"Using {strategy_name} chunking strategy\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown chunking strategy: {strategy_name}\")\n",
    "\n",
    "    def chunk(self, items: List[Element]) -> List[Tuple[int, List[int], str]]:\n",
    "        return self.strategy.chunk(items)\n",
    "\n",
    "\n",
    "class TitleChunkingStrategy(ChunkingStrategy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        combine_text_under_n_chars: Optional[int] = None,\n",
    "        include_orig_elements: Optional[\n",
    "            bool\n",
    "        ] = True,  # Default to True for accurate page tracking\n",
    "        max_characters: Optional[int] = None,\n",
    "        multipage_sections: Optional[bool] = True,  # Default to True\n",
    "        new_after_n_chars: Optional[int] = None,\n",
    "        overlap: Optional[int] = None,\n",
    "        overlap_all: Optional[bool] = None,\n",
    "    ):\n",
    "        self.combine_text_under_n_chars = combine_text_under_n_chars\n",
    "        self.include_orig_elements = include_orig_elements\n",
    "        self.max_characters = max_characters\n",
    "        self.multipage_sections = multipage_sections\n",
    "        self.new_after_n_chars = new_after_n_chars\n",
    "        self.overlap = overlap\n",
    "        self.overlap_all = overlap_all\n",
    "\n",
    "    def get_item_text(self, item: Element) -> str:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(item)):\n",
    "            return item.metadata.text_as_html\n",
    "        else:\n",
    "            return item.text\n",
    "\n",
    "    def chunk(self, items: List[Element]) -> List[Tuple[int, List[int], str]]:\n",
    "        # Use the chunk_by_title function to chunk the elements based on their title.\n",
    "        chunked_elements = chunk_by_title(\n",
    "            elements=items,\n",
    "            combine_text_under_n_chars=self.combine_text_under_n_chars,\n",
    "            include_orig_elements=self.include_orig_elements,\n",
    "            max_characters=self.max_characters,\n",
    "            multipage_sections=self.multipage_sections,\n",
    "            new_after_n_chars=self.new_after_n_chars,\n",
    "            overlap=self.overlap,\n",
    "            overlap_all=self.overlap_all,\n",
    "        )\n",
    "\n",
    "        chunks = []\n",
    "        chunk_no = 1\n",
    "\n",
    "        for chunk_element in chunked_elements:\n",
    "            content = self.get_item_text(chunk_element)\n",
    "\n",
    "            # Get the last page number from the chunk's metadata\n",
    "            last_chunk_page_number = getattr(\n",
    "                chunk_element.metadata, \"page_number\", None\n",
    "            )\n",
    "\n",
    "            # Initialize page_numbers list\n",
    "            page_numbers = []\n",
    "\n",
    "            if self.include_orig_elements:\n",
    "                # Extract original elements and find their page numbers\n",
    "                orig_elements = getattr(chunk_element.metadata, \"orig_elements\", [])\n",
    "                page_numbers = [\n",
    "                    getattr(elem.metadata, \"page_number\", None)\n",
    "                    for elem in orig_elements\n",
    "                    if getattr(elem.metadata, \"page_number\", None) is not None\n",
    "                ]\n",
    "\n",
    "                if page_numbers:\n",
    "                    # Get unique sorted page numbers\n",
    "                    page_numbers = sorted(set(page_numbers))\n",
    "                else:\n",
    "                    # If no page numbers found in original elements, fallback\n",
    "                    if last_chunk_page_number is not None:\n",
    "                        page_numbers = [last_chunk_page_number]\n",
    "            else:\n",
    "                # If orig_elements not included, use last_chunk_page_number\n",
    "                if last_chunk_page_number is not None:\n",
    "                    page_numbers = [last_chunk_page_number]\n",
    "\n",
    "            # Append the chunk number, page number list, and content\n",
    "            chunks.append((chunk_no, page_numbers, content))\n",
    "\n",
    "            # Increment the chunk number\n",
    "            chunk_no += 1\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "class CustomChunkingStrategy(ChunkingStrategy):\n",
    "    def chunk(self, items: List[Element]) -> List[Tuple[int, List[int], str]]:\n",
    "        logger.info(\"Using custom chunking strategy from plugin\")\n",
    "        chunks = plugin_manager.hook.chunk_elements(elements=items)\n",
    "        if chunks is not None:\n",
    "            return chunks\n",
    "        else:\n",
    "            raise ValueError(\"No plugin provided chunks via 'chunk_elements' hook.\")\n",
    "\n",
    "\n",
    "class MarkdownChunkingStrategy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_chunk_len: int = 512,\n",
    "        soft_max_len: int = 1024,\n",
    "        hard_max_len: int = 2048,\n",
    "        known_header_footer_phrase: str = \"YOUR KNOWN HEADER FOOTER PHRASE\",\n",
    "    ):\n",
    "        self.min_chunk_len = min_chunk_len\n",
    "        self.soft_max_len = soft_max_len\n",
    "        self.hard_max_len = hard_max_len\n",
    "        self.known_header_footer_phrase = known_header_footer_phrase\n",
    "\n",
    "        self.heading_patterns = [\n",
    "            re.compile(r\"^([0-9]+\\s*[-–.]\\s+.*|[IVXLC]+\\s*[-–.]\\s+.*)\"),\n",
    "            re.compile(r\"^[-=~]{3,}\\s*$\"),\n",
    "            re.compile(r\"^#{1,6}\\s+.*\"),\n",
    "        ]\n",
    "\n",
    "        self.footnote_definition_pattern = re.compile(r\"^\\d+\\.\\s+.*\")\n",
    "        self.footnote_marker_pattern = re.compile(r\"\\[\\d+\\]|[⁰¹²³⁴⁵⁶⁷⁸⁹]\")\n",
    "        self.image_pattern = re.compile(r\"!\\[.*\\]\\(.*\\)\")\n",
    "        self.link_pattern = re.compile(r\"\\[.*?\\]\\((https?://|www\\.).*?\\)\")\n",
    "        self.page_number_pattern = re.compile(r\"^\\s*\\d+\\s*$\")\n",
    "        self.header_footer_pattern = re.compile(\n",
    "            rf\"^.*{re.escape(self.known_header_footer_phrase)}.*$\", re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        self.existing_hashes: Set[str] = set()\n",
    "\n",
    "    def is_duplicate_exact(self, new_block: str) -> bool:\n",
    "        block_hash = hashlib.md5(new_block.encode(\"utf-8\")).hexdigest()\n",
    "        if block_hash in self.existing_hashes:\n",
    "            return True\n",
    "        self.existing_hashes.add(block_hash)\n",
    "        return False\n",
    "\n",
    "    def is_duplicate_fuzzy(self, new_block: str, threshold: float = 95.0) -> bool:\n",
    "        # Not implemented\n",
    "        return False\n",
    "\n",
    "    def is_table(self, block: str) -> bool:\n",
    "        lines = block.strip().split(\"\\n\")\n",
    "        if len(lines) < 2:\n",
    "            return False\n",
    "\n",
    "        has_pipes = any(\"|\" in line for line in lines)\n",
    "        # Check for a typical markdown table separator line\n",
    "        has_separator = any(re.search(r\"^\\s*[-:|]+\\s*$\", l.strip()) for l in lines)\n",
    "        return has_pipes and has_separator\n",
    "\n",
    "    def is_heading(self, block: str) -> bool:\n",
    "        block = block.strip()\n",
    "        for pattern in self.heading_patterns:\n",
    "            if pattern.match(block):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_footnote_definition(self, block: str) -> bool:\n",
    "        return bool(self.footnote_definition_pattern.match(block.strip()))\n",
    "\n",
    "    def is_footnote_marker(self, block: str) -> bool:\n",
    "        return bool(self.footnote_marker_pattern.search(block))\n",
    "\n",
    "    def is_image(self, block: str) -> bool:\n",
    "        return bool(self.image_pattern.search(block))\n",
    "\n",
    "    def is_link(self, block: str) -> bool:\n",
    "        return bool(self.link_pattern.search(block))\n",
    "\n",
    "    def remove_headers_footers(self, text: str) -> str:\n",
    "        lines = text.split(\"\\n\")\n",
    "        cleaned_lines = []\n",
    "        for line in lines:\n",
    "            if self.page_number_pattern.match(line):\n",
    "                continue\n",
    "            if self.header_footer_pattern.match(line):\n",
    "                continue\n",
    "            if \"Navigation menu\" in line or \"Table of Contents\" in line:\n",
    "                continue\n",
    "            cleaned_lines.append(line)\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    def classify_block(self, block: str) -> str:\n",
    "        block = block.strip()\n",
    "        if self.is_table(block):\n",
    "            return \"table\"\n",
    "        elif self.is_footnote_definition(block):\n",
    "            return \"footnote_definition\"\n",
    "        elif self.is_footnote_marker(block):\n",
    "            return \"footnote_marker\"\n",
    "        elif self.is_heading(block):\n",
    "            return \"heading\"\n",
    "        elif self.is_image(block):\n",
    "            return \"image\"\n",
    "        elif self.is_link(block):\n",
    "            return \"link\"\n",
    "        else:\n",
    "            return \"paragraph\"\n",
    "\n",
    "    def split_blocks(self, text: str) -> List[Tuple[str, str]]:\n",
    "        paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "        result_blocks = []\n",
    "        for para in paragraphs:\n",
    "            if not para.strip():\n",
    "                continue\n",
    "            block_type = self.classify_block(para)\n",
    "            result_blocks.append((para.strip(), block_type))\n",
    "        return result_blocks\n",
    "\n",
    "    def find_split_positions(self, text: str, max_len: int) -> List[int]:\n",
    "        positions = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + max_len, len(text))\n",
    "            if end == len(text):\n",
    "                positions.append(len(text))\n",
    "                break\n",
    "            else:\n",
    "                # Find a suitable split point\n",
    "                split_pos = text.rfind(\". \", start, end)\n",
    "                if split_pos == -1 or split_pos <= start:\n",
    "                    split_pos = text.rfind(\"\\n\", start, end)\n",
    "                if split_pos == -1 or split_pos <= start:\n",
    "                    split_pos = end\n",
    "                else:\n",
    "                    split_pos += 1\n",
    "                positions.append(split_pos)\n",
    "                start = split_pos + 1\n",
    "        return positions\n",
    "\n",
    "    def format_markdown(self, block: str) -> str:\n",
    "        if self.is_image(block) or self.is_link(block):\n",
    "            return block.strip()\n",
    "        if self.is_heading(block):\n",
    "            if block.strip().startswith(\"#\"):\n",
    "                return block.strip()\n",
    "            else:\n",
    "                return f\"## {block.strip()}\"\n",
    "        if self.is_footnote_definition(block):\n",
    "            return f\"_{block.strip()}_\"\n",
    "        if self.is_footnote_marker(block):\n",
    "            return block.strip()\n",
    "        if self.is_table(block):\n",
    "            return self.format_markdown_table(block)\n",
    "        return block.strip()\n",
    "\n",
    "    def format_markdown_table(self, block: str) -> str:\n",
    "        return block.strip()\n",
    "\n",
    "    def split_large_table(self, table_text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Previously used for splitting large tables. Now we do not chunk tables at all.\n",
    "        Always return the table as-is.\n",
    "        \"\"\"\n",
    "        formatted_table = self.format_markdown_table(table_text)\n",
    "        return [formatted_table]\n",
    "\n",
    "    def merge_short_chunks(\n",
    "        self, chunks: List[Tuple[int, List[int], str]]\n",
    "    ) -> List[Tuple[int, List[int], str]]:\n",
    "        merged_chunks = []\n",
    "        buffer = None\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_no, pages, content = chunk\n",
    "            content_length = len(content)\n",
    "            if content_length < self.min_chunk_len:\n",
    "                if buffer is None:\n",
    "                    buffer = chunk\n",
    "                else:\n",
    "                    merged_content = buffer[2] + \"\\n\\n\" + content\n",
    "                    merged_pages = sorted(set(buffer[1] + pages))\n",
    "                    merged_chunks.append((buffer[0], merged_pages, merged_content))\n",
    "                    buffer = None\n",
    "            else:\n",
    "                if buffer is not None:\n",
    "                    merged_content = buffer[2] + \"\\n\\n\" + content\n",
    "                    merged_pages = sorted(set(buffer[1] + pages))\n",
    "                    merged_chunks.append((buffer[0], merged_pages, merged_content))\n",
    "                    buffer = None\n",
    "                else:\n",
    "                    merged_chunks.append(chunk)\n",
    "\n",
    "        if buffer is not None:\n",
    "            merged_chunks.append(buffer)\n",
    "\n",
    "        final_chunks = []\n",
    "        for idx, chunk in enumerate(merged_chunks, start=1):\n",
    "            final_chunks.append((idx, chunk[1], chunk[2]))\n",
    "        return final_chunks\n",
    "\n",
    "    def get_elements(\n",
    "        self, pages: List[dict]\n",
    "    ) -> List[Tuple[str, int, int, List[int], str]]:\n",
    "        elements = []\n",
    "        position = 0\n",
    "        consecutive_pages = []\n",
    "        assigned_pages = set()\n",
    "\n",
    "        for page in pages:\n",
    "            try:\n",
    "                content = page[\"page_text\"]\n",
    "                page_number = page[\"page_no\"]\n",
    "            except KeyError as e:\n",
    "                print(f\"Missing key in page data: {e}\")\n",
    "                continue\n",
    "\n",
    "            if page_number in assigned_pages:\n",
    "                continue\n",
    "\n",
    "            content = self.remove_headers_footers(content)\n",
    "            content_length = len(content)\n",
    "\n",
    "            if self.min_chunk_len <= content_length <= self.soft_max_len:\n",
    "                consecutive_pages.append((content, page_number))\n",
    "            else:\n",
    "                if consecutive_pages:\n",
    "                    combined_content = \"\\n\\n\".join([cp[0] for cp in consecutive_pages])\n",
    "                    combined_pages = [cp[1] for cp in consecutive_pages]\n",
    "                    elements.append(\n",
    "                        (\n",
    "                            combined_content,\n",
    "                            position,\n",
    "                            position + len(combined_content),\n",
    "                            combined_pages,\n",
    "                            \"combined_pages\",\n",
    "                        )\n",
    "                    )\n",
    "                    position += len(combined_content) + 1\n",
    "                    assigned_pages.update(combined_pages)\n",
    "                    consecutive_pages = []\n",
    "\n",
    "                blocks = self.split_blocks(content)\n",
    "                for block_text, block_type in blocks:\n",
    "                    if self.is_duplicate_exact(block_text):\n",
    "                        continue\n",
    "                    block_start = position\n",
    "                    block_end = position + len(block_text)\n",
    "                    elements.append(\n",
    "                        (block_text, block_start, block_end, [page_number], block_type)\n",
    "                    )\n",
    "                    position = block_end + 1\n",
    "                    assigned_pages.add(page_number)\n",
    "\n",
    "        if consecutive_pages:\n",
    "            combined_content = \"\\n\\n\".join([cp[0] for cp in consecutive_pages])\n",
    "            combined_pages = [cp[1] for cp in consecutive_pages]\n",
    "            elements.append(\n",
    "                (\n",
    "                    combined_content,\n",
    "                    position,\n",
    "                    position + len(combined_content),\n",
    "                    combined_pages,\n",
    "                    \"combined_pages\",\n",
    "                )\n",
    "            )\n",
    "            position += len(combined_content) + 1\n",
    "            assigned_pages.update(combined_pages)\n",
    "\n",
    "        return elements\n",
    "\n",
    "    def chunk_elements(\n",
    "        self, elements: List[Tuple[str, int, int, List[int], str]]\n",
    "    ) -> List[Tuple[int, List[int], str]]:\n",
    "        # Since we are no longer chunking tables at all,\n",
    "        # we don't use split_large_table for them.\n",
    "        final_elements = []\n",
    "        for block, start, end, pages, block_type in elements:\n",
    "            # If it's a table, just pass it through as a single block.\n",
    "            # No matter its size, we don't chunk it.\n",
    "            final_elements.append((block, start, end, pages, block_type))\n",
    "\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_chunk_length = 0\n",
    "        current_chunk_pages: Set[int] = set()\n",
    "        chunk_no = 1\n",
    "        i = 0\n",
    "        n = len(final_elements)\n",
    "\n",
    "        def finalize_chunk():\n",
    "            nonlocal chunks, current_chunk, current_chunk_length, current_chunk_pages, chunk_no\n",
    "            if current_chunk.strip():\n",
    "                chunks.append(\n",
    "                    (chunk_no, sorted(current_chunk_pages), current_chunk.strip())\n",
    "                )\n",
    "                chunk_no += 1\n",
    "            current_chunk = \"\"\n",
    "            current_chunk_length = 0\n",
    "            current_chunk_pages = set()\n",
    "\n",
    "        while i < n:\n",
    "            block, _, _, block_pages, block_type = final_elements[i]\n",
    "            block_length = len(block)\n",
    "            formatted_block = self.format_markdown(block)\n",
    "\n",
    "            # If this is a table, we ignore soft/hard max lengths and treat it as a single chunk.\n",
    "            if block_type == \"table\":\n",
    "                # Finalize current chunk first\n",
    "                finalize_chunk()\n",
    "                # Place the entire table in a single chunk\n",
    "                current_chunk = formatted_block + \"\\n\\n\"\n",
    "                current_chunk_length = block_length + 2\n",
    "                current_chunk_pages = set(block_pages)\n",
    "                # Immediately finalize this chunk\n",
    "                finalize_chunk()\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # For non-table content:\n",
    "            if block_length > self.hard_max_len:\n",
    "                # If a non-table block exceeds hard_max_len, we still split it\n",
    "                # because the requirement only said don't chunk tables.\n",
    "                split_positions = self.find_split_positions(block, self.hard_max_len)\n",
    "                start_pos = 0\n",
    "                for pos in split_positions:\n",
    "                    split_text = block[start_pos:pos].strip()\n",
    "                    if split_text:\n",
    "                        formatted_split = self.format_markdown(split_text)\n",
    "                        if (\n",
    "                            current_chunk_length + len(split_text) + 2\n",
    "                            <= self.soft_max_len\n",
    "                        ):\n",
    "                            current_chunk += formatted_split + \"\\n\\n\"\n",
    "                            current_chunk_length += len(split_text) + 2\n",
    "                            current_chunk_pages.update(block_pages)\n",
    "                        else:\n",
    "                            if current_chunk_length >= self.min_chunk_len:\n",
    "                                finalize_chunk()\n",
    "                            current_chunk = formatted_split + \"\\n\\n\"\n",
    "                            current_chunk_length = len(split_text) + 2\n",
    "                            current_chunk_pages = set(block_pages)\n",
    "                    start_pos = pos + 1\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            # If it fits in current chunk\n",
    "            if current_chunk_length + block_length + 2 <= self.soft_max_len:\n",
    "                current_chunk += formatted_block + \"\\n\\n\"\n",
    "                current_chunk_length += block_length + 2\n",
    "                current_chunk_pages.update(block_pages)\n",
    "                i += 1\n",
    "            else:\n",
    "                # finalize current chunk if it's large enough\n",
    "                if current_chunk_length >= self.min_chunk_len:\n",
    "                    finalize_chunk()\n",
    "                # start a new chunk with this block\n",
    "                current_chunk = formatted_block + \"\\n\\n\"\n",
    "                current_chunk_length = block_length + 2\n",
    "                current_chunk_pages = set(block_pages)\n",
    "                i += 1\n",
    "\n",
    "        # finalize last chunk\n",
    "        if current_chunk.strip():\n",
    "            chunks.append(\n",
    "                (chunk_no, sorted(current_chunk_pages), current_chunk.strip())\n",
    "            )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def chunk(self, pages: List[dict]) -> List[Tuple[int, List[int], str]]:\n",
    "        elements = self.get_elements(pages)\n",
    "        chunks = self.chunk_elements(elements)\n",
    "        chunks = self.merge_short_chunks(chunks)\n",
    "        return chunks\n",
    "\n",
    "    def process_page(self, page: dict) -> List[Tuple[str, int, int, List[int], str]]:\n",
    "        elements = []\n",
    "        position = 0\n",
    "        try:\n",
    "            content = page[\"page_text\"]\n",
    "            page_number = page[\"page_no\"]\n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in page data: {e}\")\n",
    "            return elements\n",
    "\n",
    "        content = self.remove_headers_footers(content)\n",
    "        content_length = len(content)\n",
    "\n",
    "        if self.min_chunk_len <= content_length <= self.soft_max_len:\n",
    "            elements.append(\n",
    "                (\n",
    "                    content,\n",
    "                    position,\n",
    "                    position + len(content),\n",
    "                    [page_number],\n",
    "                    \"combined_pages\",\n",
    "                )\n",
    "            )\n",
    "            position += len(content) + 1\n",
    "        else:\n",
    "            blocks = self.split_blocks(content)\n",
    "            for block_text, block_type in blocks:\n",
    "                if self.is_duplicate_exact(block_text):\n",
    "                    continue\n",
    "                block_start = position\n",
    "                block_end = position + len(block_text)\n",
    "                elements.append(\n",
    "                    (block_text, block_start, block_end, [page_number], block_type)\n",
    "                )\n",
    "                position = block_end + 1\n",
    "\n",
    "        return elements\n",
    "\n",
    "    def get_elements_parallel(\n",
    "        self, pages: List[dict]\n",
    "    ) -> List[Tuple[str, int, int, List[int], str]]:\n",
    "        elements = []\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            futures = [executor.submit(self.process_page, page) for page in pages]\n",
    "            for future in as_completed(futures):\n",
    "                page_elements = future.result()\n",
    "                elements.extend(page_elements)\n",
    "        return elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_no': 11,\n",
       "  'page_text': '# Page 29-30 Content Extraction\\n\\n## Left Page (29)\\n\\n### Motorsport is and will remain an established part of the Audi strategy\\n\\nAudi resolves to enter the Dakar Rally for the first time ever in 2022 with a prototype. The vehicle\\'s drive concept combines an electric powertrain with a high-voltage battery, which can be charged as needed via an energy converter in the form of a highly efficient TFSI engine. In the future, marathon rally races will spearhead our Audi works team\\'s motorsport activities.\\n\\n[Timeline showing months from Jan to Dec with Nov highlighted]\\n\\n[Image description: A dark silhouette of an Audi racing vehicle with red accent lighting, captioned \"Audi is taking on one of the biggest challenges there is in motorsport: Dakar Rally 2022\"]\\n\\n## Right Page (30)\\n\\n### Audi increases budget for electric mobility through 2025\\n\\nWith its investment planning for the next five years, the company is accelerating the transformation to becoming a provider of connected and sustainable premium mobility. A total amount of around EUR 35 billion will ensure that despite the difficult economic environment, upfront expenditures will remain at a high level particularly with respect to future vehicle projects. Around EUR 17 billion of the investments are allocated to electric mobility, hybridization and digitalization.\\n\\n[Timeline showing months from Jan to Dec with Nov highlighted]\\n\\n[Image description: Close-up detail shot of a black Audi vehicle showing the illuminated Audi rings logo and carbon fiber detailing, with text overlay \"Audi is pursuing a product initiative with the clear focus on electric mobility\" and a technical chart showing \"Audi e-tron GT quattro combined electric power consumption in kWh/100 km: 19.6–18.8 (NEDC); combined CO2 emissions in g/km: 0\"]'},\n",
       " {'page_no': 12,\n",
       "  'page_text': \"# How is Audi shaping the FUTURE\\n\\n# Brief portrait\\n\\nAt a quick glance:\\nGroup performance in 2020.\\n\\nThe Audi Group, with its brands Audi, Lamborghini and Ducati, is one of the most successful manufacturers of automobiles and motorcycles in the premium and supercar segment.\\n\\nAudi has been a fully owned subsidiary of the Volkswagen Group since November 16, 2020. Until this time, the latter held around 99.64 percent of the share capital of AUDI AG.\\n\\nIn 2020, the Audi Group delivered 1,692,773 (1,845,573)¹ cars of the Audi brand, 7,430 (8,205) sports cars of the Lamborghini brand and 48,042 (53,183) motorcycles of the Ducati brand to customers.\\n\\n86,860 (90,640) people were working for the company all over the world as of December 31, 2020; 59,817 (62,377) of them in Germany.\\n\\nAudi (headquarters: Ingolstadt) is present in more than 100 markets worldwide and produced at 18 sites¹ in 12 countries in 2020.\\n\\nThe current overview of sites for 2021 can be found → here.\\n\\n[Map showing Sites in Europe with detailed information about two main locations:]\\n\\nIngolstadt, Germany\\nAUDI AG\\nWith 338,055 (441,608) cars built in 2020, the headquarters in Ingolstadt is the second largest production site in the Audi Group. The plant in the heart of Bavaria is not only a production facility, but is also home to the Audi Group head office and Technical Development. Audi has 43,142 (44,458) employees here, making it the region's largest employer. Measures have already been implemented at the Ingolstadt site to prevent 70 percent of the CO₂ emissions that would otherwise have been produced.\\n\\nAudi models produced at the site:\\n- Q2,\\n- SQ2, A3 Sedan,\\n- A3 Sportback, S3 Sedan,\\n- S3 Sportback,\\n- RS 3 Sportback,\\n- RS 3 Sedan, A4 Avant,\\n- A4 Sedan, S4 Sedan,\\n- S4 Avant, RS 4 Avant,\\n- A5 Coupé, A5 Sportback,\\n- S5 Coupé, S5 Sportback,\\n- RS 5 Coupé, RS 5 Sportback\\n\\nNeckarsulm, Germany\\nAUDI AG, Audi Sport GmbH\\nThe Audi Neckarsulm site has a long tradition and the most diverse range of products. In 2020, 157,230 (177,209) cars rolled off the production lines here. Audi Sport GmbH (formerly quattro GmbH) has had its headquarters here since 1983. Various measures in place at the site currently result in around 70 percent of all CO₂ emissions otherwise produced here. The company produces the Audi e-tron GT quattro, the RS e-tron GT and Audi Sport models at Böllinger Höfe at the Neckarsulm site.\\n\\nAudi models produced at the site:\\n- A4 Sedan,\\n- A5 Cabriolet, S5 Cabriolet,\\n- A6 Avant, A6 Sedan,\\n- A6 allroad, S6 Sedan,\\n- S6 Avant, A7 Sportback,\\n- S7 Sportback, RS7 Sportback,\\n- A8, A8 L, S8, S8 L,\\n- R8 Coupé, R8 Spyder,\\n- e-tron GT, RS e-tron GT\\n\\n¹ The figures in brackets represent the respective prior-year figures\\n² Since 2020, the Audi Brussels site has also been included in emissions → see page 51 ff. The direct, first and second-level models are not declared specifically\\n³ Status of December 31, 2020\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/home/saeed/code/personal/rag/outputs/db-re-sonnet/processed/parsed/audi-report-2020-desktop.parsed.json\"\n",
    "with open(file_path, \"r\") as f:\n",
    "    doc = json.load(f)\n",
    "\n",
    "# display(Markdown(doc[\"doc_md_text\"]))\n",
    "\n",
    "# Only select page 260\n",
    "page = doc[\"pages\"][10: 12]\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 6\n",
      "Min chunk length: 262\n",
      "Max chunk length: 977\n",
      "Average chunk length: 769\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the strategy\n",
    "strategy = MarkdownChunkingStrategy(min_chunk_len=512, soft_max_len=1024, hard_max_len=2048)\n",
    "\n",
    "# Get the chunks\n",
    "chunks = strategy.chunk(page)\n",
    "\n",
    "# Print min, max, and avg chunk lengths\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Min chunk length: {min(len(chunk) for _, _, chunk in chunks)}\")\n",
    "print(f\"Max chunk length: {max(len(chunk) for _, _, chunk in chunks)}\")\n",
    "print(f\"Average chunk length: {sum(len(chunk) for _, _, chunk in chunks) // len(chunks)}\")\n",
    "\n",
    "# Print the chunk with max length\n",
    "max_chunk_len = max(len(chunk) for _, _, chunk in chunks)\n",
    "max_chunk = next((chunk for _, _, chunk in chunks if len(chunk) == max_chunk_len), None)\n",
    "# print(f\"Chunk with max length: {max_chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1, Chunk Len: 842 , Pages: [11]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Page 29-30 Content Extraction\n",
       "\n",
       "## Left Page (29)\n",
       "\n",
       "### Motorsport is and will remain an established part of the Audi strategy\n",
       "\n",
       "Audi resolves to enter the Dakar Rally for the first time ever in 2022 with a prototype. The vehicle's drive concept combines an electric powertrain with a high-voltage battery, which can be charged as needed via an energy converter in the form of a highly efficient TFSI engine. In the future, marathon rally races will spearhead our Audi works team's motorsport activities.\n",
       "\n",
       "[Timeline showing months from Jan to Dec with Nov highlighted]\n",
       "\n",
       "[Image description: A dark silhouette of an Audi racing vehicle with red accent lighting, captioned \"Audi is taking on one of the biggest challenges there is in motorsport: Dakar Rally 2022\"]\n",
       "\n",
       "## Right Page (30)\n",
       "\n",
       "### Audi increases budget for electric mobility through 2025"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Chunk 2, Chunk Len: 977 , Pages: [11, 12]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "With its investment planning for the next five years, the company is accelerating the transformation to becoming a provider of connected and sustainable premium mobility. A total amount of around EUR 35 billion will ensure that despite the difficult economic environment, upfront expenditures will remain at a high level particularly with respect to future vehicle projects. Around EUR 17 billion of the investments are allocated to electric mobility, hybridization and digitalization.\n",
       "\n",
       "[Image description: Close-up detail shot of a black Audi vehicle showing the illuminated Audi rings logo and carbon fiber detailing, with text overlay \"Audi is pursuing a product initiative with the clear focus on electric mobility\" and a technical chart showing \"Audi e-tron GT quattro combined electric power consumption in kWh/100 km: 19.6–18.8 (NEDC); combined CO2 emissions in g/km: 0\"]\n",
       "\n",
       "# How is Audi shaping the FUTURE\n",
       "\n",
       "# Brief portrait\n",
       "\n",
       "At a quick glance:\n",
       "Group performance in 2020."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Chunk 3, Chunk Len: 955 , Pages: [12]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Audi Group, with its brands Audi, Lamborghini and Ducati, is one of the most successful manufacturers of automobiles and motorcycles in the premium and supercar segment.\n",
       "\n",
       "Audi has been a fully owned subsidiary of the Volkswagen Group since November 16, 2020. Until this time, the latter held around 99.64 percent of the share capital of AUDI AG.\n",
       "\n",
       "In 2020, the Audi Group delivered 1,692,773 (1,845,573)¹ cars of the Audi brand, 7,430 (8,205) sports cars of the Lamborghini brand and 48,042 (53,183) motorcycles of the Ducati brand to customers.\n",
       "\n",
       "86,860 (90,640) people were working for the company all over the world as of December 31, 2020; 59,817 (62,377) of them in Germany.\n",
       "\n",
       "Audi (headquarters: Ingolstadt) is present in more than 100 markets worldwide and produced at 18 sites¹ in 12 countries in 2020.\n",
       "\n",
       "The current overview of sites for 2021 can be found → here.\n",
       "\n",
       "[Map showing Sites in Europe with detailed information about two main locations:]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Chunk 4, Chunk Len: 798 , Pages: [12]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Ingolstadt, Germany\n",
       "AUDI AG\n",
       "With 338,055 (441,608) cars built in 2020, the headquarters in Ingolstadt is the second largest production site in the Audi Group. The plant in the heart of Bavaria is not only a production facility, but is also home to the Audi Group head office and Technical Development. Audi has 43,142 (44,458) employees here, making it the region's largest employer. Measures have already been implemented at the Ingolstadt site to prevent 70 percent of the CO₂ emissions that would otherwise have been produced.\n",
       "\n",
       "Audi models produced at the site:\n",
       "- Q2,\n",
       "- SQ2, A3 Sedan,\n",
       "- A3 Sportback, S3 Sedan,\n",
       "- S3 Sportback,\n",
       "- RS 3 Sportback,\n",
       "- RS 3 Sedan, A4 Avant,\n",
       "- A4 Sedan, S4 Sedan,\n",
       "- S4 Avant, RS 4 Avant,\n",
       "- A5 Coupé, A5 Sportback,\n",
       "- S5 Coupé, S5 Sportback,\n",
       "- RS 5 Coupé, RS 5 Sportback"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Chunk 5, Chunk Len: 785 , Pages: [12]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Neckarsulm, Germany\n",
       "AUDI AG, Audi Sport GmbH\n",
       "The Audi Neckarsulm site has a long tradition and the most diverse range of products. In 2020, 157,230 (177,209) cars rolled off the production lines here. Audi Sport GmbH (formerly quattro GmbH) has had its headquarters here since 1983. Various measures in place at the site currently result in around 70 percent of all CO₂ emissions otherwise produced here. The company produces the Audi e-tron GT quattro, the RS e-tron GT and Audi Sport models at Böllinger Höfe at the Neckarsulm site.\n",
       "\n",
       "Audi models produced at the site:\n",
       "- A4 Sedan,\n",
       "- A5 Cabriolet, S5 Cabriolet,\n",
       "- A6 Avant, A6 Sedan,\n",
       "- A6 allroad, S6 Sedan,\n",
       "- S6 Avant, A7 Sportback,\n",
       "- S7 Sportback, RS7 Sportback,\n",
       "- A8, A8 L, S8, S8 L,\n",
       "- R8 Coupé, R8 Spyder,\n",
       "- e-tron GT, RS e-tron GT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Chunk 6, Chunk Len: 262 , Pages: [12]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "¹ The figures in brackets represent the respective prior-year figures\n",
       "² Since 2020, the Audi Brussels site has also been included in emissions → see page 51 ff. The direct, first and second-level models are not declared specifically\n",
       "³ Status of December 31, 2020"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display the chunks\n",
    "for chunk_no, page_numbers, content in chunks[:100]:\n",
    "    print(f\"Chunk {chunk_no}, Chunk Len: {len(content)} , Pages: {page_numbers}\")\n",
    "    # print(content)\n",
    "    # print(\"-\" * 40)\n",
    "    display(Markdown(content))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
