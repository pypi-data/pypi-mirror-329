{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c710fe95-4292-4818-ad22-5332e59afb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/pypoetry/virtualenvs/orign-Sz0VPFy2-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-25 14:42:25,129\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-25 14:42:25 config.py:1664] Downcasting torch.float32 to torch.float16.\n",
      "INFO 10-25 14:42:29 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='allenai/Molmo-7B-D-0924', speculative_config=None, tokenizer='allenai/Molmo-7B-D-0924', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=allenai/Molmo-7B-D-0924, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-25 14:42:30 model_runner.py:1056] Starting to load model allenai/Molmo-7B-D-0924...\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "WARNING 10-25 14:42:30 utils.py:513] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "INFO 10-25 14:42:30 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:02,  2.21it/s]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:01<00:04,  1.23it/s]\n",
      "Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:02<00:04,  1.04s/it]\n",
      "Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:04<00:03,  1.14s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:05<00:02,  1.22s/it]\n",
      "Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:06<00:01,  1.09s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.09s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:07<00:00,  1.06s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-25 14:42:39 model_runner.py:1067] Loading model weights took 14.9975 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.cache/pypoetry/virtualenvs/orign-Sz0VPFy2-py3.12/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/ubuntu/.cache/pypoetry/virtualenvs/orign-Sz0VPFy2-py3.12/lib/python3.12/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-25 14:42:41 gpu_executor.py:122] # GPU blocks: 26351, # CPU blocks: 4681\n",
      "INFO 10-25 14:42:41 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 102.93x\n",
      "INFO 10-25 14:42:45 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-25 14:42:45 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-25 14:42:56 model_runner.py:1523] Graph capturing finished in 11 secs.\n",
      "RequestOutput(request_id=0, prompt='A robot may not injure a human being', prompt_token_ids=tensor([151643,   2657,     25,    362,  12305,   1231,    537,   5811,    552,\n",
      "           264,   3738,   1660,  21388,     25], dtype=torch.int32), encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=[None, {2657: Logprob(logprob=-12.830034255981445, rank=45656, decoded_token=' User'), 90312: Logprob(logprob=-4.3710503578186035, rank=1, decoded_token='.NULL')}, {25: Logprob(logprob=-5.491201877593994, rank=19, decoded_token=':'), 82: Logprob(logprob=-4.306402683258057, rank=1, decoded_token='s')}, {362: Logprob(logprob=-4.926170349121094, rank=17, decoded_token=' A'), 3555: Logprob(logprob=-2.0658185482025146, rank=1, decoded_token=' What')}, {12305: Logprob(logprob=-8.755316734313965, rank=672, decoded_token=' robot'), 78510: Logprob(logprob=-2.421332359313965, rank=1, decoded_token=' cylindrical')}, {1231: Logprob(logprob=-10.313693046569824, rank=741, decoded_token=' may'), 374: Logprob(logprob=-0.7902557253837585, rank=1, decoded_token=' is')}, {537: Logprob(logprob=-0.28631895780563354, rank=1, decoded_token=' not')}, {5811: Logprob(logprob=-2.2159347534179688, rank=2, decoded_token=' inj'), 11428: Logprob(logprob=-0.22765350341796875, rank=1, decoded_token=' harm')}, {552: Logprob(logprob=-0.00038985759601928294, rank=1, decoded_token='ure')}, {264: Logprob(logprob=-0.07231926172971725, rank=1, decoded_token=' a')}, {3738: Logprob(logprob=-0.001095886342227459, rank=1, decoded_token=' human')}, {1660: Logprob(logprob=-0.6076101660728455, rank=1, decoded_token=' being')}, {21388: Logprob(logprob=-23.36225128173828, rank=77122, decoded_token=' Assistant'), 476: Logprob(logprob=-0.002876432379707694, rank=1, decoded_token=' or')}, {25: Logprob(logprob=-3.8252158164978027, rank=7, decoded_token=':'), 1096: Logprob(logprob=-0.9629110097885132, rank=1, decoded_token=' This')}], outputs=[CompletionOutput(index=0, text=\" This is a fundamental ethical principle often associated with robotics and artificial intelligence. It's\", token_ids=(1096, 374, 264, 15811, 30208, 17508, 3545, 5815, 448, 73606, 323, 20443, 11229, 13, 1084, 594), cumulative_logprob=-4.775830920920271, logprobs=[{1096: Logprob(logprob=-0.8004919290542603, rank=1, decoded_token=' This')}, {374: Logprob(logprob=-1.2339112758636475, rank=1, decoded_token=' is')}, {264: Logprob(logprob=-0.5930209159851074, rank=1, decoded_token=' a')}, {15811: Logprob(logprob=-0.19759918749332428, rank=1, decoded_token=' fundamental')}, {30208: Logprob(logprob=-0.1868925541639328, rank=1, decoded_token=' ethical')}, {17508: Logprob(logprob=-0.012940242886543274, rank=1, decoded_token=' principle')}, {3545: Logprob(logprob=-1.0689486265182495, rank=1, decoded_token=' often')}, {5815: Logprob(logprob=-0.25225868821144104, rank=1, decoded_token=' associated')}, {448: Logprob(logprob=-3.528532761265524e-05, rank=1, decoded_token=' with')}, {73606: Logprob(logprob=-0.24734023213386536, rank=1, decoded_token=' robotics')}, {323: Logprob(logprob=-0.015065106563270092, rank=1, decoded_token=' and')}, {20443: Logprob(logprob=-0.053635966032743454, rank=1, decoded_token=' artificial')}, {11229: Logprob(logprob=-0.0004999579978175461, rank=1, decoded_token=' intelligence')}, {13: Logprob(logprob=-0.014294019900262356, rank=1, decoded_token='.')}, {1084: Logprob(logprob=-0.09101378172636032, rank=1, decoded_token=' It')}, {594: Logprob(logprob=-0.007883151061832905, rank=1, decoded_token=\"'s\")}], finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729867376.7392864, last_token_time=1729867377.3077161, first_scheduled_time=1729867376.7432182, first_token_time=1729867376.8680754, time_in_queue=0.003931760787963867, finished_time=1729867377.3076775, scheduler_time=0.0020706980430986732, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
      "RequestOutput(request_id=1, prompt='To be or not to be,', prompt_token_ids=tensor([151643,   2657,     25,   2014,    387,    476,    537,    311,    387,\n",
      "            11,  21388,     25], dtype=torch.int32), encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\" That's the question.\\n\\nTo be, or not to be - that is the\", token_ids=(2938, 594, 279, 3405, 382, 1249, 387, 11, 476, 537, 311, 387, 481, 429, 374, 279), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729867376.8107789, last_token_time=1729867377.3077161, first_scheduled_time=1729867376.8143592, first_token_time=1729867376.8904297, time_in_queue=0.003580331802368164, finished_time=1729867377.307685, scheduler_time=0.001884334022179246, model_forward_time=None, model_execute_time=None), lora_request=None)\n",
      "RequestOutput(request_id=2, prompt='What is the meaning of life?', prompt_token_ids=tensor([151643,   2657,     25,   3555,    374,    279,   7290,    315,   2272,\n",
      "            30,  21388,     25], dtype=torch.int32), encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' The meaning of life is a profound and complex question that has been debated throughout human', token_ids=(576, 7290, 315, 2272, 374, 264, 27155, 323, 6351, 3405, 429, 702, 1012, 58574, 6814, 3738), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None), CompletionOutput(index=1, text=' The meaning of life is a profound and complex question that has been pondered by', token_ids=(576, 7290, 315, 2272, 374, 264, 27155, 323, 6351, 3405, 429, 702, 1012, 47783, 291, 553), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None), CompletionOutput(index=2, text=' The meaning of life is a deeply personal and complex question that has been pondered', token_ids=(576, 7290, 315, 2272, 374, 264, 17247, 4345, 323, 6351, 3405, 429, 702, 1012, 47783, 291), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None), CompletionOutput(index=3, text=' The meaning of life is a profound and complex question that has intrigued philosophers, theolog', token_ids=(576, 7290, 315, 2272, 374, 264, 27155, 323, 6351, 3405, 429, 702, 68018, 60687, 11, 89502), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None), CompletionOutput(index=4, text=' The meaning of life is a profound and complex question that has been pondered by', token_ids=(576, 7290, 315, 2272, 374, 264, 27155, 323, 6351, 3405, 429, 702, 1012, 47783, 291, 553), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1729867376.8868504, last_token_time=1729867377.3077161, first_scheduled_time=1729867376.890405, first_token_time=1729867376.9518473, time_in_queue=0.0035545825958251953, finished_time=1729867377.3076985, scheduler_time=0.0017076200165320188, model_forward_time=None, model_execute_time=None), lora_request=None)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from typing import List, Tuple\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams\n",
    "from vllm.utils import FlexibleArgumentParser\n",
    "\n",
    "\n",
    "def create_test_prompts() -> List[Tuple[str, SamplingParams]]:\n",
    "    \"\"\"Create a list of test prompts with their sampling parameters.\"\"\"\n",
    "    return [\n",
    "        (\"A robot may not injure a human being\",\n",
    "         SamplingParams(temperature=0.0, logprobs=1, prompt_logprobs=1)),\n",
    "        (\"To be or not to be,\",\n",
    "         SamplingParams(temperature=0.8, top_k=5, presence_penalty=0.2)),\n",
    "        (\"What is the meaning of life?\",\n",
    "         SamplingParams(n=2,\n",
    "                        best_of=5,\n",
    "                        temperature=0.8,\n",
    "                        top_p=0.95,\n",
    "                        frequency_penalty=0.1)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def process_requests(engine: LLMEngine,\n",
    "                     test_prompts: List[Tuple[str, SamplingParams]]):\n",
    "    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\n",
    "    request_id = 0\n",
    "\n",
    "    while test_prompts or engine.has_unfinished_requests():\n",
    "        if test_prompts:\n",
    "            prompt, sampling_params = test_prompts.pop(0)\n",
    "            engine.add_request(str(request_id), prompt, sampling_params)\n",
    "            request_id += 1\n",
    "\n",
    "        request_outputs: List[RequestOutput] = engine.step()\n",
    "\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                print(request_output)\n",
    "\n",
    "\n",
    "def initialize_engine(args: argparse.Namespace) -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine from the command line arguments.\"\"\"\n",
    "    engine_args = EngineArgs.from_cli_args(args)\n",
    "    return LLMEngine.from_engine_args(engine_args)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function that sets up and runs the prompt processing.\"\"\"\n",
    "    engine_args = EngineArgs(model=\"allenai/Molmo-7B-D-0924\", trust_remote_code=True)\n",
    "    engine = LLMEngine.from_engine_args(engine_args)\n",
    "    test_prompts = create_test_prompts()\n",
    "    process_requests(engine, test_prompts)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a8118-1984-4b54-9bd2-4e73454be253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
