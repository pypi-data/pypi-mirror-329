{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/molmo/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "GPU: NVIDIA L40S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:06<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Configuration:\n",
      "\n",
      "Config attributes:\n",
      "vocab_size: 152064\n",
      "embedding_size: 152064\n",
      "max_position_embeddings: 4096\n",
      "hidden_size: 3584\n",
      "intermediate_size: 37888\n",
      "num_hidden_layers: 28\n",
      "num_attention_heads: 28\n",
      "layer_norm_eps: 1e-06\n",
      "weight_tying: False\n",
      "use_position_ids: True\n",
      "attention_layer_norm: False\n",
      "num_key_value_heads: 4\n",
      "initializer_range: 0.02\n",
      "use_cache: True\n",
      "rope_theta: 1000000.0\n",
      "clip_qkv: None\n",
      "qkv_bias: True\n",
      "norm_after: False\n",
      "tie_word_embeddings: False\n",
      "layer_norm_type: rms\n",
      "return_dict: True\n",
      "output_hidden_states: False\n",
      "output_attentions: False\n",
      "torchscript: False\n",
      "torch_dtype: torch.float16\n",
      "use_bfloat16: False\n",
      "tf_legacy_loss: False\n",
      "pruned_heads: {}\n",
      "chunk_size_feed_forward: 0\n",
      "is_encoder_decoder: False\n",
      "is_decoder: False\n",
      "cross_attention_hidden_size: None\n",
      "add_cross_attention: False\n",
      "tie_encoder_decoder: False\n",
      "max_length: 20\n",
      "min_length: 0\n",
      "do_sample: False\n",
      "early_stopping: False\n",
      "num_beams: 1\n",
      "num_beam_groups: 1\n",
      "diversity_penalty: 0.0\n",
      "temperature: 1.0\n",
      "top_k: 50\n",
      "top_p: 1.0\n",
      "typical_p: 1.0\n",
      "repetition_penalty: 1.0\n",
      "length_penalty: 1.0\n",
      "no_repeat_ngram_size: 0\n",
      "encoder_no_repeat_ngram_size: 0\n",
      "bad_words_ids: None\n",
      "num_return_sequences: 1\n",
      "output_scores: False\n",
      "return_dict_in_generate: False\n",
      "forced_bos_token_id: None\n",
      "forced_eos_token_id: None\n",
      "remove_invalid_values: False\n",
      "exponential_decay_length_penalty: None\n",
      "suppress_tokens: None\n",
      "begin_suppress_tokens: None\n",
      "architectures: ['MolmoForCausalLM']\n",
      "finetuning_task: None\n",
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "label2id: {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "tokenizer_class: None\n",
      "prefix: None\n",
      "bos_token_id: None\n",
      "pad_token_id: None\n",
      "eos_token_id: None\n",
      "sep_token_id: None\n",
      "decoder_start_token_id: None\n",
      "task_specific_params: None\n",
      "problem_type: None\n",
      "transformers_version: 4.43.3\n",
      "auto_map: {'AutoConfig': 'allenai/Molmo-7B-D-0924--config_molmo.MolmoConfig', 'AutoModelForCausalLM': 'allenai/Molmo-7B-D-0924--modeling_molmo.MolmoForCausalLM'}\n",
      "model_type: molmo\n",
      "\n",
      "Model Structure:\n",
      "Number of parameters: 8,021,025,280\n",
      "Number of trainable parameters: 8,021,025,280\n",
      "\n",
      "Model Methods:\n",
      "Available generation methods: ['can_generate', 'generate', 'generate_from_batch']\n",
      "\n",
      "can_generate method signature:\n",
      "Arguments: ('self',)\n",
      "\n",
      "generate method signature:\n",
      "Arguments: ()\n",
      "\n",
      "generate_from_batch method signature:\n",
      "Arguments: ()\n",
      "\n",
      "Tokenizer Information:\n",
      "Vocabulary size: 152064\n",
      "Model max length: 4096\n",
      "\n",
      "Running processor debug...\n",
      "\n",
      "Raw processor outputs:\n",
      "input_ids: torch.Size([324])\n",
      "images: torch.Size([2, 576, 588])\n",
      "image_input_idx: torch.Size([2, 144])\n",
      "image_masks: torch.Size([2, 576])\n",
      "\n",
      "Debug output received. Proceeding with main script...\n",
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 44.31 GiB of which 41.00 MiB is free. Process 11287 has 24.34 GiB memory in use. Including non-PyTorch memory, this process has 19.91 GiB memory in use. Of the allocated memory 19.15 GiB is allocated by PyTorch, and 267.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 400\u001b[0m\n\u001b[1;32m    397\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 400\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 372\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m pending_threads \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     {\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_url\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://picsum.photos/id/237/536/354\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     },\n\u001b[1;32m    369\u001b[0m ]\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Process sequences\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m finished_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpending_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpending_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\n\u001b[1;32m    379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Decode and print results\u001b[39;00m\n\u001b[1;32m    382\u001b[0m results \u001b[38;5;241m=\u001b[39m decode_sequences(finished_sequences, processor)\n",
      "Cell \u001b[0;32mIn[1], line 263\u001b[0m, in \u001b[0;36mprocess_sequences\u001b[0;34m(model, processor, pending_threads, device, max_new_tokens, max_batch_size)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast() \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nullcontext():\n\u001b[0;32m--> 263\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# Update sequences\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(initial_sequences):\n",
      "File \u001b[0;32m~/miniconda3/envs/molmo/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/molmo/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/allenai/Molmo-7B-D-0924/1721478b71306fb7dc671176d5c204dc7a4d27d7/modeling_molmo.py:2106\u001b[0m, in \u001b[0;36mMolmoForCausalLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, attention_bias, response_mask, images, image_masks, image_input_idx, subsegment_ids, position_ids, past_key_values, labels, loss_masks, use_cache, last_logits_only, output_attentions, output_hidden_states, append_last_valid_logits, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   2103\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 2106\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_input_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_input_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2115\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubsegment_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubsegment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2118\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_logits_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_logits_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2120\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mappend_last_valid_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mappend_last_valid_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2124\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m   2125\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/allenai/Molmo-7B-D-0924/1721478b71306fb7dc671176d5c204dc7a4d27d7/modeling_molmo.py:1954\u001b[0m, in \u001b[0;36mMolmo.forward\u001b[0;34m(self, input_ids, input_embeddings, attention_mask, attention_bias, response_mask, images, image_masks, image_input_idx, subsegment_ids, position_ids, past_key_values, use_cache, last_logits_only, output_hidden_states, append_last_valid_logits)\u001b[0m\n\u001b[1;32m   1951\u001b[0m     all_hidden_states\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m   1953\u001b[0m layer_past \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m past_key_values[block_idx]\n\u001b[0;32m-> 1954\u001b[0m x, cache \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1957\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/molmo/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/molmo/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/allenai/Molmo-7B-D-0924/1721478b71306fb7dc671176d5c204dc7a4d27d7/modeling_molmo.py:520\u001b[0m, in \u001b[0;36mMolmoSequentialBlock.forward\u001b[0;34m(self, x, attention_bias, position_ids, layer_past, use_cache)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    518\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff_norm(x)\n\u001b[0;32m--> 520\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact, x)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/molmo/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/molmo/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/molmo/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 44.31 GiB of which 41.00 MiB is free. Process 11287 has 24.34 GiB memory in use. Including non-PyTorch memory, this process has 19.91 GiB memory in use. Of the allocated memory 19.15 GiB is allocated by PyTorch, and 267.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure pad_token is set\n",
    "\n",
    "print(\"Special tokens map:\", tokenizer.special_tokens_map)\n",
    "print(\"All special tokens:\", tokenizer.all_special_tokens)\n",
    "print(\"All special token IDs:\", tokenizer.all_special_ids)\n",
    "\n",
    "# Get the model's default dtype\n",
    "default_dtype = next(model.parameters()).dtype\n",
    "\n",
    "# All message threads\n",
    "pending_threads = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Tell me about the history of artificial intelligence.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the capital city of France?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Explain the theory of relativity.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"How does the process of photosynthesis work?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the tallest mountain in the world?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who wrote 'To Kill a Mockingbird'?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the speed of light?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Describe the process of evolution.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is quantum computing?\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who was Albert Einstein?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "max_new_tokens = 50  # Maximum tokens to generate per sequence\n",
    "max_batch_size = 4   # Maximum number of sequences in a batch\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Initialize lists\n",
    "active_sequences = []\n",
    "finished_sequences = []\n",
    "sequence_id = 0  # Unique identifier for each sequence\n",
    "\n",
    "# Get model configurations\n",
    "num_layers = model.config.num_hidden_layers\n",
    "\n",
    "# Adjusted for Grouped Query Attention (GQA)\n",
    "if hasattr(model.config, \"num_key_value_heads\"):\n",
    "    num_kv_heads = model.config.num_key_value_heads\n",
    "elif hasattr(model.config, \"num_key_value_groups\"):\n",
    "    num_kv_heads = model.config.num_key_value_groups\n",
    "else:\n",
    "    # Default to num_attention_heads if no GQA is used\n",
    "    num_kv_heads = model.config.num_attention_heads\n",
    "\n",
    "# Main loop\n",
    "while pending_threads or active_sequences:\n",
    "    # Fill up the batch with new message threads if we have space\n",
    "    while len(active_sequences) < max_batch_size and pending_threads:\n",
    "        thread = pending_threads.pop(0)\n",
    "        messages = thread[\"messages\"]\n",
    "        print(\"\\n\\n---processing messages: \", messages)\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_input = tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = model_input[\"input_ids\"].to(model.device)  # Shape: [1, seq_len]\n",
    "        attention_mask = model_input[\"attention_mask\"].to(model.device)\n",
    "        # Initialize position_ids\n",
    "        position_ids = (attention_mask.cumsum(dim=1) - 1).clamp(min=0)\n",
    "\n",
    "        # Initialize past_key_values as None for the sequence\n",
    "        sequence = {\n",
    "            \"id\": sequence_id,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"generated_ids\": input_ids.clone(),\n",
    "            \"past_key_values\": None,  # Initialize as None\n",
    "            \"finished\": False,\n",
    "            \"max_length\": input_ids.shape[1] + max_new_tokens,\n",
    "            \"prompt_length\": input_ids.shape[1],  # Save the prompt length\n",
    "        }\n",
    "        active_sequences.append(sequence)\n",
    "        sequence_id += 1\n",
    "        print(f\"Added sequence {sequence['id']} to active sequences\")\n",
    "\n",
    "    if not active_sequences:\n",
    "        break  # No active sequences left to process\n",
    "\n",
    "    # Separate sequences into initial and subsequent sequences\n",
    "    initial_sequences = [seq for seq in active_sequences if seq[\"past_key_values\"] is None]\n",
    "    subsequent_sequences = [seq for seq in active_sequences if seq[\"past_key_values\"] is not None]\n",
    "\n",
    "    # Process initial sequences\n",
    "    if initial_sequences:\n",
    "        batch_input_ids = [seq[\"input_ids\"].squeeze(0) for seq in initial_sequences]  # Remove batch dimension\n",
    "        batch_attention_mask = [seq[\"attention_mask\"].squeeze(0) for seq in initial_sequences]\n",
    "        batch_position_ids = [seq[\"position_ids\"].squeeze(0) for seq in initial_sequences]\n",
    "\n",
    "        # Pad sequences to the same length\n",
    "        batch_input_ids = torch.nn.utils.rnn.pad_sequence(batch_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "        batch_attention_mask = torch.nn.utils.rnn.pad_sequence(batch_attention_mask, batch_first=True, padding_value=0)\n",
    "        batch_position_ids = torch.nn.utils.rnn.pad_sequence(batch_position_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "        # Prepare model inputs\n",
    "        model_inputs = {\n",
    "            \"input_ids\": batch_input_ids,\n",
    "            \"attention_mask\": batch_attention_mask,\n",
    "            \"position_ids\": batch_position_ids,\n",
    "            \"use_cache\": True,\n",
    "        }\n",
    "\n",
    "        # Forward pass\n",
    "        print(f\"Calling model for initial sequences...\")\n",
    "        outputs = model(**model_inputs)\n",
    "        print(\"Model call completed.\")\n",
    "\n",
    "        logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "        new_past_key_values = outputs.past_key_values  # List of tuples per layer\n",
    "\n",
    "        # Update each sequence\n",
    "        for idx, seq in enumerate(initial_sequences):\n",
    "            # Extract the key and value tensors for this sequence\n",
    "            seq_past_key_values = []\n",
    "            for layer_idx in range(num_layers):\n",
    "                key = new_past_key_values[layer_idx][0][idx:idx+1]\n",
    "                value = new_past_key_values[layer_idx][1][idx:idx+1]\n",
    "                seq_past_key_values.append((key, value))\n",
    "            seq[\"past_key_values\"] = seq_past_key_values\n",
    "\n",
    "            # Get next token logits (last token)\n",
    "            next_token_logits = logits[idx, -1, :]\n",
    "\n",
    "            # Apply greedy decoding\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # Shape: [1]\n",
    "            next_token = next_token.to(seq[\"generated_ids\"].device)\n",
    "\n",
    "            # Update generated_ids\n",
    "            seq[\"generated_ids\"] = torch.cat([seq[\"generated_ids\"], next_token.unsqueeze(0)], dim=1)\n",
    "            # Update position_ids\n",
    "            seq[\"position_ids\"] = torch.cat([seq[\"position_ids\"], seq[\"position_ids\"][:, -1:] + 1], dim=1)\n",
    "\n",
    "            # Check for EOS token or max length\n",
    "            if next_token.item() == eos_token_id or seq[\"generated_ids\"].shape[1] >= seq[\"max_length\"]:\n",
    "                seq[\"finished\"] = True\n",
    "                print(f\"Sequence {seq['id']} finished generating.\")\n",
    "            else:\n",
    "                print(f\"Sequence {seq['id']} generated token id {next_token.item()}\")\n",
    "\n",
    "    # Process subsequent sequences\n",
    "    if subsequent_sequences:\n",
    "        # Group sequences by past_key_values seq_len\n",
    "        seq_len_to_sequences = defaultdict(list)\n",
    "        for seq in subsequent_sequences:\n",
    "            seq_len = seq[\"past_key_values\"][0][0].shape[2]  # seq_len dimension\n",
    "            seq_len_to_sequences[seq_len].append(seq)\n",
    "\n",
    "        # Process each group separately\n",
    "        for seq_len, sequences in seq_len_to_sequences.items():\n",
    "            batch_input_ids = []\n",
    "            batch_attention_mask = []\n",
    "            batch_position_ids = []\n",
    "            batch_past_key_values = []\n",
    "            next_position_ids_per_sequence = []  # Store per-sequence next_position_id\n",
    "\n",
    "            # Collect inputs and past_key_values\n",
    "            for idx, seq in enumerate(sequences):\n",
    "                # Collect past_key_values for each layer\n",
    "                for layer_idx in range(num_layers):\n",
    "                    past_key, past_value = seq[\"past_key_values\"][layer_idx]\n",
    "                    if idx == 0:\n",
    "                        # Initialize lists for this layer\n",
    "                        batch_past_key_values.append([[], []])\n",
    "                    batch_past_key_values[layer_idx][0].append(past_key)\n",
    "                    batch_past_key_values[layer_idx][1].append(past_value)\n",
    "\n",
    "                # Prepare input_ids, attention_mask, position_ids\n",
    "                next_input_id = seq[\"generated_ids\"][:, -1:]  # Shape: [1, 1]\n",
    "                batch_input_ids.append(next_input_id)\n",
    "\n",
    "                attention_mask = torch.ones_like(next_input_id, dtype=seq[\"attention_mask\"].dtype)\n",
    "                batch_attention_mask.append(attention_mask)\n",
    "\n",
    "                next_position_id = seq[\"position_ids\"][:, -1:] + 1  # Shape: [1, 1]\n",
    "                batch_position_ids.append(next_position_id)\n",
    "                # Store per-sequence next_position_id for later use\n",
    "                next_position_ids_per_sequence.append(next_position_id)\n",
    "\n",
    "            # Stack past_key_values for each layer\n",
    "            for layer_idx in range(num_layers):\n",
    "                keys = torch.cat(batch_past_key_values[layer_idx][0], dim=0)\n",
    "                values = torch.cat(batch_past_key_values[layer_idx][1], dim=0)\n",
    "                batch_past_key_values[layer_idx] = (keys, values)\n",
    "\n",
    "            # Concatenate input tensors along batch dimension\n",
    "            batch_input_ids = torch.cat(batch_input_ids, dim=0)\n",
    "            batch_attention_mask = torch.cat(batch_attention_mask, dim=0)\n",
    "            batch_position_ids = torch.cat(batch_position_ids, dim=0)\n",
    "\n",
    "            # Prepare model inputs\n",
    "            model_inputs = {\n",
    "                \"input_ids\": batch_input_ids,\n",
    "                \"attention_mask\": batch_attention_mask,\n",
    "                \"position_ids\": batch_position_ids,\n",
    "                \"past_key_values\": batch_past_key_values,\n",
    "                \"use_cache\": True,\n",
    "            }\n",
    "\n",
    "            # Forward pass\n",
    "            print(f\"Processing sequences with past_key_values seq_len = {seq_len}\")\n",
    "            outputs = model(**model_inputs)\n",
    "            print(\"Model call completed.\")\n",
    "\n",
    "            logits = outputs.logits  # Shape: (batch_size, seq_length, vocab_size)\n",
    "            new_past_key_values = outputs.past_key_values  # List of tuples per layer\n",
    "\n",
    "            # Update each sequence in the group\n",
    "            for idx, seq in enumerate(sequences):\n",
    "                # Extract the key and value tensors for this sequence\n",
    "                seq_past_key_values = []\n",
    "                for layer_idx in range(num_layers):\n",
    "                    key = new_past_key_values[layer_idx][0][idx:idx+1]\n",
    "                    value = new_past_key_values[layer_idx][1][idx:idx+1]\n",
    "                    seq_past_key_values.append((key, value))\n",
    "                seq[\"past_key_values\"] = seq_past_key_values\n",
    "\n",
    "                # Get next token logits (last token)\n",
    "                next_token_logits = logits[idx, -1, :]\n",
    "\n",
    "                # Apply any decoding strategy here (e.g., greedy)\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # Shape: [1]\n",
    "                next_token = next_token.to(seq[\"generated_ids\"].device)\n",
    "\n",
    "                # Update generated_ids\n",
    "                seq[\"generated_ids\"] = torch.cat([seq[\"generated_ids\"], next_token.unsqueeze(0)], dim=1)\n",
    "                # Update position_ids using the stored next_position_id\n",
    "                seq[\"position_ids\"] = torch.cat([seq[\"position_ids\"], next_position_ids_per_sequence[idx]], dim=1)\n",
    "\n",
    "                # Check for EOS token or max length\n",
    "                if next_token.item() == eos_token_id or seq[\"generated_ids\"].shape[1] >= seq[\"max_length\"]:\n",
    "                    seq[\"finished\"] = True\n",
    "                    print(f\"Sequence {seq['id']} finished generating.\")\n",
    "                else:\n",
    "                    print(f\"Sequence {seq['id']} generated token id {next_token.item()}\")\n",
    "\n",
    "    # Remove finished sequences and add to finished_sequences\n",
    "    new_active_sequences = []\n",
    "    for seq in active_sequences:\n",
    "        if seq[\"finished\"]:\n",
    "            finished_sequences.append(seq)\n",
    "        else:\n",
    "            new_active_sequences.append(seq)\n",
    "    active_sequences = new_active_sequences\n",
    "\n",
    "    # Refill the batch after processing\n",
    "    while len(active_sequences) < max_batch_size and pending_threads:\n",
    "        thread = pending_threads.pop(0)\n",
    "        messages = thread[\"messages\"]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        print(f\"Formatted input in working script:\\n{text}\\n\")\n",
    "        model_input = tokenizer(text, return_tensors=\"pt\")\n",
    "        input_ids = model_input[\"input_ids\"].to(model.device)\n",
    "        attention_mask = model_input[\"attention_mask\"].to(model.device)\n",
    "        # Initialize position_ids\n",
    "        position_ids = (attention_mask.cumsum(dim=1) - 1).clamp(min=0)\n",
    "\n",
    "        # Initialize past_key_values as None for the sequence\n",
    "        sequence = {\n",
    "            \"id\": sequence_id,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"position_ids\": position_ids,\n",
    "            \"generated_ids\": input_ids.clone(),\n",
    "            \"past_key_values\": None,  # Initialize as None\n",
    "            \"finished\": False,\n",
    "            \"max_length\": input_ids.shape[1] + max_new_tokens,\n",
    "            \"prompt_length\": input_ids.shape[1],  # Save the prompt length\n",
    "        }\n",
    "        active_sequences.append(sequence)\n",
    "        sequence_id += 1\n",
    "        print(f\"Added sequence {sequence['id']} to active sequences\")\n",
    "\n",
    "# Decode generated sequences\n",
    "for seq in finished_sequences:\n",
    "    generated_ids = seq[\"generated_ids\"]\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze(0), skip_special_tokens=True)\n",
    "    print(f\"\\nFull Generated Text {seq['id']}:\\n{generated_text}\\n\")\n",
    "\n",
    "    # Extract only the assistant's response by removing the prompt\n",
    "    response_ids = generated_ids[:, seq[\"prompt_length\"]:]  # Remove the prompt tokens\n",
    "    assistant_response = tokenizer.decode(response_ids.squeeze(0), skip_special_tokens=True)\n",
    "    print(f\"Assistant's Response {seq['id']}:\\n{assistant_response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
