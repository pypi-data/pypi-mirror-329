{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "import requests\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "from contextlib import contextmanager\n",
    "from contextlib import nullcontext\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    # Enable TF32 for better performance on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "def resize_and_pad_image(image):\n",
    "    \"\"\"Resize image to a fixed size and pad if necessary.\"\"\"\n",
    "    target_size = (224, 224)  # Standard size for many vision models\n",
    "    image = image.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    return image\n",
    "\n",
    "def debug_processor_output(processor):\n",
    "    \"\"\"Debug function to examine processor output shapes.\"\"\"\n",
    "    image = Image.open(requests.get(\"https://picsum.photos/id/237/536/354\", stream=True).raw)\n",
    "    image = resize_and_pad_image(image)\n",
    "    \n",
    "    print(\"\\nRunning processor debug...\")\n",
    "    raw_inputs = processor.process(\n",
    "        images=[image],\n",
    "        text=\"Test image.\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRaw processor outputs:\")\n",
    "    for k, v in raw_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            print(f\"{k}: {v.shape}\")\n",
    "    return raw_inputs\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage of the process\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    cpu_mem = process.memory_info().rss / 1024 / 1024  # MB\n",
    "    gpu_mem = torch.cuda.memory_allocated() / 1024 / 1024 if torch.cuda.is_available() else 0  # MB\n",
    "    return f\"CPU Memory: {cpu_mem:.2f}MB, GPU Memory: {gpu_mem:.2f}MB\"\n",
    "\n",
    "@contextmanager\n",
    "def batch_memory_manager():\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"Memory after batch cleanup: {get_memory_usage()}\")\n",
    "\n",
    "def print_model_config(model):\n",
    "    \"\"\"Print detailed model configuration for debugging.\"\"\"\n",
    "    print(\"\\nModel Configuration:\")\n",
    "    config = model.config\n",
    "    print(\"\\nConfig attributes:\")\n",
    "    for key, value in config.__dict__.items():\n",
    "        if not key.startswith('_'):\n",
    "            print(f\"{key}: {value}\")\n",
    "    \n",
    "    print(\"\\nModel Structure:\")\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    print(\"\\nModel Methods:\")\n",
    "    generation_methods = [method for method in dir(model) if 'generate' in method and not method.startswith('_')]\n",
    "    print(\"Available generation methods:\", generation_methods)\n",
    "    \n",
    "    for method in generation_methods:\n",
    "        if hasattr(model, method):\n",
    "            print(f\"\\n{method} method signature:\")\n",
    "            method_obj = getattr(model, method)\n",
    "            if hasattr(method_obj, '__code__'):\n",
    "                print(f\"Arguments: {method_obj.__code__.co_varnames[:method_obj.__code__.co_argcount]}\")\n",
    "    \n",
    "    print(\"\\nTokenizer Information:\")\n",
    "    print(f\"Vocabulary size: {model.config.vocab_size}\")\n",
    "    print(f\"Model max length: {model.config.max_position_embeddings if hasattr(model.config, 'max_position_embeddings') else 'Not specified'}\")\n",
    "\n",
    "def process_single_thread(thread, processor, device):\n",
    "    \"\"\"Process a single thread with image and text.\"\"\"\n",
    "    image = Image.open(requests.get(thread[\"image_url\"], stream=True).raw)\n",
    "\n",
    "    with torch.cuda.amp.autocast() if device.type == \"cuda\" else nullcontext():\n",
    "        inputs = processor.process(\n",
    "            images=[image],\n",
    "            text=thread[\"text\"],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    \n",
    "    # Process and reshape inputs\n",
    "    processed_inputs = {}\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            if k == \"input_ids\":\n",
    "                processed_inputs[k] = v.unsqueeze(0) if len(v.shape) == 1 else v\n",
    "            elif k == \"images\":\n",
    "                processed_inputs[k] = v.unsqueeze(0) if len(v.shape) < 4 else v\n",
    "            elif k in [\"image_masks\", \"image_input_idx\"]:\n",
    "                processed_inputs[k] = v.reshape(1, 2, -1) if len(v.shape) == 2 else v\n",
    "            else:\n",
    "                processed_inputs[k] = v\n",
    "            processed_inputs[k] = processed_inputs[k].to(device)\n",
    "        else:\n",
    "            processed_inputs[k] = v\n",
    "    \n",
    "    processed_inputs[\"attention_mask\"] = torch.ones_like(\n",
    "        processed_inputs[\"input_ids\"],\n",
    "        dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n",
    "    )\n",
    "    \n",
    "    return processed_inputs\n",
    "\n",
    "def process_subsequent_sequences(subsequent_sequences, model, processor, num_layers, device):\n",
    "    \"\"\"Process subsequent sequence generation with proper attention handling.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ENTERING PROCESS_SUBSEQUENT_SEQUENCES\")\n",
    "    print(f\"Number of sequences to process: {len(subsequent_sequences)}\")\n",
    "    print(f\"Number of layers: {num_layers}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Create generation config\n",
    "        generation_config = GenerationConfig(\n",
    "            max_new_tokens=1,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            use_cache=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        )\n",
    "        print(\"\\nGeneration Config:\")\n",
    "        print(f\"max_new_tokens: {generation_config.max_new_tokens}\")\n",
    "        print(f\"pad_token_id: {generation_config.pad_token_id}\")\n",
    "        print(f\"eos_token_id: {generation_config.eos_token_id}\")\n",
    "\n",
    "        # Group sequences by past_key_values sequence length\n",
    "        print(\"\\nGrouping sequences by sequence length...\")\n",
    "        seq_len_to_sequences = defaultdict(list)\n",
    "        for idx, seq in enumerate(subsequent_sequences):\n",
    "            seq_len = seq[\"past_key_values\"][0][0].shape[2]\n",
    "            seq_len_to_sequences[seq_len].append(seq)\n",
    "            print(f\"Sequence {idx}: length = {seq_len}, generated_ids shape = {seq['generated_ids'].shape}\")\n",
    "        \n",
    "        print(f\"\\nFound {len(seq_len_to_sequences)} different sequence lengths: {list(seq_len_to_sequences.keys())}\")\n",
    "        \n",
    "        results = []\n",
    "        for seq_len, sequences in seq_len_to_sequences.items():\n",
    "            print(\"\\n\" + \"-\"*80)\n",
    "            print(f\"Processing sequence group with seq_len = {seq_len}\")\n",
    "            print(f\"Number of sequences in this group: {len(sequences)}\")\n",
    "            \n",
    "            batch_size = len(sequences)\n",
    "            print(\"\\nPreparing batch inputs...\")\n",
    "            print(\"Collecting last tokens from each sequence...\")\n",
    "            batch_input_ids = torch.cat([seq[\"generated_ids\"][..., -1:] for seq in sequences], dim=0)\n",
    "            print(f\"batch_input_ids shape: {batch_input_ids.shape}\")\n",
    "            print(f\"batch_input_ids values: {batch_input_ids.tolist()}\")\n",
    "            \n",
    "            # Calculate mask length\n",
    "            mask_len = seq_len + generation_config.max_new_tokens\n",
    "            print(f\"\\nCalculating attention mask length: {seq_len} (current) + {generation_config.max_new_tokens} (new) = {mask_len}\")\n",
    "            \n",
    "            # Create attention mask\n",
    "            print(\"Creating attention mask...\")\n",
    "            batch_attention_mask = torch.ones(\n",
    "                (batch_size, mask_len),\n",
    "                dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "                device=device\n",
    "            )\n",
    "            print(f\"batch_attention_mask shape: {batch_attention_mask.shape}\")\n",
    "            \n",
    "            # Stack past key values\n",
    "            print(\"\\nStacking past key values...\")\n",
    "            batch_past_key_values = []\n",
    "            for layer_idx in range(num_layers):\n",
    "                print(f\"\\nProcessing layer {layer_idx}\")\n",
    "                layer_keys = []\n",
    "                layer_values = []\n",
    "                \n",
    "                for seq_idx, seq in enumerate(sequences):\n",
    "                    past_key, past_value = seq[\"past_key_values\"][layer_idx]\n",
    "                    print(f\"Sequence {seq_idx} - Key shape: {past_key.shape}, Value shape: {past_value.shape}\")\n",
    "                    layer_keys.append(past_key)\n",
    "                    layer_values.append(past_value)\n",
    "                \n",
    "                keys = torch.cat(layer_keys, dim=0)\n",
    "                values = torch.cat(layer_values, dim=0)\n",
    "                print(f\"Concatenated - Key shape: {keys.shape}, Value shape: {values.shape}\")\n",
    "                batch_past_key_values.append((keys, values))\n",
    "\n",
    "            # Handle position IDs\n",
    "            position_ids = None\n",
    "            if model.config.use_position_ids:\n",
    "                print(\"\\nModel uses position IDs. Creating position tensor...\")\n",
    "                position_ids = torch.full(\n",
    "                    (batch_size, 1),\n",
    "                    seq_len,\n",
    "                    dtype=torch.long,\n",
    "                    device=device\n",
    "                )\n",
    "                print(f\"position_ids shape: {position_ids.shape}\")\n",
    "                print(f\"position_ids values: {position_ids.tolist()}\")\n",
    "\n",
    "            # Prepare model inputs\n",
    "            print(\"\\nPreparing final model inputs...\")\n",
    "            model_inputs = {\n",
    "                \"input_ids\": batch_input_ids,\n",
    "                \"attention_mask\": batch_attention_mask,\n",
    "                \"position_ids\": position_ids,\n",
    "                \"past_key_values\": batch_past_key_values,\n",
    "                \"use_cache\": True,\n",
    "            }\n",
    "\n",
    "            print(\"\\nModel input shapes:\")\n",
    "            for k, v in model_inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    print(f\"{k}: shape={v.shape}, dtype={v.dtype}, device={v.device}\")\n",
    "                elif isinstance(v, list):\n",
    "                    print(f\"{k}: {len(v)} layers\")\n",
    "                    print(f\"First layer shapes - Key: {v[0][0].shape}, Value: {v[0][1].shape}\")\n",
    "\n",
    "            try:\n",
    "                print(\"\\nStarting model forward pass...\")\n",
    "                with torch.cuda.amp.autocast() if device.type == \"cuda\" else nullcontext():\n",
    "                    print(\"Running model forward pass...\")\n",
    "                    outputs = model(\n",
    "                        **model_inputs,\n",
    "                        return_dict=True\n",
    "                    )\n",
    "                    \n",
    "                    print(\"\\nModel outputs received:\")\n",
    "                    print(f\"Logits shape: {outputs.logits.shape}\")\n",
    "                    print(f\"Past key values: {len(outputs.past_key_values)} layers\")\n",
    "                    print(f\"First layer past_key_values shapes - Key: {outputs.past_key_values[0][0].shape}, Value: {outputs.past_key_values[0][1].shape}\")\n",
    "                    \n",
    "                    print(\"\\nComputing next tokens...\")\n",
    "                    next_token_logits = outputs.logits[:, -1, :]\n",
    "                    print(f\"Next token logits shape: {next_token_logits.shape}\")\n",
    "                    \n",
    "                    next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
    "                    next_tokens = next_tokens.unsqueeze(-1)\n",
    "                    print(f\"Next tokens shape: {next_tokens.shape}\")\n",
    "                    print(f\"Next tokens values: {next_tokens.tolist()}\")\n",
    "\n",
    "                    print(\"\\nCreating result object...\")\n",
    "                    result = type('GenerationResult', (), {\n",
    "                        'logits': outputs.logits,\n",
    "                        'past_key_values': outputs.past_key_values,\n",
    "                        'generated_tokens': next_tokens\n",
    "                    })\n",
    "                    print(\"Result object created successfully\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"\\nERROR DURING GENERATION:\")\n",
    "                print(f\"Exception type: {type(e)}\")\n",
    "                print(f\"Exception message: {str(e)}\")\n",
    "                print(\"Exception args:\", e.args)\n",
    "                print(\"\\nModel input shapes at time of error:\")\n",
    "                for k, v in model_inputs.items():\n",
    "                    if isinstance(v, torch.Tensor):\n",
    "                        print(f\"{k}: shape={v.shape}, dtype={v.dtype}, device={v.device}\")\n",
    "                    elif isinstance(v, list):\n",
    "                        print(f\"{k}: {len(v)} layers\")\n",
    "                        print(f\"First layer shapes - Key: {v[0][0].shape}, Value: {v[0][1].shape}\")\n",
    "                raise\n",
    "\n",
    "            results.append((sequences, result))\n",
    "            print(f\"\\nAdded results for sequence group with seq_len = {seq_len}\")\n",
    "            print(f\"Number of results so far: {len(results)}\")\n",
    "        \n",
    "        print(\"\\nCompleted all sequence groups\")\n",
    "        print(f\"Total number of results: {len(results)}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def process_sequences(model, processor, pending_threads, device, max_new_tokens=200, max_batch_size=4):\n",
    "    \"\"\"Main sequence processing function with fixed dimension handling.\"\"\"\n",
    "    active_sequences = []\n",
    "    finished_sequences = []\n",
    "    sequence_id = 0\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    eos_token_id = get_eos_token_id(processor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        print(\"\\nStarting sequence processing...\")\n",
    "        print(f\"Number of layers: {num_layers}\")\n",
    "        print(f\"EOS token ID: {eos_token_id}\")\n",
    "        print(f\"Initial memory usage: {get_memory_usage()}\")\n",
    "\n",
    "        while pending_threads or active_sequences:\n",
    "            # Fill batch with new threads\n",
    "            while len(active_sequences) < max_batch_size and pending_threads:\n",
    "                thread = pending_threads.pop(0)\n",
    "                inputs = process_single_thread(thread, processor, device)\n",
    "                \n",
    "                sequence = {\n",
    "                    \"id\": sequence_id,\n",
    "                    \"inputs\": inputs,\n",
    "                    \"generated_ids\": inputs[\"input_ids\"].clone(),  # Should be [1, seq_len]\n",
    "                    \"past_key_values\": None,\n",
    "                    \"finished\": False,\n",
    "                    \"max_length\": inputs[\"input_ids\"].size(-1) + max_new_tokens,\n",
    "                    \"prompt_length\": inputs[\"input_ids\"].size(-1),\n",
    "                    \"original_prompt\": thread[\"text\"],  # Store original prompt for debugging\n",
    "                }\n",
    "                active_sequences.append(sequence)\n",
    "                sequence_id += 1\n",
    "                print(f\"\\nInitialized sequence {sequence_id}:\")\n",
    "                print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "                print(f\"Generated IDs shape: {sequence['generated_ids'].shape}\")\n",
    "                print(f\"Memory after sequence initialization: {get_memory_usage()}\")\n",
    "\n",
    "            if not active_sequences:\n",
    "                break\n",
    "\n",
    "            # Process sequences\n",
    "            initial_sequences = [seq for seq in active_sequences if seq[\"past_key_values\"] is None]\n",
    "            subsequent_sequences = [seq for seq in active_sequences if seq[\"past_key_values\"] is not None]\n",
    "\n",
    "            # Handle initial sequences\n",
    "            if initial_sequences:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"Processing initial sequences\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                max_input_length = max(seq[\"inputs\"][\"input_ids\"].size(-1) for seq in initial_sequences)\n",
    "                print(f\"Max input length: {max_input_length}\")\n",
    "                \n",
    "                # Prepare batched inputs\n",
    "                with batch_memory_manager():\n",
    "                    batch_inputs = prepare_batch_inputs(initial_sequences, max_input_length, processor, device)\n",
    "                    print(\"\\nPrepared batch inputs:\")\n",
    "                    for k, v in batch_inputs.items():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            print(f\"{k} shape: {v.shape}\")\n",
    "\n",
    "                    # Forward pass\n",
    "                    with torch.cuda.amp.autocast() if device.type == \"cuda\" else nullcontext():\n",
    "                        outputs = model(**batch_inputs)\n",
    "\n",
    "                    # Update sequences\n",
    "                    for idx, seq in enumerate(initial_sequences):\n",
    "                        # Extract past key values\n",
    "                        seq_past_key_values = [\n",
    "                            (outputs.past_key_values[layer_idx][0][idx:idx+1],\n",
    "                            outputs.past_key_values[layer_idx][1][idx:idx+1])\n",
    "                            for layer_idx in range(num_layers)\n",
    "                        ]\n",
    "                        seq[\"past_key_values\"] = seq_past_key_values\n",
    "\n",
    "                        # Get next token and ensure correct dimensions\n",
    "                        next_token_logits = outputs.logits[idx, -1, :]\n",
    "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "                        next_token = next_token.unsqueeze(0)  # Add batch dimension [1, 1]\n",
    "                        \n",
    "                        # Debug logging\n",
    "                        current_text = processor.tokenizer.decode(seq['generated_ids'].squeeze(), skip_special_tokens=True)\n",
    "                        next_token_text = processor.tokenizer.decode(next_token.squeeze())\n",
    "                        print(f\"\\nOriginal prompt: {seq['original_prompt']}\")\n",
    "                        print(f\"Previously generated: {current_text}\")\n",
    "                        print(f\"New token: '{next_token_text}' (id: {next_token.squeeze().item()})\")\n",
    "\n",
    "                        seq[\"generated_ids\"] = torch.cat([seq[\"generated_ids\"], next_token], dim=1)\n",
    "                        seq[\"inputs\"][\"attention_mask\"] = torch.cat([\n",
    "                            seq[\"inputs\"][\"attention_mask\"],\n",
    "                            torch.ones((1, 1), dtype=seq[\"inputs\"][\"attention_mask\"].dtype, device=device)\n",
    "                        ], dim=1)\n",
    "\n",
    "                        if next_token.squeeze().item() == eos_token_id or seq[\"generated_ids\"].shape[1] >= seq[\"max_length\"]:\n",
    "                            seq[\"finished\"] = True\n",
    "                            print(f\"Sequence {seq['id']} finished\")\n",
    "                        \n",
    "                        print(f\"Memory after processing sequence {seq['id']}: {get_memory_usage()}\")\n",
    "\n",
    "            # Handle subsequent sequences\n",
    "            if subsequent_sequences:\n",
    "                print(\"\\n\" + \"=\"*50)\n",
    "                print(\"Processing subsequent sequences\")\n",
    "                print(\"=\"*50)\n",
    "                \n",
    "                with batch_memory_manager():\n",
    "                    batch_results = process_subsequent_sequences(\n",
    "                        subsequent_sequences, model, processor, num_layers, device\n",
    "                    )\n",
    "                    \n",
    "                    for sequences, outputs in batch_results:\n",
    "                        for idx, seq in enumerate(sequences):\n",
    "                            print(f\"\\nSequence {seq['id']} Generation State:\")\n",
    "                            print(f\"Original prompt: {seq['original_prompt']}\")\n",
    "                            current_text = processor.tokenizer.decode(seq['generated_ids'].squeeze(), skip_special_tokens=True)\n",
    "                            print(f\"Currently generated: {current_text}\")\n",
    "                            \n",
    "                            # Update past key values\n",
    "                            seq[\"past_key_values\"] = [\n",
    "                                (outputs.past_key_values[layer_idx][0][idx:idx+1],\n",
    "                                outputs.past_key_values[layer_idx][1][idx:idx+1])\n",
    "                                for layer_idx in range(num_layers)\n",
    "                            ]\n",
    "                            \n",
    "                            # Ensure next_token has correct dimensions [1, 1]\n",
    "                            next_token = outputs.generated_tokens[idx].unsqueeze(0)\n",
    "                            if len(next_token.shape) == 1:\n",
    "                                next_token = next_token.unsqueeze(0)\n",
    "                                \n",
    "                            current_text = processor.tokenizer.decode(seq['generated_ids'].squeeze(), skip_special_tokens=True)\n",
    "                            next_token_text = processor.tokenizer.decode(next_token.squeeze())\n",
    "                            print(f\"\\nOriginal prompt: {seq['original_prompt']}\")\n",
    "                            print(f\"Previously generated: {current_text}\")\n",
    "                            print(f\"New token: '{next_token_text}' (id: {next_token.squeeze().item()})\")\n",
    "                            print(f\"Next token shape: {next_token.shape}\")\n",
    "                            \n",
    "                            # Concatenate along sequence length dimension (dim=1)\n",
    "                            seq[\"generated_ids\"] = torch.cat([seq[\"generated_ids\"], next_token], dim=1)\n",
    "                            print(f\"Updated generated_ids shape: {seq['generated_ids'].shape}\")\n",
    "                            \n",
    "                            seq[\"inputs\"][\"attention_mask\"] = torch.cat([\n",
    "                                seq[\"inputs\"][\"attention_mask\"],\n",
    "                                torch.ones((1, 1), dtype=seq[\"inputs\"][\"attention_mask\"].dtype, device=device)\n",
    "                            ], dim=1)\n",
    "\n",
    "                            if next_token.squeeze().item() == eos_token_id or seq[\"generated_ids\"].shape[1] >= seq[\"max_length\"]:\n",
    "                                seq[\"finished\"] = True\n",
    "                                print(f\"Sequence {seq['id']} finished\")\n",
    "                            \n",
    "                            print(f\"Memory after processing sequence {seq['id']}: {get_memory_usage()}\")\n",
    "\n",
    "            # Clean up finished sequences\n",
    "            newly_finished = [seq for seq in active_sequences if seq[\"finished\"]]\n",
    "            for seq in newly_finished:\n",
    "                # Explicitly clear tensors\n",
    "                for k in list(seq[\"inputs\"].keys()):\n",
    "                    if isinstance(seq[\"inputs\"][k], torch.Tensor):\n",
    "                        seq[\"inputs\"][k] = None\n",
    "                seq[\"past_key_values\"] = None\n",
    "                # Don't clear generated_ids yet as we need it for decoding\n",
    "                \n",
    "            # Update active sequences\n",
    "            active_sequences = [seq for seq in active_sequences if not seq[\"finished\"]]\n",
    "            finished_sequences.extend(newly_finished)\n",
    "            \n",
    "            print(f\"\\nActive sequences remaining: {len(active_sequences)}\")\n",
    "            print(f\"Finished sequences: {len(finished_sequences)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            print(f\"Memory after sequence cleanup: {get_memory_usage()}\")\n",
    "            \n",
    "            # Force garbage collection\n",
    "            gc.collect()\n",
    "\n",
    "        print(\"\\nSequence processing completed\")\n",
    "        print(f\"Final memory usage: {get_memory_usage()}\")\n",
    "    return finished_sequences\n",
    "\n",
    "def prepare_batch_inputs(sequences, max_length, processor, device):\n",
    "    \"\"\"Helper function to prepare batch inputs with consistent dimensions.\"\"\"\n",
    "    batch_inputs = {}\n",
    "    for key in sequences[0][\"inputs\"].keys():\n",
    "        if isinstance(sequences[0][\"inputs\"][key], torch.Tensor):\n",
    "            tensors = []\n",
    "            for seq in sequences:\n",
    "                if key in [\"input_ids\", \"attention_mask\"]:\n",
    "                    tensor = seq[\"inputs\"][key]\n",
    "                    if tensor.size(1) < max_length:\n",
    "                        padding_length = max_length - tensor.size(1)\n",
    "                        padding_value = 0 if key == \"attention_mask\" else processor.tokenizer.pad_token_id\n",
    "                        padding = torch.full(\n",
    "                            (tensor.size(0), padding_length),\n",
    "                            padding_value,\n",
    "                            dtype=tensor.dtype,\n",
    "                            device=device\n",
    "                        )\n",
    "                        tensor = torch.cat([tensor, padding], dim=1)\n",
    "                    tensors.append(tensor)\n",
    "                else:\n",
    "                    tensors.append(seq[\"inputs\"][key])\n",
    "            batch_inputs[key] = torch.cat(tensors, dim=0)\n",
    "        else:\n",
    "            batch_inputs[key] = sequences[0][\"inputs\"][key]\n",
    "\n",
    "    batch_inputs[\"use_cache\"] = True\n",
    "    return batch_inputs\n",
    "\n",
    "def decode_sequences(finished_sequences, processor):\n",
    "    \"\"\"Decode and print generated sequences.\"\"\"\n",
    "    results = []\n",
    "    for seq in finished_sequences:\n",
    "        generated_ids = seq[\"generated_ids\"].squeeze().cpu()\n",
    "        \n",
    "        # Decode full sequence and response\n",
    "        full_text = processor.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        response_ids = generated_ids[seq[\"prompt_length\"]:]\n",
    "        response_text = processor.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "        \n",
    "        results.append({\n",
    "            \"sequence_id\": seq[\"id\"],\n",
    "            \"full_text\": full_text,\n",
    "            \"response_text\": response_text,\n",
    "            \"total_tokens\": len(generated_ids),\n",
    "            \"response_tokens\": len(response_ids)\n",
    "        })\n",
    "        \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Load model and processor\n",
    "    model_name = \"allenai/Molmo-7B-D-0924\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    ).to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    # Print model configuration and debug processor output\n",
    "    print_model_config(model)\n",
    "    debug_output = debug_processor_output(processor)\n",
    "    print(\"\\nDebug output received. Proceeding with main script...\\n\")\n",
    "\n",
    "    # Example threads\n",
    "    pending_threads = [\n",
    "        {\n",
    "            \"image_url\": \"https://picsum.photos/id/237/536/354\",\n",
    "            \"text\": \"Describe this image.\"\n",
    "        },\n",
    "        {\n",
    "            \"image_url\": \"https://picsum.photos/id/238/536/354\",\n",
    "            \"text\": \"What do you see in this picture?\"\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Process sequences\n",
    "    finished_sequences = process_sequences(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        pending_threads=pending_threads,\n",
    "        device=device,\n",
    "        max_new_tokens=200,\n",
    "        max_batch_size=4\n",
    "    )\n",
    "\n",
    "    # Decode and print results\n",
    "    results = decode_sequences(finished_sequences, processor)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nGeneration Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    for result in results:\n",
    "        print(f\"\\nSequence {result['sequence_id']}:\")\n",
    "        print(f\"Full text:\\n{result['full_text']}\\n\")\n",
    "        print(f\"Response:\\n{result['response_text']}\\n\")\n",
    "        print(f\"Total tokens: {result['total_tokens']}\")\n",
    "        print(f\"Response tokens: {result['response_tokens']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Clean up CUDA cache\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
