"""ESMFold implementation for protein structure prediction using Meta AI's ESM-2 model."""

import logging
from dataclasses import dataclass
from typing import Optional, List, Union

import modal
import numpy as np

from . import app
from .base import FoldingAlgorithm, StructurePrediction, PredictionMetadata
from .images.esmfold import esmfold_image
from .images.volumes import model_weights
from .utils import MINUTES, MODEL_DIR
from .utils import Timer

# Set up basic logging configuration
logging.basicConfig(level=logging.INFO)


@dataclass
class ESMFoldOutput(StructurePrediction):
    """Output from ESMFold prediction including all model outputs."""

    # TODO: we should figure out what should be the verbosity of the output,
    # as a usual user does not need all of this information

    # Required by StructurePrediction protocol
    positions: np.ndarray  # (model_layer, batch_size, residue, atom=14, xyz=3)
    metadata: PredictionMetadata

    # Additional ESMFold-specific outputs
    frames: np.ndarray  # (model_layer, batch_size, residue, qxyz=7)
    sidechain_frames: np.ndarray  # (model_layer, batch_size, residue, ?, 4, 4) ??
    unnormalized_angles: np.ndarray  # (model_layer, batch_size, residue, 7, 2) ??
    angles: np.ndarray  # (model_layer, batch_size, residue, 7, 2) ??
    states: np.ndarray  # (model_layer, batch_size, residue, ???)
    s_s: np.ndarray  # (batch_size, residue, 1024)
    s_z: np.ndarray  # (batch_size, residue, residue, 128)
    distogram_logits: np.ndarray  # (batch_size, residue, residue, 64) ???
    lm_logits: np.ndarray  # (batch_size, residue, 23) ???
    aatype: np.ndarray  # (batch_size, residue) amino acid identity
    atom14_atom_exists: np.ndarray  # (batch_size, residue, atom=14)
    residx_atom14_to_atom37: np.ndarray  # (batch_size, residue, atom=14)
    residx_atom37_to_atom14: np.ndarray  # (batch_size, residue, atom=37)
    atom37_atom_exists: np.ndarray  # (batch_size, residue, atom=37)
    residue_index: np.ndarray  # (batch_size, residue)
    lddt_head: np.ndarray  # (model_layer, batch_size, residue, atom=37, 50) ??
    plddt: np.ndarray  # (batch_size, residue, residue, atom=37)
    ptm_logits: np.ndarray  # (batch_size, residue, residue, 64) ???
    ptm: np.ndarray  # float # TODO: make it into a float when sending to the client
    aligned_confidence_probs: np.ndarray  # (batch_size, residue, residue, 64)
    predicted_aligned_error: np.ndarray  # (batch_size, residue, residue)
    max_predicted_aligned_error: np.ndarray  # float # TODO: make it into a float when sending to the client
    chain_index: np.ndarray  # (batch_size, residue)
    pdb: Optional[list[str]] = None
    cif: Optional[list[str]] = None

    # TODO: can add a save method here (to a pickle and a pdb file) that can be run locally


with esmfold_image.imports():
    import torch
    from transformers import EsmForProteinFolding, AutoTokenizer


@app.cls(
    image=esmfold_image,
    gpu="T4",
    timeout=20 * MINUTES,
    container_idle_timeout=10 * MINUTES,
    volumes={MODEL_DIR: model_weights},
)
class ESMFold(FoldingAlgorithm):
    """ESMFold protein structure prediction model."""

    # TODO: maybe this config should be input to the fold function, so that it can
    # changed programmatically on a single ephermal app, rather than re-creating the app?
    DEFAULT_CONFIG = {
        "output_pdb": False,
        "output_cif": False,
    }
    GLYCINE_LINKER = "G" * 50
    POSITION_IDS_SKIP = 512

    # We need to properly asses whether using this or the original ESMFold is better
    # based on speed, accuracy, bugs, etc.; as well as customizability
    # For instance, if we want to also allow differently sized structure modules, than this would be good
    # TODO: we should add a settings dictionary or something, that would make it easier to add new options
    # TODO: maybe use OmegaConf instead to make it easier instead of config
    def __init__(self, config: Optional[dict] = None) -> None:
        """Initialize ESMFold."""
        super().__init__()
        self.config = self.DEFAULT_CONFIG.copy()
        if config is not None:
            self.config.update(config)
        self.metadata = self._initialize_metadata(
            model_name="ESMFold",
            model_version="v4.49.0",  # HuggingFace transformers version
        )

        self._linker_map: Optional[torch.Tensor] = None

    @modal.enter()
    def _load(self) -> None:
        """Load the ESMFold model and tokenizer."""
        self.tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained("facebook/esmfold_v1", cache_dir=MODEL_DIR)
        self.model: EsmForProteinFolding = EsmForProteinFolding.from_pretrained(
            "facebook/esmfold_v1", cache_dir=MODEL_DIR
        )
        self.device = "cuda"
        self.model = self.model.cuda()
        self.model.eval()
        self.model.trunk.set_chunk_size(64)
        self.ready = True

    @modal.method()
    def fold(self, sequences: Union[str, List[str]]) -> ESMFoldOutput:
        """Predict protein structure(s) using ESMFold."""
        if not hasattr(self, "tokenizer"):
            raise RuntimeError("Model not loaded. Call _load() first.")
        if not hasattr(self, "model"):
            raise RuntimeError("Model not loaded. Call _load() first.")

        if isinstance(sequences, str):
            sequences = [sequences]

        sequences = self._validate_sequences(sequences)
        self.metadata.sequence_lengths = self._compute_sequence_lengths(sequences)

        tokenized_input = self._tokenize_sequences(sequences)

        with Timer("Model Inference") as timer:
            with torch.inference_mode():
                outputs = self.model(**tokenized_input)

        outputs = self._convert_outputs(outputs, timer.duration)
        return outputs

    @staticmethod
    def _create_chain_indices(linker_map: torch.Tensor) -> List[List[int]]:
        """Create chain indices tensor marking different chains in the multimer.
        Only works for multimers, as that is assumed given that we are in this function.

        Args:
            linker_map: Tensor indicating linker positions (1) and chain positions (0)

        Returns:
            List[List[int]]: A list of lists where values indicate chain indices (0, 1, 2, etc.)
        """

        chain_indices = []

        if linker_map is None:
            raise RuntimeError("Linker map not stored. Call _store_linker_map() first.")

        assert isinstance(linker_map, torch.Tensor), "linker_map must be a tensor"
        mask = 1 - linker_map.unsqueeze(-1)

        for multimer in mask:
            current_chain_id = 0
            residue_indices = np.nonzero(multimer.squeeze(-1).cpu().numpy())[0].tolist()
            for i, res_idx in enumerate(residue_indices):
                if i == 0:
                    _multimer_chain_indices = [0]
                else:
                    if res_idx - residue_indices[i - 1] > 1:  # new chain
                        current_chain_id += 1
                    _multimer_chain_indices.append(int(current_chain_id))
            chain_indices.append(_multimer_chain_indices)
        return chain_indices

    def _tokenize_sequences(self, sequences: List[str]) -> torch.Tensor:
        if ":" in "".join(sequences):  # MULTIMER setting
            tokenized = self._tokenize_multimer(sequences)
        else:  # MONOMER setting
            tokenized = self.tokenizer(
                sequences, return_tensors="pt", add_special_tokens=False, padding=True, truncation=True, max_length=1024
            )
        tokenized = {k: v.to(self.device) for k, v in tokenized.items()}

        return tokenized

    @staticmethod
    def _compute_position_ids(_sequences: List[str], glycine_linker: str, position_ids_skip: int) -> torch.Tensor:
        position_ids = []
        for multimer_seq in _sequences:  # for every sequence in the batch
            # TODO: Should position_ids start with 0 or 1?
            # Stefano worked with 0, as that is what was used Meta's original implementation
            # Does it even matter though?
            multimer_position_ids = []
            previous_chain_end = 0
            for chain_id, chain_seq in enumerate(multimer_seq.split(":")):
                intrachain_position_ids = np.arange(len(chain_seq))
                if chain_id != 0:
                    intrachain_position_ids = (intrachain_position_ids + (previous_chain_end + 1)) + position_ids_skip

                # add linker if not last chain
                if chain_id != len(multimer_seq.split(":")) - 1:
                    linker_position_ids = np.arange(len(glycine_linker)) + intrachain_position_ids[-1] + 1
                    intrachain_position_ids = np.concatenate([intrachain_position_ids, linker_position_ids])

                previous_chain_end = intrachain_position_ids[-1]
                multimer_position_ids += intrachain_position_ids.tolist()
            position_ids.append(torch.tensor(multimer_position_ids))

        # add padding to the position ids
        max_length = max(len(ids) for ids in position_ids)
        for i, pos_ids in enumerate(position_ids):
            position_ids[i] = torch.cat([pos_ids, torch.zeros(max_length - len(pos_ids), dtype=torch.long)])
            # TODO: adding ZEROs might not be the best idea, but it works for now
        return torch.stack(position_ids)

    @staticmethod
    def _store_linker_map(_sequences: List[str], glycine_linker: str) -> torch.Tensor:
        """Store the linker map for the sequences.

        Args:
            _sequences: List of sequences, each containing chains separated by ":"
            glycine_linker: The glycine linker string used between chains

        Returns:
            torch.Tensor: A tensor of shape (batch_size, sequence_length) where 1 indicates
                        linker positions and 0 indicates chain positions
        """
        linker_map = []
        for seq in _sequences:
            # Initialize mask for the full sequence
            full_seq_len = len(seq.replace(":", glycine_linker))
            seq_mask = torch.zeros(full_seq_len, dtype=torch.long)

            # Find positions where linkers should be
            current_pos = 0
            chains = seq.split(":")

            for i, chain in enumerate(chains[:-1]):  # Process all chains except the last
                current_pos += len(chain)  # Move past the chain
                # Mark the linker positions as 1
                seq_mask[current_pos : current_pos + len(glycine_linker)] = 1
                current_pos += len(glycine_linker)  # Move past the linker

            linker_map.append(seq_mask)

        return torch.stack(linker_map)

    @staticmethod
    def _replace_glycine_linkers(_sequences: List[str], glycine_linker: str) -> List[str]:
        for i, multimer_seq in enumerate(_sequences):
            _sequences[i] = multimer_seq.replace(":", glycine_linker)  # replace : with glycine linkers
        return _sequences

    def _tokenize_multimer(self, sequences: List[str]) -> torch.Tensor:
        self._linker_map = self._store_linker_map(sequences, self.GLYCINE_LINKER).to(self.device)
        _sequences = self._replace_glycine_linkers(sequences, self.GLYCINE_LINKER).copy()
        tokenized = self.tokenizer(_sequences, return_tensors="pt", add_special_tokens=False)
        tokenized["position_ids"] = self._compute_position_ids(_sequences, self.GLYCINE_LINKER, self.POSITION_IDS_SKIP)
        return tokenized

    def _mask_linker_region(self, outputs: dict) -> dict:
        """Mask the linker region in the outputs.
        This includes all the metrics.

        Args:
            outputs: Dictionary containing model outputs
            sequences: List of input sequences

        Returns:
            dict: Updated outputs with linker regions masked
        """
        # Invert the linker map (0->1, 1->0)
        assert isinstance(self._linker_map, torch.Tensor), "linker_map must be a tensor"
        mask = 1 - self._linker_map.unsqueeze(-1)

        for multimer in mask:
            chain_indices = np.nonzero(multimer.squeeze(-1).cpu().numpy())[0].tolist()
            outputs["positions"] = outputs["positions"][
                :, :, chain_indices
            ]  # 3rd dim is residue index (for others, it differs)
            outputs["frames"] = outputs["frames"][:, :, chain_indices]
            outputs["sidechain_frames"] = outputs["sidechain_frames"][:, :, chain_indices]
            outputs["unnormalized_angles"] = outputs["unnormalized_angles"][:, :, chain_indices]
            outputs["angles"] = outputs["angles"][:, :, chain_indices]
            outputs["states"] = outputs["states"][:, :, chain_indices]
            outputs["s_s"] = outputs["s_s"][:, chain_indices]
            outputs["s_z"] = outputs["s_z"][:, chain_indices]
            outputs["distogram_logits"] = outputs["distogram_logits"][:, chain_indices]
            outputs["lm_logits"] = outputs["lm_logits"][:, chain_indices]
            outputs["aatype"] = outputs["aatype"][:, chain_indices]
            outputs["atom14_atom_exists"] = outputs["atom14_atom_exists"][:, chain_indices]
            outputs["residx_atom14_to_atom37"] = outputs["residx_atom14_to_atom37"][:, chain_indices]
            outputs["residx_atom37_to_atom14"] = outputs["residx_atom37_to_atom14"][:, chain_indices]
            outputs["atom37_atom_exists"] = outputs["atom37_atom_exists"][:, chain_indices]
            outputs["residue_index"] = outputs["residue_index"][:, chain_indices]
            outputs["lddt_head"] = outputs["lddt_head"][:, :, chain_indices]
            outputs["plddt"] = outputs["plddt"][:, chain_indices]
            outputs["ptm_logits"] = outputs["ptm_logits"][:, chain_indices, chain_indices]
            outputs["aligned_confidence_probs"] = outputs["aligned_confidence_probs"][:, chain_indices, chain_indices]
            outputs["predicted_aligned_error"] = outputs["predicted_aligned_error"][:, chain_indices, chain_indices]
        return outputs

    def _convert_outputs(self, outputs: dict, prediction_time: float) -> ESMFoldOutput:
        """Convert model outputs to ESMFoldOutput format."""

        outputs = {k: v.cpu().numpy() for k, v in outputs.items()}
        if self._linker_map is not None:
            # TODO: maybe add a proper MULTIMER flag?
            outputs = self._mask_linker_region(outputs)
            outputs["chain_index"] = np.array(self._create_chain_indices(self._linker_map), dtype=np.int32)
        else:
            outputs["chain_index"] = np.zeros(outputs["residue_index"].shape, dtype=np.int32)

        self.metadata.prediction_time = prediction_time

        if self.config["output_pdb"]:
            outputs["pdb"] = self._convert_outputs_to_pdb(outputs)
        if self.config["output_cif"]:
            outputs["cif"] = self._convert_outputs_to_cif(outputs)

        return ESMFoldOutput(metadata=self.metadata, **outputs)

    def _convert_outputs_to_pdb(self, outputs: dict) -> list[str]:
        from transformers.models.esm.openfold_utils.protein import to_pdb, Protein as OFProtein
        from transformers.models.esm.openfold_utils.feats import atom14_to_atom37

        final_atom_positions = atom14_to_atom37(outputs["positions"][-1], outputs)
        final_atom_mask = outputs["atom37_atom_exists"]
        pdbs = []

        for i in range(outputs["aatype"].shape[0]):
            aa = outputs["aatype"][i]
            pred_pos = final_atom_positions[i]
            mask = final_atom_mask[i]
            resid = outputs["residue_index"][i] + 1
            pred = OFProtein(
                aatype=aa,
                atom_positions=pred_pos,
                atom_mask=mask,
                residue_index=resid,
                b_factors=outputs["plddt"][i],
                chain_index=outputs["chain_index"][i] if "chain_index" in outputs else None,
            )
            pdbs.append(to_pdb(pred))

        return pdbs

    def _convert_outputs_to_cif(self, outputs: dict) -> list[str]:
        raise NotImplementedError("CIF conversion not implemented yet")
        from transformers.models.esm.openfold_utils.protein import to_modelcif, Protein as OFProtein
        from transformers.models.esm.openfold_utils.feats import atom14_to_atom37

        final_atom_positions = atom14_to_atom37(outputs["positions"][-1], outputs)
        final_atom_mask = outputs["atom37_atom_exists"]
        cifs = []
        for i in range(outputs["aatype"].shape[0]):
            aa = outputs["aatype"][i]
            pred_pos = final_atom_positions[i]
            mask = final_atom_mask[i]
            resid = outputs["residue_index"][i] + 1
            pred = OFProtein(
                aatype=aa,
                atom_positions=pred_pos,
                atom_mask=mask,
                residue_index=resid,
                b_factors=outputs["plddt"][i],
                chain_index=outputs["chain_index"][i] if "chain_index" in outputs else None,
            )
            cifs.append(to_modelcif(pred))

        return cifs
