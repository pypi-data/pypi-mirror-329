# Copyright (c) 2023 ING Analytics Wholesale Banking
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of
# this software and associated documentation files (the "Software"), to deal in
# the Software without restriction, including without limitation the rights to
# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
# the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

from __future__ import annotations

import gc
import re
from functools import partial, reduce

import pyspark
import pyspark.sql.functions as F
from pyspark.ml import Estimator, Model
from pyspark.ml.param.shared import HasInputCol, HasOutputCol
from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable
from pyspark.sql import DataFrame

from emm.helper.spark_custom_reader_writer import SparkReadable, SparkWriteable
from emm.helper.spark_utils import logical_repartitioning, set_spark_job_group, spark_checkpoint
from emm.indexing.spark_sni import SNIMatcherModel
from emm.loggers.logger import logger


class SparkCandidateSelectionEstimator(
    Estimator, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable
):
    """Unfitted Spark middleware class that aggregates candidate pairs of possible matches."""

    def __init__(
        self,
        indexers,
        index_col: str = "entity_id",
        uid_col: str = "uid",
        name_col: str = "preprocessed",
        force_execution: bool = False,
        unpersist_broadcast: bool = False,
        with_no_matches: bool = True,
        carry_on_cols: list[str] | None = None,
    ) -> None:
        """Unfitted Spark middleware class that aggregates name-pair candidates for possible matches.

        When fitted with ground truth names it returns SparkCandidateSelectionModel.

        CandidateSelectionEstimator is the second step of the SparkEntityMatching pipeline, after name preprocessing.
        The candidate selector aggregates name-pair candidates, which are generated by so-called `indexers`.
        The most important input is the list of indexers. Example Indexers are:

        - SparkCosSimIndexer()
        - SparkSortedNeighbourhoodIndexer()

        See indexers under `emm.indexing` for more details on usage.

        Args:
            indexers: list of indexing objects that will be used for generating candidates
            index_col: id column in dataframe
            uid_col: name of the unique id column that will be copied to the dataframe with candidates
            name_col: name column in dataframe
            force_execution: if true, force spark execution after each indexer's transform call.
            unpersist_broadcast: after indexer transform, free up memory that has been broadcast.
            with_no_matches: if true, for each name with no match add an artificial row.
            carry_on_cols: list of column names that should be copied to the dataframe with candidates (optional)

        Examples:
            >>> c = SparkCandidateSelectionEstimator(
            >>>     indexers=[SparkSortedNeighbourhoodIndexer(window_length=5)],
            >>> )
            >>> c.fit(ground_truth_sdf)
            >>> candidates_sdf = c.transform(names_sdf)

        """
        super().__init__()
        self.indexers = indexers
        self.index_col = index_col
        self.uid_col = uid_col
        self.name_col = name_col
        self.models = None
        self.candidate_selection_model = None
        self.force_execution = force_execution
        self.unpersist_broadcast = unpersist_broadcast
        self.with_no_matches = with_no_matches
        self.carry_on_cols = carry_on_cols

    def _fit(self, ground_truth_df: pyspark.sql.DataFrame) -> Model:
        """Fit the indexers to ground truth names

        For example this creates TFIDF matrices for the cosine similarity indexers.

        Args:
            ground_truth_df: ground truth dataframe with preprocessed names.

        Returns:
            fitted SparkCandidateSelectionModel
        """
        logger.info("SparkCandidateSelectionEstimator._fit(): %d indexers.", len(self.indexers))
        fitted_indexers = [idx.fit(ground_truth_df) for idx in self.indexers]
        candidate_selection_model = SparkCandidateSelectionModel(
            fitted_indexers=fitted_indexers,
            index_col=self.index_col,
            uid_col=self.uid_col,
            name_col=self.name_col,
            ground_truth_df=ground_truth_df,
            force_execution=self.force_execution,
            unpersist_broadcast=self.unpersist_broadcast,
            with_no_matches=self.with_no_matches,
            carry_on_cols=self.carry_on_cols,
        )
        self.candidate_selection_model = candidate_selection_model
        return candidate_selection_model


class SparkCandidateSelectionModel(
    Model, HasInputCol, HasOutputCol, SparkReadable, SparkWriteable, DefaultParamsReadable, DefaultParamsWritable
):
    """Fitted pipeline stage that aggregates candidates from multiple indexers."""

    SERIALIZE_ATTRIBUTES = (
        "index_col",
        "uid_col",
        "name_col",
        "num_partitions",
        "fitted_indexers",
        "ground_truth_df",
        "force_execution",
        "unpersist_broadcast",
        "with_no_matches",
        "carry_on_cols",
    )

    def __init__(
        self,
        fitted_indexers=None,
        index_col: str = "id",
        uid_col: str = "uid",
        name_col: str = "preprocessed",
        ground_truth_df=None,
        num_partitions=None,
        force_execution=False,
        unpersist_broadcast=False,
        with_no_matches=True,
        carry_on_cols=None,
    ) -> None:
        """Fitted pipeline stage that aggregates candidates from multiple indexers.

        See SparkCandidateSelectionEstimator for unfitted spark class and details on usage.

        Args:
            index_col: id column in dataframe
            uid_col: name of the unique id column that will be copied to the dataframe with candidates.
            name_col: name column in dataframe.
            fitted_indexers: list of fitted indexers that will be used for generating candidates.
               Alternatively accepts: fitted_indexer0, fitted_indexer1, fitted_indexer2, etc, picked up from kwargs.
            ground_truth_df: ground truth dataframe with preprocessed names.
            num_partitions: number of partitions for repartitioning. optional.
            force_execution: if true, force spark execution after each indexer's transform call.
            unpersist_broadcast: after indexer transform, free up memory that has been broadcast.
            with_no_matches: if true, for each name with no match add an artificial row.
            carry_on_cols: list of column names that should be copied to the dataframe with candidates (optional)

        """
        super().__init__()
        self.fitted_indexers = fitted_indexers or []
        self.index_col = index_col
        self.uid_col = uid_col
        self.name_col = name_col
        self.ground_truth_df = ground_truth_df
        self.num_partitions = num_partitions
        self.force_execution = force_execution
        self.unpersist_broadcast = unpersist_broadcast
        self.with_no_matches = with_no_matches
        self.carry_on_cols = carry_on_cols

        # check if ground truth needs setting for SNI indexers
        self._set_sni_ground_truth()

    def _set_sni_ground_truth(self):
        """Set ground truth for SNI indexers.

        This is needed if GT has not been persisted at serialization of SNI indexers,
        and these then are reloaded from disk. If so, update the GT from here for proper running.
        """
        for fitted_indexer in self.fitted_indexers:
            if isinstance(fitted_indexer, SNIMatcherModel) and not fitted_indexer.store_ground_truth:
                fitted_indexer.ground_truth_df = self.ground_truth_df

    def _transform(self, names_df: pyspark.sql.DataFrame) -> pyspark.sql.DataFrame:
        """Match processed names in names_df to the previously fitted ground truth.

        Args:
            names_df: Spark dataframe with preprocessed names that should be matched

        Returns:
            Spark dataframe with the candidate matches returned by indexers. Each row contains a single candidate name-pair.
            Columns `gt_uid`, `uid` contains index value from ground truth and X.
            Optionally id column (specified by `self.uid_col`) are copied from gt/X dataframes with the prefix: `gt_` or ``.
            Any additional columns calculated by indexers are also preserved (i.e. score).
        """
        matcheds = []
        for i, idx in enumerate(self.fitted_indexers):
            matched_df = idx.transform(names_df)

            # Handling the case when cosine_similarity == False
            if "gt_uid" in matched_df.columns:
                matched_df = matched_df.withColumnRenamed("indexer_score", f"score_{i}")
                matched_df = matched_df.withColumnRenamed("indexer_rank", f"rank_{i}")

            if self.force_execution:
                logger.info("SparkCandidateSelectionModel._transform: force execution of indexer %d.", i)
                matched_df = matched_df.cache()
                _ = matched_df.count()
            if self.unpersist_broadcast and hasattr(idx, "_unpersist"):
                # execution is done, free up any memory (eg. of tfidf memory)
                idx._unpersist()
            matcheds.append(matched_df)

        set_spark_job_group("CandidateSelectionModel._transform()", f"len(indexers)={len(self.fitted_indexers)}")
        logger.info("SparkCandidateSelectionModel._transform: %d indexers.", len(self.fitted_indexers))

        full_matched_df = reduce(partial(DataFrame.unionByName, allowMissingColumns=True), matcheds)
        logger.debug("CandidateSelection._transform schema after unionByName")

        # merge the same candidate pairs that come from different indexers
        if len(self.fitted_indexers) > 1:
            columns = [f"score_{i}" for i in range(len(self.fitted_indexers))]
            columns += [f"rank_{i}" for i in range(len(self.fitted_indexers))]
            full_matched_df = full_matched_df.groupby(self.uid_col, "gt_uid").agg(*(F.max(c).alias(c) for c in columns))

        # full_matched_df has here a schema like that, with 1 column per indexer:
        # root
        # |-- uid: long (nullable = true)
        # |-- gt_uid: long (nullable = true)
        # |-- score_0: float (nullable = true)
        # |-- score_1: float (nullable = true)
        # |-- score_2: float (nullable = true)

        # Handling the case when cosine_similarity == False
        if "gt_uid" in full_matched_df.columns:
            # Join ground_truth information
            full_matched_df, _ = join_ground_truth_info(
                full_matched_df, self.ground_truth_df, self.index_col, self.uid_col, self.carry_on_cols
            )

            # Join back for names_to_match columns
            # In principle all columns get joined (takes care of carry_on_cols)
            # this join is also taking care of the situation when we have no candidates
            for c in names_df.columns:
                if re.match(r"^(score|rank)_\d+$", c) or re.match(r"^gt_(uid|name|entity_id)$", c):
                    # get rid of conflicting columns like score_*, rank_* and gt_*
                    names_df = names_df.drop(c)
            join_how = "leftouter" if self.with_no_matches else "inner"
            full_matched_df = names_df.join(full_matched_df, on=self.uid_col, how=join_how)

            # Free some space before the checkpoint
            gc.collect()

            # checkpointing is added to avoid recalculation of indexers scores
            # (double check that spark.sparkContext.setCheckpointDir has been used)
            full_matched_df = spark_checkpoint(full_matched_df)

        # Repartitioning to have consistent names_to_match partitions,
        # This is necessary for rank features, account aggregation and Spark memory parallelism
        full_matched_df = logical_repartitioning(full_matched_df, self.uid_col, self.num_partitions)

        if self.force_execution:
            logger.info("SparkCandidateSelectionModel._transform : force execution of combined matches.")
            full_matched_df = full_matched_df.cache()
            _ = full_matched_df.count()

        return full_matched_df

    def increase_window_by_one_step(self):
        """Utility function for negative sample creation during training"""
        for fitted_indexer in self.fitted_indexers:
            fitted_indexer.increase_window_by_one_step()

    def decrease_window_by_one_step(self):
        """Utility function for negative sample creation during training"""
        for fitted_indexer in self.fitted_indexers:
            fitted_indexer.decrease_window_by_one_step()


def join_ground_truth_info(matched_df, ground_truth_df, index_col, uid_col, carry_on_cols=None):
    # Prepare right side of the join, to add the following columns to each candidate: gt_entity_id, gt_name, gt_preprocessed and gt_em_feature:
    ground_truth_candidates_df = ground_truth_df
    ground_truth_candidates_df = ground_truth_candidates_df.withColumnRenamed(uid_col, "gt_uid")
    # make the gt_entity_id column, if same as uid duplicate column, if not same rename:
    if uid_col == index_col:
        ground_truth_candidates_df = ground_truth_candidates_df.withColumn("gt_entity_id", F.col("gt_uid"))
    else:
        ground_truth_candidates_df = ground_truth_candidates_df.withColumnRenamed(index_col, "gt_entity_id")
    # preprocessed -> gt_preprocessed
    ground_truth_candidates_df = ground_truth_candidates_df.withColumnRenamed("preprocessed", "gt_preprocessed")
    # name -> gt_name
    ground_truth_candidates_df = ground_truth_candidates_df.withColumnRenamed("name", "gt_name")
    ground_truth_add_col = ["gt_entity_id", "gt_name", "gt_preprocessed"]
    if "country" in ground_truth_candidates_df.columns:
        ground_truth_candidates_df = ground_truth_candidates_df.withColumnRenamed("country", "gt_country")
        ground_truth_add_col.append("gt_country")
    # keep all carry-on columns that are found
    if isinstance(carry_on_cols, list):
        for col in carry_on_cols:
            if col in ground_truth_candidates_df.columns:
                ground_truth_candidates_df = ground_truth_candidates_df.withColumnRenamed(col, f"gt_{col}")
                ground_truth_add_col.append(f"gt_{col}")

    ground_truth_candidates_df = ground_truth_candidates_df.select(["gt_uid", *ground_truth_add_col])

    # Perform the candidate join
    matched_df = matched_df.join(ground_truth_candidates_df, "gt_uid")

    return matched_df, ground_truth_add_col
