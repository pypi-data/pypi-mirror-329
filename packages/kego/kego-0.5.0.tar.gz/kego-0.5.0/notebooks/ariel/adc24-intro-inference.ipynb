{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.002836,
     "end_time": "2024-08-14T20:04:16.638828",
     "exception": false,
     "start_time": "2024-08-14T20:04:16.635992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ariel Data Challenge 2024: Introductory model: inference\n",
    "\n",
    "In this notebook, we compute test predictions using the model saved in [ADC24 Intro training](https://www.kaggle.com/code/ambrosm/adc24-intro-training).\n",
    "\n",
    "<img width=\"700\" src=\"https://www.ariel-datachallenge.space/static/images/transit_situation.png\" />\n",
    "\n",
    "This image has been taken from [last year's competition](https://www.ariel-datachallenge.space/ML/documentation/about). It shows how a planet transits in front of its star and how this transit maps to the lightcurve (a dip in the brightness of the star). This dip is directly proportional to the ratio of the areas of the planet and star. It's this ratio (the \"transit depth\") that we are modeling in the present notebook.\n",
    "\n",
    "The present notebook is simple:\n",
    "- It reads the pre- and postprocessing code, which is the same as the code used for training.\n",
    "- It reads the test data.\n",
    "- It reads the saved model.\n",
    "- It executes the prediction pipeline and saves the submission file.\n",
    "\n",
    "The real work was done in the [training notebook](https://www.kaggle.com/code/ambrosm/adc24-intro-training)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 2.508435,
     "end_time": "2024-08-14T20:04:19.149736",
     "exception": false,
     "start_time": "2024-08-14T20:04:16.641301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 0.029395,
     "end_time": "2024-08-14T20:04:19.181954",
     "exception": false,
     "start_time": "2024-08-14T20:04:19.152559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FOLDER_COMPETITION = os.environ[\"PATH_EFOLDER\"] + \"ariel-data-challenge-2024/\"\n",
    "# directory = \"/kaggle/input/adc24-intro-training/\"\n",
    "directory = \"\"\n",
    "\n",
    "exec(open(directory + \"f_read_and_preprocess.py\", \"r\").read())\n",
    "exec(open(directory + \"a_read_and_preprocess.py\", \"r\").read())\n",
    "exec(open(directory + \"feature_engineering.py\", \"r\").read())\n",
    "exec(open(directory + \"postprocessing.py\", \"r\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.002169,
     "end_time": "2024-08-14T20:04:19.186676",
     "exception": false,
     "start_time": "2024-08-14T20:04:19.184507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "People have been asking how to choose a good value for sigma_pred. As explained in [Understanding the competition metric](https://www.kaggle.com/competitions/ariel-data-challenge-2024/discussion/528114), with sigma_pred we indicate what root mean squared error (rmse) we expect for our test predictions.\n",
    "\n",
    "The training data cover planets of only two stars (stars 0 and 1), but the test data include planets of other stars.\n",
    "\n",
    "This leads to the following recipe:\n",
    "- For known stars (stars 0 and 1), we expect the test rmse to be equal to our cross-validation rmse, i.e. we predict the out-of-fold rmse of our model (0.000293 as shown in the training notebook).\n",
    "- For unknown stars, the prediction error can only be higher. We thus predict a higher value (0.001 in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 3.14272,
     "end_time": "2024-08-14T20:04:22.331939",
     "exception": false,
     "start_time": "2024-08-14T20:04:19.189219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "wavelengths = pd.read_csv(FOLDER_COMPETITION + \"wavelengths.csv\")\n",
    "test_adc_info = pd.read_csv(\n",
    "    FOLDER_COMPETITION + \"test_adc_info.csv\", index_col=\"planet_id\"\n",
    ")\n",
    "f_raw_test = f_read_and_preprocess(\"test\", test_adc_info, test_adc_info.index)\n",
    "a_raw_test = a_read_and_preprocess(\"test\", test_adc_info, test_adc_info.index)\n",
    "test = feature_engineering(f_raw_test, a_raw_test)\n",
    "\n",
    "# Load the model\n",
    "with open(directory + \"model.pickle\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "with open(directory + \"sigma_pred.pickle\", \"rb\") as f:\n",
    "    sigma_pred = pickle.load(f)\n",
    "\n",
    "# Predict\n",
    "test_pred = model.predict(test)\n",
    "\n",
    "# Package into submission file\n",
    "sub_df = postprocessing(\n",
    "    test_pred,\n",
    "    test_adc_info.index,\n",
    "    sigma_pred=np.tile(\n",
    "        np.where(test_adc_info[[\"star\"]] <= 1, sigma_pred, 0.001), (1, 283)\n",
    "    ),\n",
    ")\n",
    "display(sub_df)\n",
    "sub_df.to_csv(\"submission.csv\")\n",
    "#!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 0.003059,
     "end_time": "2024-08-14T20:04:22.338556",
     "exception": false,
     "start_time": "2024-08-14T20:04:22.335497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9188054,
     "sourceId": 70367,
     "sourceType": "competition"
    },
    {
     "sourceId": 191155259,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.152807,
   "end_time": "2024-08-14T20:04:22.963694",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-14T20:04:13.810887",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
