{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer NER baseline [lb 0.881]\n",
    "\n",
    "The following is a basic script to train and run inference using `transformers` using 2x T4 GPUs. You might get better performance if you use a bigger model, or one that has already been trained on NER.\n",
    "\n",
    "It includes processing to correctly map the given tokens with labels during training and vice versa when running inference.\n",
    "\n",
    "\n",
    "Update 1: Thanks to @takanashihumbert, I switched `tokens` for `token_map` which helped improve the score from 0.569 to 0.854!  \n",
    "Update 2: [Thanks to @ysteinnygaard](https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-881/comments#2659393), I localized the `triplets` variable to each doc. Apparently, there are multiple entities with the same text and token idx across different documents! Score went from 0.854 --> 0.881\n",
    "\n",
    "\n",
    "deberta-v3-small: lb 0.576  \n",
    "deberta-v3-base: lb 0.569 (before update) --> 0.854 (after update 1) --> 0.881 (after update 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING = False  # be sure to turn internet off if doing inference\n",
    "\n",
    "TRAINING_MODEL_PATH = \"microsoft/deberta-v3-large\"\n",
    "TRAINING_MAX_LENGTH = 512\n",
    "\n",
    "INFERENCE_MODEL_PATH = \"/kaggle/input/pii-data-detection-baseline/output/checkpoint-240\"\n",
    "INFERENCE_MAX_LENGTH = 2000\n",
    "\n",
    "# if TRAINING:\n",
    "#     !pip install seqeval evaluate -q\n",
    "#     !pip install -U datasets accelerate transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    import json\n",
    "\n",
    "    data = json.load(\n",
    "        open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\")\n",
    "    )\n",
    "\n",
    "    print(len(data))\n",
    "    print(data[0].keys())\n",
    "\n",
    "    x = data[0]\n",
    "\n",
    "    print(x[\"tokens\"][:10])\n",
    "    print(x[\"labels\"][:10])\n",
    "    print(x[\"trailing_whitespace\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    from itertools import chain\n",
    "\n",
    "    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "    label2id = {l: i for i, l in enumerate(all_labels)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "    id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    from transformers import AutoTokenizer\n",
    "    import numpy as np\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n",
    "\n",
    "    def tokenize(example, tokenizer, label2id):\n",
    "        text = []\n",
    "\n",
    "        # these are at the character level\n",
    "        labels = []\n",
    "\n",
    "        for t, l, ws in zip(\n",
    "            example[\"tokens\"],\n",
    "            example[\"provided_labels\"],\n",
    "            example[\"trailing_whitespace\"],\n",
    "        ):\n",
    "\n",
    "            text.append(t)\n",
    "            labels.extend([l] * len(t))\n",
    "\n",
    "            # if there is trailing whitespace\n",
    "            if ws:\n",
    "                text.append(\" \")\n",
    "                labels.append(\"O\")\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            \"\".join(text), return_offsets_mapping=True, truncation=False\n",
    "        )\n",
    "\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        text = \"\".join(text)\n",
    "        token_labels = []\n",
    "\n",
    "        for start_idx, end_idx in tokenized.offset_mapping:\n",
    "\n",
    "            # CLS token\n",
    "            if start_idx + end_idx == 0:\n",
    "                token_labels.append(label2id[\"O\"])\n",
    "                continue\n",
    "\n",
    "            # case when token starts with whitespace\n",
    "            if text[start_idx].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            while start_idx >= len(labels):\n",
    "                start_idx -= 1\n",
    "\n",
    "            token_labels.append(label2id[labels[start_idx]])\n",
    "\n",
    "        length = len(tokenized.input_ids)\n",
    "\n",
    "        return {**tokenized, \"labels\": token_labels, \"length\": length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    from datasets import Dataset\n",
    "\n",
    "    ds = Dataset.from_dict(\n",
    "        {\n",
    "            \"full_text\": [x[\"full_text\"] for x in data],\n",
    "            \"document\": [x[\"document\"] for x in data],\n",
    "            \"tokens\": [x[\"tokens\"] for x in data],\n",
    "            \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "            \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    ds = ds.map(\n",
    "        tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id}, num_proc=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    # Confirm that alignment is good\n",
    "\n",
    "    # run multiple times to see different rows\n",
    "    x = ds.shuffle()[0]\n",
    "\n",
    "    for t, l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n",
    "        if l != \"O\":\n",
    "            print((t, l))\n",
    "\n",
    "    print(\"*\" * 100)\n",
    "\n",
    "    for t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n",
    "        if id2label[l] != \"O\":\n",
    "            print((t, id2label[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are some long ones that will get truncated when training if you use a typical max_length\n",
    "\n",
    "There might be some key labels that are at the end that are being missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.hist(ds[\"length\"], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    from collections import Counter\n",
    "\n",
    "    group = []\n",
    "    labels = []\n",
    "\n",
    "    group_thresholds = [0, 50, 100, 200, 500, 1000, 2000, 10000]\n",
    "\n",
    "    for sample_labels in ds[\"provided_labels\"]:\n",
    "        for i, label in enumerate(sample_labels):\n",
    "            if label != \"O\":\n",
    "                for j in range(1, len(group_thresholds)):\n",
    "                    lower = group_thresholds[j - 1]\n",
    "                    upper = group_thresholds[j]\n",
    "\n",
    "                    if lower <= i < upper:\n",
    "                        group.append(f\"{lower}-{upper}\")\n",
    "                        labels.append(label)\n",
    "                        break\n",
    "\n",
    "    pairs = list(zip(labels, group))\n",
    "\n",
    "    counts = Counter(pairs)\n",
    "\n",
    "    data = {\n",
    "        \"label\": [],\n",
    "        \"count\": [],\n",
    "        \"range\": [],\n",
    "    }\n",
    "\n",
    "    for (label, range_), count in counts.items():\n",
    "        data[\"label\"].append(label)\n",
    "        data[\"range\"].append(range_)\n",
    "        data[\"count\"].append(count)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    px.scatter(df, x=\"range\", y=\"count\", color=\"label\", log_y=True, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from tokenizers import AddedToken\n",
    "import evaluate\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/468844\n",
    "def filter_no_pii(example, percent_allow=0.2):\n",
    "    # Return True if there is PII\n",
    "    # Or 20% of the time if there isn't\n",
    "    \n",
    "    has_pii = set(\"O\") != set(example[\"provided_labels\"])\n",
    "    \n",
    "    return has_pii or (random.random() < percent_allow)\n",
    "\n",
    "def tokenize(example, tokenizer, label2id, max_length):\n",
    "    text = []\n",
    "    labels = []\n",
    "    \n",
    "    for t, l, ws in zip(example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        labels.extend([l]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            labels.append(\"O\")\n",
    "    \n",
    "    \n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    text = \"\".join(text)\n",
    "    token_labels = []\n",
    "    \n",
    "    for start_idx, end_idx in tokenized.offset_mapping:\n",
    "        \n",
    "        # CLS token\n",
    "        if start_idx == 0 and end_idx == 0: \n",
    "            token_labels.append(label2id[\"O\"])\n",
    "            continue\n",
    "        \n",
    "        # case when token starts with whitespace\n",
    "        if text[start_idx].isspace():\n",
    "            start_idx += 1\n",
    "        \n",
    "        while start_idx >= len(labels):\n",
    "            start_idx -= 1\n",
    "            \n",
    "        token_labels.append(label2id[labels[start_idx]])\n",
    "        \n",
    "    length = len(tokenized.input_ids)\n",
    "        \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"labels\": token_labels,\n",
    "        \"length\": length\n",
    "    }\n",
    "    \n",
    "def compute_metrics(p, metric, all_labels):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    # Unpack nested dictionaries\n",
    "    final_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            for n, v in value.items():\n",
    "                final_results[f\"{key}_{n}\"] = v\n",
    "        else:\n",
    "            final_results[key] = value\n",
    "    return final_results   \n",
    "\n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"--model_path\", type=str)\n",
    "    parser.add_argument(\"--max_length\", type=int)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n",
    "\n",
    "\n",
    "    all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\n",
    "    label2id = {l: i for i,l in enumerate(all_labels)}\n",
    "    id2label = {v:k for k,v in label2id.items()}\n",
    "\n",
    "\n",
    "    ds = Dataset.from_dict({\n",
    "        \"full_text\": [x[\"full_text\"] for x in data],\n",
    "        \"document\": [x[\"document\"] for x in data],\n",
    "        \"tokens\": [x[\"tokens\"] for x in data],\n",
    "        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "        \"provided_labels\": [x[\"labels\"] for x in data],\n",
    "    })\n",
    "\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
    "    \n",
    "    # lots of newlines in the text\n",
    "    # adding this should be helpful\n",
    "    tokenizer.add_tokens(AddedToken(\"\\n\", normalized=False))\n",
    "    \n",
    "    ds = ds.filter(\n",
    "        filter_no_pii,\n",
    "        num_proc=2,\n",
    "    )\n",
    "    \n",
    "    ds = ds.map(\n",
    "        tokenize, \n",
    "        fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": args.max_length}, \n",
    "        num_proc=2,\n",
    "    )\n",
    "\n",
    "\n",
    "    metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(args.model_path, num_labels=len(all_labels), id2label=id2label, label2id=label2id)\n",
    "    model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=16)\n",
    "\n",
    "    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \"output\", \n",
    "        fp16=True, \n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=4, \n",
    "        report_to=\"none\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        logging_steps=5,\n",
    "        metric_for_best_model=\"overall_recall\",\n",
    "        greater_is_better=True,\n",
    "#         gradient_checkpointing=True,\n",
    "        num_train_epochs=1,\n",
    "        dataloader_num_workers=1,\n",
    "    )\n",
    "\n",
    "    # may want to try to balance classes in splits\n",
    "    final_ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=args, \n",
    "        train_dataset=final_ds[\"train\"], \n",
    "        eval_dataset=final_ds[\"test\"], \n",
    "        data_collator=collator, \n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(compute_metrics, metric=metric, all_labels=all_labels),\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAINING:\n",
    "    # utilize both t4 gpus\n",
    "    !accelerate launch --num_processes 2 --mixed_precision=fp16 run.py \\\n",
    "      --model_path $TRAINING_MODEL_PATH \\\n",
    "      --max_length $TRAINING_MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile infer.py\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "from itertools import chain\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(example, tokenizer, max_length):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        \n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "            \n",
    "        \n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n",
    "    \n",
    "        \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument(\"--model_path\", type=str)\n",
    "    parser.add_argument(\"--max_length\", type=int)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n",
    "    \n",
    "    ds = Dataset.from_dict({\n",
    "        \"full_text\": [x[\"full_text\"] for x in data],\n",
    "        \"document\": [x[\"document\"] for x in data],\n",
    "        \"tokens\": [x[\"tokens\"] for x in data],\n",
    "        \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "    })\n",
    "\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_path)\n",
    "    ds = ds.map(\n",
    "        tokenize, \n",
    "        fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": args.max_length}, \n",
    "        num_proc=2,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(args.model_path)\n",
    "\n",
    "    collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        \".\", \n",
    "        per_device_eval_batch_size=4, \n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        args=args, \n",
    "        data_collator=collator, \n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    predictions = trainer.predict(ds).predictions\n",
    "\n",
    "    ds.to_parquet(\"test_ds.pq\")\n",
    "    \n",
    "    np.save(\"preds.npy\", predictions)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    \n",
    "    !accelerate launch --num_processes 2 infer.py \\\n",
    "      --model_path $INFERENCE_MODEL_PATH \\\n",
    "      --max_length $INFERENCE_MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall is much more important than precision, so it might make sense to make predictions even if they aren't the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "\n",
    "    import numpy as np\n",
    "    import json\n",
    "    from datasets import Dataset\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "\n",
    "    config = json.load(open(Path(INFERENCE_MODEL_PATH) / \"config.json\"))\n",
    "\n",
    "    id2label = config[\"id2label\"]\n",
    "\n",
    "    preds = np.load(\"preds.npy\")\n",
    "\n",
    "    ds = Dataset.from_parquet(\"test_ds.pq\")\n",
    "\n",
    "    preds = preds.argmax(-1)\n",
    "\n",
    "    document, token, label, token_str = [], [], [], []\n",
    "    for p, token_map, offsets, tokens, doc in zip(\n",
    "        preds, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]\n",
    "    ):\n",
    "\n",
    "        triplets = []\n",
    "        for token_pred, (start_idx, end_idx) in zip(p, offsets):\n",
    "            label_pred = id2label[str(token_pred)]\n",
    "\n",
    "            if start_idx + end_idx == 0:\n",
    "                continue\n",
    "\n",
    "            if token_map[start_idx] == -1:\n",
    "                start_idx += 1\n",
    "\n",
    "            # ignore \"\\n\\n\"\n",
    "            while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                start_idx += 1\n",
    "\n",
    "            if start_idx >= len(token_map):\n",
    "                break\n",
    "\n",
    "            token_id = token_map[start_idx]\n",
    "\n",
    "            # ignore \"O\" predictions and whitespace preds\n",
    "            if label_pred != \"O\" and token_id != -1:\n",
    "                triplet = (label_pred, token_id, tokens[token_id])\n",
    "\n",
    "                if triplet not in triplets:\n",
    "                    document.append(doc)\n",
    "                    token.append(token_id)\n",
    "                    label.append(label_pred)\n",
    "                    token_str.append(tokens[token_id])\n",
    "                    triplets.append(triplet)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\"document\": document, \"token\": token, \"label\": label, \"token_str\": token_str}\n",
    "    )\n",
    "\n",
    "    df[\"row_id\"] = list(range(len(df)))\n",
    "\n",
    "    display(df.head(50))\n",
    "\n",
    "    df[[\"row_id\", \"document\", \"token\", \"label\"]].to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4319117,
     "sourceId": 7429898,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
