{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Device\n",
    "\n",
    "print(Device.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import Tensor, nn\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.l1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
    "        self.l2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
    "        self.l3 = nn.Linear(1600, 10)\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        x = self.l1(x).relu().max_pool2d((2, 2))\n",
    "        x = self.l2(x).relu().max_pool2d((2, 2))\n",
    "        return self.l3(x.flatten(1).dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad.nn.datasets import mnist\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = mnist()\n",
    "print(X_train.shape, X_train.dtype, Y_train.shape, Y_train.dtype)\n",
    "# (60000, 1, 28, 28) dtypes.uchar (60000,) dtypes.uchar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "acc = (model(X_test).argmax(axis=1) == Y_test).mean()\n",
    "# NOTE: tinygrad is lazy, and hasn't actually run anything by this point\n",
    "print(acc.item())  # ~10% accuracy, as expected from a random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = nn.optim.Adam(nn.state.get_parameters(model))\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "def step():\n",
    "    Tensor.training = True  # makes dropout work\n",
    "    samples = Tensor.randint(batch_size, high=X_train.shape[0])\n",
    "    X, Y = X_train[samples], Y_train[samples]\n",
    "    optim.zero_grad()\n",
    "    loss = model(X).sparse_categorical_crossentropy(Y).backward()\n",
    "    optim.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "timeit.repeat(step, repeat=5, number=1)\n",
    "# [0.08268719699981375,\n",
    "# 0.07478952900009972,\n",
    "# 0.07714716600003158,\n",
    "# 0.07785399599970333,\n",
    "# 0.07605237000007037]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import TinyJit\n",
    "\n",
    "jit_step = TinyJit(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "timeit.repeat(jit_step, repeat=5, number=1)\n",
    "# [0.2596786549997887,\n",
    "#  0.08989566299987928,\n",
    "#  0.0012115650001760514,\n",
    "#  0.001010227999813651,\n",
    "#  0.0012164899999334011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(7000):\n",
    "    loss = jit_step()\n",
    "    if step % 100 == 0:\n",
    "        Tensor.training = False\n",
    "        acc = (model(X_test).argmax(axis=1) == Y_test).mean().item()\n",
    "        print(f\"step {step:4d}, loss {loss.item():.2f}, acc {acc*100.:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
