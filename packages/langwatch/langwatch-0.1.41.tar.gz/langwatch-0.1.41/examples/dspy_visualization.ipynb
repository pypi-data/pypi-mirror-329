{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangWatch DSPy Visualizer\n",
        "\n",
        "This notebook shows an example of a simple DSPy optimization process integrated with LangWatch for training visualization and debugging.\n",
        "\n",
        "[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/langwatch/langwatch/blob/main/python-sdk/examples/dspy_visualization.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgy1Fjhh_lOB"
      },
      "outputs": [],
      "source": [
        "# Install langwatch along with dspy for the visualization\n",
        "!pip install dspy-ai langwatch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51OWavv1CCVV"
      },
      "source": [
        "## Preparing the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xycw8IWs_qnt",
        "outputId": "40844780-608a-4162-cac7-35f57c5764f1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OPENAI_API_KEY: \")\n",
        "\n",
        "import dspy\n",
        "import openai\n",
        "\n",
        "llm = dspy.OpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    max_tokens=2048,\n",
        "    temperature=0,\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "\n",
        "print(\"LLM test response:\", llm(\"hello there\"))\n",
        "\n",
        "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
        "dspy.settings.configure(lm=llm, rm=colbertv2_wiki17_abstracts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIAYLNlcCFdO"
      },
      "source": [
        "## Preparing the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXH8qF-QBcEJ",
        "outputId": "203d4516-b9a2-4748-8fcc-7dd64be1342d"
      },
      "outputs": [],
      "source": [
        "from dspy.datasets import HotPotQA\n",
        "\n",
        "# Load the dataset.\n",
        "dataset = HotPotQA(train_seed=1, train_size=32, eval_seed=2023, dev_size=50, test_size=0)\n",
        "\n",
        "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
        "trainset = [x.with_inputs('question') for x in dataset.train]\n",
        "devset = [x.with_inputs('question') for x in dataset.dev]\n",
        "\n",
        "len(trainset), len(devset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOXtqnmfCNzS"
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxAaf1IABgxM",
        "outputId": "03155b63-bbaf-4ceb-bf26-07cda4f52b6f"
      },
      "outputs": [],
      "source": [
        "class GenerateAnswer(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "\n",
        "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")\n",
        "\n",
        "\n",
        "class RAG(dspy.Module):\n",
        "    def __init__(self, num_passages=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.retrieve = dspy.Retrieve(k=num_passages)\n",
        "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
        "\n",
        "    def forward(self, question):\n",
        "        context = self.retrieve(question).passages\n",
        "        prediction = self.generate_answer(context=context, question=question)\n",
        "        return dspy.Prediction(context=context, answer=prediction.answer)\n",
        "\n",
        "\n",
        "dev_example = devset[18]\n",
        "print(f\"[Devset] Question: {dev_example.question}\")\n",
        "print(f\"[Devset] Answer: {dev_example.answer}\")\n",
        "print(f\"[Devset] Relevant Wikipedia Titles: {dev_example.gold_titles}\")\n",
        "\n",
        "generate_answer = RAG()\n",
        "\n",
        "pred = generate_answer(question=dev_example.question)\n",
        "\n",
        "# Print the input and the prediction.\n",
        "print(f\"[Prediction] Question: {dev_example.question}\")\n",
        "print(f\"[Prediction] Predicted Answer: {pred.answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytbRU9jJCSj8"
      },
      "source": [
        "## Login to LangWatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF9DxTGeCU15",
        "outputId": "29ebbacf-9ca8-4322-fd77-32e47f4c93aa"
      },
      "outputs": [],
      "source": [
        "import langwatch\n",
        "\n",
        "langwatch.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o69S-BlkE-bV"
      },
      "source": [
        "## Start Training Session!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef67q2B-FCIP",
        "outputId": "cefe1935-1ef3-46ad-a54c-74f08bfb75f0"
      },
      "outputs": [],
      "source": [
        "from dspy.teleprompt import MIPROv2\n",
        "import dspy.evaluate\n",
        "\n",
        "# Define our metric validation\n",
        "def validate_context_and_answer(example, pred, trace=None):\n",
        "    answer_EM = dspy.evaluate.answer_exact_match(example, pred)\n",
        "    answer_PM = dspy.evaluate.answer_passage_match(example, pred)\n",
        "    return answer_EM and answer_PM\n",
        "\n",
        "# Set up a MIPROv2 optimizer, which will compile our RAG program.\n",
        "optimizer = MIPROv2(metric=validate_context_and_answer, prompt_model=llm, task_model=llm, num_candidates=2, init_temperature=0.7)\n",
        "\n",
        "# Initialize langwatch for this run, to track the optimizer compilation\n",
        "langwatch.dspy.init(experiment=\"my-awesome-experiment\", optimizer=optimizer)\n",
        "\n",
        "# Compile\n",
        "compiled_rag = optimizer.compile( RAG(),\n",
        "    trainset=trainset,\n",
        "    num_batches=10,\n",
        "    max_bootstrapped_demos=3,\n",
        "    max_labeled_demos=5,\n",
        "    eval_kwargs=dict(num_threads=16, display_progress=True, display_table=0),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_rag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5vA80_JJX-q"
      },
      "outputs": [],
      "source": [
        "compiled_rag.save(\"optimized_model.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
