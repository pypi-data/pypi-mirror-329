{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Prepare decision tree classifier in python using sklearn library for data set\n",
    "# diabetes.csv - (pregnant, glucose, bp, skin, insulin, bmi, pedigree, age, label). \n",
    "# Use all features except label as independent variable. \n",
    "# Use complete dataset for training and limit the depth of tree upto 3 levels and plot using\n",
    "# tree.plotTree()method.\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier,plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data=pd.read_csv('Diabetes.csv')\n",
    "x=data.drop(columns='Outcome')\n",
    "y=data['Outcome']\n",
    "\n",
    "clf=DecisionTreeClassifier(max_depth=3,random_state=42)\n",
    "clf.fit(x,y)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(clf,feature_names=x.columns,class_names=['No diabetes','diabetes'],filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# Using the diabetes dataset, implement a Decision Tree Classifier and \n",
    "# determine the importance of each feature in predicting diabetes. \n",
    "# Generate different plots to justify your model.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Diabetes.csv\")\n",
    "\n",
    "# Split dataset into features and target variable\n",
    "X = df.drop(columns=[\"Outcome\"])\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "# Feature Importance Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=clf.feature_importances_, y=X.columns, palette=\"viridis\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance in Decision Tree Classifier\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Diabetes\", \"Diabetes\"], yticklabels=[\"No Diabetes\", \"Diabetes\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the Decision Tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(clf, feature_names=X.columns, class_names=[\"No Diabetes\", \"Diabetes\"], filled=True, rounded=True)\n",
    "plt.title(\"Decision Tree Visualization\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Implement a Decision Tree Classifier and compare its performance with a SVM Classifier on the iris dataset. \n",
    "# Display the accuracy of both models.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Train an SVM Classifier\n",
    "svm_clf = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Print Accuracy\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4\n",
    "# Build decision tree classifier for iris data set. One with maximum leaf nodes up to 8 and \n",
    "# another one with minimum sample per leaf as 5. Compare accuracy of both models.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data # Features (sepal length, sepal width, petal length, petal width)\n",
    "y = iris.target # Target (setosa, versicolor, virginica)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# 1. Create the first Decision Tree Classifier with a maximum of 8 leaf nodes\n",
    "dtc_max_leaf = DecisionTreeClassifier(max_leaf_nodes=8, random_state=42)\n",
    "dtc_max_leaf.fit(X_train, y_train)\n",
    "\n",
    "# 2. Create the second Decision Tree Classifier with a minimum of 5 samples per leaf\n",
    "dtc_min_samples_leaf = DecisionTreeClassifier(min_samples_leaf=5, random_state=42)\n",
    "dtc_min_samples_leaf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the models using accuracy score\n",
    "y_pred_max_leaf = dtc_max_leaf.predict(X_test)\n",
    "y_pred_min_samples_leaf = dtc_min_samples_leaf.predict(X_test)\n",
    "accuracy_max_leaf = accuracy_score(y_test, y_pred_max_leaf)\n",
    "accuracy_min_samples_leaf = accuracy_score(y_test, y_pred_min_samples_leaf)\n",
    "\n",
    "# Visualize the first decision tree (Max Leaf Nodes = 8)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(dtc_max_leaf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True)\n",
    "plt.title(\"Decision Tree Classifier (Max Leaf Nodes = 8)\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize the second decision tree (Min Samples Per Leaf = 5)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(dtc_min_samples_leaf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True)\n",
    "plt.title(\"Decision Tree Classifier (Min Samples Per Leaf = 5)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# Implement a Decision Tree Classifier and compare its performance with a\n",
    "# Logistic Classifier on the diabetes dataset. Develop Python code to display the accuracy of both models.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Diabetes.csv\")\n",
    "\n",
    "# Split dataset into features and target variable\n",
    "X = df.drop(columns=[\"Outcome\"])\n",
    "y = df[\"Outcome\"]\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Train a Logistic Regression Classifier\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred_lr = lr_clf.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Print Accuracy\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# Develop a random dataset using 3 columns and 350 rows. \n",
    "# Generate the data frame and apply decision tree classifier to train the model. \n",
    "# Compare the accuracy of the model with any 2 other models you can apply on the generated dataset. \n",
    "# Generate plot of comparing the accuracy for different models.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate random dataset\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"Feature1\": np.random.randint(350) * 10,  # Random values between 0 and 10\n",
    "    \"Feature2\": np.random.rand(350) * 20,  # Random values between 0 and 20\n",
    "    \"Feature3\": np.random.rand(350) * 30,  # Random values between 0 and 30\n",
    "    \"Target\": np.random.choice([0, 1], size=350)  # Binary classification\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split dataset into features and target variable\n",
    "X = df.drop(columns=[\"Target\"])\n",
    "y = df[\"Target\"]\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Decision Tree Classifier\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Train a Logistic Regression Classifier\n",
    "lr_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "y_pred_lr = lr_clf.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Train an SVM Classifier\n",
    "svm_clf = SVC(kernel=\"linear\", random_state=42)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "y_pred_svm = svm_clf.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "\n",
    "# Compare Accuracies\n",
    "model_names = [\"Decision Tree\", \"Logistic Regression\", \"SVM\"]\n",
    "accuracies = [dt_accuracy, lr_accuracy, svm_accuracy]\n",
    "\n",
    "# Plot accuracy comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=model_names, y=accuracies, palette=\"viridis\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)  # Accuracy ranges between 0 and 1\n",
    "plt.title(\"Comparison of Model Accuracies\")\n",
    "plt.show()\n",
    "\n",
    "# Print accuracy scores\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.2f}\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "# Using the diabetes dataset, implement a Decision Tree Classifier and \n",
    "# plot ccp_alpha of built tree against any one DT parameter (node count or maximum depth).\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the diabetes dataset\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=[\"Outcome\"])  # Independent variables\n",
    "y = df[\"Outcome\"]  # Target variable\n",
    "\n",
    "# Train Decision Tree Classifier with different ccp_alpha values\n",
    "ccp_alphas = np.linspace(0, 0.02, 20)  # Varying ccp_alpha values\n",
    "depths = []  # To store max depth for each ccp_alpha\n",
    "nodes = []  # To store node count for each ccp_alpha\n",
    "\n",
    "for alpha in ccp_alphas:\n",
    "    model = DecisionTreeClassifier(random_state=42, ccp_alpha=alpha)\n",
    "    model.fit(X, y)\n",
    "    depths.append(model.get_depth())  # Store max depth\n",
    "    nodes.append(model.tree_.node_count)  # Store number of nodes\n",
    "\n",
    "# Plot ccp_alpha vs Maximum Depth\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ccp_alphas, depths, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel(\"ccp_alpha\")\n",
    "plt.ylabel(\"Maximum Depth\")\n",
    "plt.title(\"ccp_alpha vs Maximum Depth\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "# Write a Python script that implements the Elbow Method \n",
    "# to determine the optimal number of clusters for a generated dataset. (Generate dataset of your choice with 14 points)\n",
    "import numpy as np\n",
    "X = np.random.rand(14, 2) * 10\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "elbow=[]\n",
    "for i in range(1,10):\n",
    "  km=KMeans(n_clusters=i)\n",
    "  km.fit(x)\n",
    "  elbow.append(km.inertia_)\n",
    "plt.plot(range(1,10),elbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "# Generate a random dataset of urban areas with features such as \n",
    "# population density, average income, and amenities, (min. 100 data). \n",
    "# Apply K-medoids clustering to identify neighbourhoods with similar characteristics. Visualize the clusters on a map.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate random dataset (100 urban areas)\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"Latitude\": np.random.uniform(40.5, 40.9, 100),\n",
    "    \"Longitude\": np.random.uniform(-74.0, -73.7, 100),\n",
    "    \"Population Density\": np.random.randint(5000, 30000, 100),\n",
    "    \"Average Income\": np.random.randint(20000, 100000, 100),\n",
    "    \"Amenities\": np.random.randint(5, 50, 100),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize features\n",
    "X = df[[\"Population Density\", \"Average Income\", \"Amenities\"]]\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply K-Medoids clustering\n",
    "kmedoids = KMedoids(n_clusters=4, random_state=42, init=\"random\")\n",
    "df[\"Cluster\"] = kmedoids.fit_predict(X_scaled)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=df[\"Population Density\"], y=df[\"Average Income\"], hue=df[\"Cluster\"], palette=\"Set2\")\n",
    "plt.xlabel(\"Population Density\")\n",
    "plt.ylabel(\"Average Income\")\n",
    "plt.title(\"K-Medoids Clustering\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "# Generate a random dataset containing customer information (e.g., age, spending score)(100 rows), \n",
    "# apply K-medoids clustering to segment customers into distinct groups. \n",
    "# Visualize the clusters and summarize the characteristics of each segment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# Generate random dataset (100 customers)\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"Age\": np.random.randint(18, 70, 100),\n",
    "    \"Spending Score\": np.random.randint(1, 100, 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize features\n",
    "X_scaled = StandardScaler().fit_transform(df)\n",
    "\n",
    "# Apply K-Medoids clustering\n",
    "kmedoids = KMedoids(n_clusters=4, random_state=42, init=\"random\")\n",
    "df[\"Cluster\"] = kmedoids.fit_predict(X_scaled)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=df[\"Age\"], y=df[\"Spending Score\"], hue=df[\"Cluster\"], palette=\"Set2\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Spending Score\")\n",
    "plt.title(\"Customer Segmentation using K-Medoids\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Summarize characteristics of each segment\n",
    "summary = df.groupby(\"Cluster\").agg({\"Age\": [\"mean\", \"min\", \"max\"], \"Spending Score\": [\"mean\", \"min\", \"max\"]})\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11\n",
    "# You are working with an e-commerce company that wants to segment its customers based on their purchasing behaviour. \n",
    "# Generate random dataset that includes features like total spending, frequency of purchases, and average basket size. \n",
    "# Develop a Python program to perform K-means clustering. \n",
    "# Visualize the clusters and summarize the characteristics of each segment.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate random dataset (100 customers)\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"Total Spending\": np.random.randint(100, 5000, 100),\n",
    "    \"Frequency of Purchases\": np.random.randint(1, 50, 100),\n",
    "    \"Average Basket Size\": np.random.randint(10, 500, 100)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Standardize features\n",
    "X_scaled = StandardScaler().fit_transform(df)\n",
    "\n",
    "# Apply K-Means clustering\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=df[\"Total Spending\"], y=df[\"Frequency of Purchases\"], hue=df[\"Cluster\"], palette=\"Set2\")\n",
    "plt.xlabel(\"Total Spending\")\n",
    "plt.ylabel(\"Frequency of Purchases\")\n",
    "plt.title(\"Customer Segmentation using K-Means\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Summarize characteristics of each segment\n",
    "summary = df.groupby(\"Cluster\").agg({\"Total Spending\": [\"mean\", \"min\", \"max\"], \n",
    "                                     \"Frequency of Purchases\": [\"mean\", \"min\", \"max\"], \n",
    "                                     \"Average Basket Size\": [\"mean\", \"min\", \"max\"]})\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12\n",
    "# Use make_blobs to generate 300 samples. Analyse the data and apply KMeans clustering and \n",
    "# generate the graph for better representation. \n",
    "# Show what happens if you change n_clusters count from 2 to 7(2,3,4,5,6,7)?\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Step 1: Generate synthetic dataset with 2 features\n",
    "X, y = make_blobs(n_samples=300, centers=2, cluster_std=0.60, random_state=42)\n",
    "\n",
    "# Step 2: Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2)  # Specify number of clusters\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Step 3: Get the cluster centers and labels\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Step 4: Visualize the clusters and the cluster centers\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "\n",
    "# Plot the cluster centers\n",
    "plt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
    "\n",
    "plt.title('K-Means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13\n",
    "# A grocery store wants to analyze customer purchase patterns to improve product placements. \n",
    "# Generate random transaction data (products purchased together), \n",
    "# apply K-means clustering to group similar transactions.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate synthetic transaction data (300 transactions, 2 product categories)\n",
    "np.random.seed(42)\n",
    "X = np.random.randint(0, 10, size=(300, 2))  # Products purchased together\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster centers and labels\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Visualize the clusters and the cluster centers\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "\n",
    "# Plot the cluster centers\n",
    "plt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
    "\n",
    "plt.title('K-Means Clustering for Grocery Transactions')\n",
    "plt.xlabel('Product Category 1 Purchases')\n",
    "plt.ylabel('Product Category 2 Purchases')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14\n",
    "# Generate random dataset with 35 points. \n",
    "# Apply KMeans clustering and generate the plot with each cluster points with different colours. \n",
    "# What is the impact of varying cluster count? \n",
    "# Justify your understanding with suitable graphs.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic dataset with 35 points and 2 features\n",
    "X, _ = make_blobs(n_samples=35, centers=3, cluster_std=0.80, random_state=42)\n",
    "\n",
    "# Function to apply KMeans clustering and plot results\n",
    "def plot_kmeans_clusters(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', s=100, alpha=0.7)\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
    "    plt.title(f'K-Means Clustering (k={n_clusters})')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for different cluster counts\n",
    "for k in [2, 3, 4, 5]:\n",
    "    plot_kmeans_clusters(X, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15\n",
    "# Develop a Python program to implement DBSCAN using the sklearn library. Use a 2D dataset (ex. make_moons) for clustering.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset (moons shape)\n",
    "X, _ = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "# Standardize the dataset\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Plot the DBSCAN clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "plt.title('DBSCAN Clustering on make_moons Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "# Generate a random dataset with 5 columns and use 3rd and 4th column as data to implement DBSCAN. \n",
    "# Consider eps=5 and minPts=5.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate random dataset (100 samples, 5 columns)\n",
    "np.random.seed(42)\n",
    "data = np.random.randint(0, 100, size=(100, 5))\n",
    "\n",
    "# Extract 3rd and 4th columns for clustering\n",
    "X = data[:, 2:4]\n",
    "\n",
    "# Standardize the dataset\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Plot the DBSCAN clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1],c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "# plt.scatter(X[labels==-1,0],X[labels==-1,1],s=20,color='black')\n",
    "# plt.scatter(X[labels==0,0],X[labels==0,1],s=20,color='blue')\n",
    "# plt.scatter(X[labels==1,0],X[labels==1,1],s=20,color='red')\n",
    "# plt.scatter(X[labels==2,0],X[labels==2,1],s=20,color='grey')\n",
    "# plt.scatter(X[labels==3,0],X[labels==3,1],s=20,color='green')\n",
    "plt.title('DBSCAN Clustering using 3rd and 4th Columns')\n",
    "plt.xlabel('Feature 3')\n",
    "plt.ylabel('Feature 4')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17\n",
    "# Use DBSCAN to cluster customers based on their purchasing behaviour \n",
    "# (ex. frequency of purchases, average transaction amount)(Generate random dataset with min. 100 customers). \n",
    "# Develop a program that analyses the clusters to identify different customer segments for targeted marketing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate random dataset for 100 customers\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"CustomerID\": np.arange(1, 101),\n",
    "    \"Frequency_of_Purchases\": np.random.randint(1, 50, 100),  # Number of purchases\n",
    "    \"Avg_Transaction_Amount\": np.random.randint(10, 500, 100)  # Average amount spent\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract relevant features for clustering\n",
    "X = df[[\"Frequency_of_Purchases\", \"Avg_Transaction_Amount\"]]\n",
    "\n",
    "# Standardize the dataset\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=1, min_samples=5)\n",
    "df[\"Cluster\"] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df[\"Frequency_of_Purchases\"], df[\"Avg_Transaction_Amount\"], c=df[\"Cluster\"], cmap='viridis', s=50, alpha=0.7)\n",
    "plt.xlabel(\"Frequency of Purchases\")\n",
    "plt.ylabel(\"Average Transaction Amount\")\n",
    "plt.title(\"DBSCAN Clustering of Customers\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze customer segments\n",
    "cluster_summary = df.groupby(\"Cluster\")[[\"Frequency_of_Purchases\", \"Avg_Transaction_Amount\"]].mean()\n",
    "print(cluster_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18\n",
    "# Generate a data frame containing two features(Spending,features) for 100 data. \n",
    "# Apply DBSCAN and do parametric analysis for eps value and show the impact on the performance of model via graph.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate random dataset with 100 data points\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    \"Spending\": np.random.randint(10, 500, 100),\n",
    "    \"Features\": np.random.randint(1, 50, 100)\n",
    "})\n",
    "\n",
    "# Extract features for clustering\n",
    "X = df[[\"Spending\", \"Features\"]]\n",
    "\n",
    "# Standardize the dataset\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Different epsilon values for analysis\n",
    "eps_values = [0.5, 1, 2, 5]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, eps in enumerate(eps_values, 1):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    # Plot clustering result\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(df[\"Spending\"], df[\"Features\"], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "    plt.xlabel(\"Spending\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.title(f\"DBSCAN Clustering (eps={eps})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19\n",
    "# Generate a random dataset with 4 columns and use 1st and 3rd column as data to implement DBSCAN. \n",
    "# What if eps is changed from 1 to 5? \n",
    "# How it will affect the performance of the model? What is the impact of minPts on the performance of the model? \n",
    "# Justify it with graph by changing minPts.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate random dataset (100 samples, 4 columns)\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    \"Feature1\": np.random.randint(10, 100, 100),\n",
    "    \"Feature2\": np.random.randint(20, 200, 100),\n",
    "    \"Feature3\": np.random.randint(5, 50, 100),\n",
    "    \"Feature4\": np.random.randint(1, 10, 100)\n",
    "})\n",
    "\n",
    "# Extract 1st and 3rd columns for clustering\n",
    "X = df[[\"Feature1\", \"Feature3\"]]\n",
    "\n",
    "# Standardize the dataset\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Analyze the impact of changing eps values\n",
    "eps_values = [1, 2, 3, 5]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, eps in enumerate(eps_values, 1):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(df[\"Feature1\"], df[\"Feature3\"], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "    plt.xlabel(\"Feature1\")\n",
    "    plt.ylabel(\"Feature3\")\n",
    "    plt.title(f\"DBSCAN Clustering (eps={eps})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze the impact of changing minPts values\n",
    "min_samples_values = [3, 5, 7, 10]\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, min_pts in enumerate(min_samples_values, 1):\n",
    "    dbscan = DBSCAN(eps=2, min_samples=min_pts)\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.scatter(df[\"Feature1\"], df[\"Feature3\"], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "    plt.xlabel(\"Feature1\")\n",
    "    plt.ylabel(\"Feature3\")\n",
    "    plt.title(f\"DBSCAN Clustering (minPts={min_pts})\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20\n",
    "# Generate random dataset for mall_customer dataset with features like Customer, Genre, Age, Annual Imcome, Spending Score. \n",
    "# Consider minimum 200 random samples for analysis. Apply DBSCAN as well as agglomerative clustering and \n",
    "# generate the dendrogram. Plot points in each cluster for DBSCAN with different colours.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.cluster.hierarchy as shc\n",
    "\n",
    "# Generate random dataset (200 samples)\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    \"CustomerID\": np.arange(1, 201),\n",
    "    \"Genre\": np.random.choice([\"Male\", \"Female\"], 200),\n",
    "    \"Age\": np.random.randint(18, 70, 200),\n",
    "    \"Annual_Income\": np.random.randint(15000, 100000, 200),\n",
    "    \"Spending_Score\": np.random.randint(1, 100, 200)\n",
    "})\n",
    "\n",
    "# Convert categorical 'Genre' column to numerical\n",
    "df[\"Genre\"] = df[\"Genre\"].map({\"Male\": 0, \"Female\": 1})\n",
    "\n",
    "# Extract relevant features for clustering\n",
    "X = df[[\"Age\", \"Annual_Income\", \"Spending_Score\"]]\n",
    "\n",
    "# Standardize the dataset\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=1, min_samples=5)\n",
    "df[\"DBSCAN_Cluster\"] = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Plot DBSCAN Clustering\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df[\"Annual_Income\"], df[\"Spending_Score\"], c=df[\"DBSCAN_Cluster\"], cmap='viridis', s=50, alpha=0.7)\n",
    "plt.xlabel(\"Annual Income\")\n",
    "plt.ylabel(\"Spending Score\")\n",
    "plt.title(\"DBSCAN Clustering of Mall Customers\")\n",
    "plt.colorbar(label=\"Cluster\")\n",
    "plt.show()\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=5)\n",
    "df[\"Agglomerative_Cluster\"] = agg_clustering.fit_predict(X_scaled)\n",
    "\n",
    "# Generate Dendrogram for Agglomerative Clustering\n",
    "plt.figure(figsize=(10, 6))\n",
    "linked = shc.linkage(X_scaled, method='ward')\n",
    "shc.dendrogram(linked)\n",
    "plt.title(\"Dendrogram for Agglomerative Clustering\")\n",
    "plt.xlabel(\"Customers\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21\n",
    "# Write a Python code to perform k-fold cross-validation on the dataset using a Support Vector Machine (SVM) classifier. \n",
    "# Use the following steps:\n",
    "# • Import necessary libraries.\n",
    "# • Prepare the dataset synthetically using make_classification.\n",
    "# • Preprocess the data if necessary (e.g., scaling).\n",
    "# • Implement k-fold cross-validation with 10 folds.\n",
    "# • Train the SVM classifier on the training set and evaluate it on the test set for each fold.\n",
    "# • Print the accuracy for each fold and the average accuracy across all folds.\n",
    "# Dataset Description:\n",
    "# • The dataset consists of 100 samples with 20 features.\n",
    "# The target variable is binary (0 or 1).\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Step 2: Preprocess the data (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Initialize SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Step 4: Implement 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(svm_classifier, X_scaled, y, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Step 5: Print the accuracy for each fold and the average accuracy\n",
    "for i, score in enumerate(scores, 1):\n",
    "    print(f\"Fold {i}: Accuracy = {score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage Accuracy: {np.mean(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22\n",
    "# Write a Python code to perform bootstrap sampling on a dataset using a Random Forest Classifier. \n",
    "# The dataset consists of features and labels as specified below.\n",
    "# Dataset Description:\n",
    "# • The dataset consists of 150 samples with 10 features.\n",
    "# • The target variable is categorical with three classes (0, 1, 2).\n",
    "# Instructions:\n",
    "# • Import the necessary libraries.\n",
    "# • Load or create a synthetic dataset with the specified characteristics.\n",
    "# • Implement bootstrap sampling to create multiple samples from the original dataset.\n",
    "# • Train a Random Forest Classifier on each bootstrap sample.\n",
    "# • Evaluate the model on the original dataset and print the accuracy for each bootstrap sample.\n",
    "# • Calculate and print the average accuracy across all bootstrap samples.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Step 1: Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=150, n_features=10, n_classes=3, n_informative=8, random_state=42)\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(10)])\n",
    "df[\"Target\"] = y\n",
    "\n",
    "# Step 2: Perform bootstrap sampling and train Random Forest\n",
    "n_bootstrap_samples = 10  # Number of bootstrap iterations\n",
    "accuracies = []\n",
    "\n",
    "for i in range(n_bootstrap_samples):\n",
    "    # Create a bootstrap sample\n",
    "    df_sample = resample(df, replace=True, n_samples=len(df), random_state=i)\n",
    "    \n",
    "    # Split into features and target\n",
    "    X_sample = df_sample.drop(columns=[\"Target\"])\n",
    "    y_sample = df_sample[\"Target\"]\n",
    "    \n",
    "    # Train Random Forest Classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_classifier.fit(X_sample, y_sample)\n",
    "    \n",
    "    # Evaluate on the original dataset\n",
    "    y_pred = rf_classifier.predict(X)\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Bootstrap Sample {i+1}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Step 3: Print average accuracy\n",
    "print(f\"\\nAverage Accuracy across Bootstrap Samples: {np.mean(accuracies):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23\n",
    "# Write a Python code to perform Leave-One-Out Cross-Validation (LOOCV) on a dataset using a Decision Tree Classifier. \n",
    "# The dataset consists of features and labels as specified below.\n",
    "# Dataset Description:\n",
    "# • The dataset consists of 100 samples with 5 features.\n",
    "# • The target variable is binary (0 or 1).\n",
    "# Instructions:\n",
    "#     • Import the necessary libraries.\n",
    "# • Load or create a synthetic dataset with the specified characteristics.\n",
    "# • Implement Leave-One-Out Cross-Validation (LOOCV).\n",
    "# • Train a Decision Tree Classifier on each training set and evaluate it on the left-out sample.\n",
    "# Print the accuracy for each iteration and the average accuracy across all iterations.\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_classes=2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "accuracies = []\n",
    "\n",
    "# Step 3: Perform LOOCV with Decision Tree Classifier\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on left-out sample\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Iteration {len(accuracies)}: Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# Step 4: Print average accuracy\n",
    "print(f\"\\nAverage Accuracy across LOOCV iterations: {np.mean(accuracies):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24\n",
    "# Write a Python code to perform Leave-One-Out Cross-Validation (LOOCV) on a dataset using a Decision Tree Classifier. \n",
    "# The dataset consists of features and labels as specified below.\n",
    "# Dataset Description:\n",
    "# • The dataset consists of 150 samples with 10 features.\n",
    "# • The target variable is categorical with three classes (0, 1, 2).\n",
    "# • Introduce some noise to the dataset to make the classification task more\n",
    "# Instructions:\n",
    "# • Import the necessary libraries.\n",
    "# • Load or create a synthetic dataset with the specified characteristics (make_classification), including noise.\n",
    "# • Implement Leave-One-Out Cross-Validation (LOOCV).\n",
    "# • Train a Decision Tree Classifier on each training set and evaluate it on the left-out sample.\n",
    "# • Calculate and print the accuracy for each iteration, as well as the confusion matrix for the final model.\n",
    "# • Calculate and print the average accuracy across all iterations.\n",
    "# Print confusion matrix and classificarion report.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Generate synthetic dataset with noise\n",
    "X, y = make_classification(\n",
    "    n_samples=150, n_features=10, n_classes=3, n_informative=7, n_redundant=2, \n",
    "    n_clusters_per_class=1, flip_y=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "accuracies = []\n",
    "y_true, y_pred_all = [], []\n",
    "\n",
    "# Perform LOOCV\n",
    "for train_index, test_index in loo.split(X):\n",
    "    # Split dataset\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train Decision Tree Classifier\n",
    "    model = DecisionTreeClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on left-out sample\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    # Store true labels and predictions for final confusion matrix\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred_all.append(y_pred[0])\n",
    "\n",
    "# Calculate and print overall accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy across LOOCV iterations: {average_accuracy:.4f}\")\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_all)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# Print classification report\n",
    "class_report = classification_report(y_true, y_pred_all)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25\n",
    "# Write a Python code to perform Stratified K-Fold Cross-Validation on a dataset using a Random Forest Classifier. \n",
    "# The dataset consists of features and labels as specified below.\n",
    "# Dataset Description:\n",
    "# • The dataset consists of 200 samples with 15 features.\n",
    "# • The target variable is categorical with four classes (0, 1, 2, 3).\n",
    "# Instructions:\n",
    "# • Import the necessary libraries.\n",
    "# • Load or create a synthetic dataset with the specified characteristics, ensuring class imbalance.\n",
    "# • Implement Stratified K-Fold Cross-Validation with 5 folds.\n",
    "# • Train a Random Forest Classifier on each training set and evaluate it on the validation set.\n",
    "# • Calculate and print the accuracy for each fold, as well as the average accuracy across all folds.\n",
    "# Generate and visualize the classification report for the final model, \n",
    "# including precision, recall, and F1-score for each class.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate synthetic dataset with class imbalance\n",
    "X, y = make_classification(\n",
    "    n_samples=200, n_features=15, n_classes=4, n_informative=10, \n",
    "    n_redundant=3, weights=[0.5, 0.2, 0.2, 0.1], random_state=42\n",
    ")\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "y_true_all, y_pred_all = [], []\n",
    "\n",
    "# Perform Stratified K-Fold Cross-Validation\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Train Random Forest Classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    print(f\"Fold {fold} Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Store true and predicted labels for final classification report\n",
    "    y_true_all.extend(y_test)\n",
    "    y_pred_all.extend(y_pred)\n",
    "\n",
    "# Calculate and print average accuracy\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"\\nAverage Accuracy across Stratified K-Folds: {average_accuracy:.4f}\")\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(y_true_all, y_pred_all)\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "\n",
    "# Visualize classification report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26\n",
    "# Write a Python code to perform a Grid Search with Cross-Validation on the Iris dataset using \n",
    "# a Support Vector Machine (SVM) classifier. \n",
    "# The goal is to find the best hyperparameters for the SVM model.Dataset Description:\n",
    "# • The Iris dataset consists of 150 samples with 4 features.\n",
    "# • The target variable is categorical with three classes (Setosa, Versicolor, Virginica).\n",
    "# Instructions:\n",
    "# • Import the necessary libraries.\n",
    "# • Load the Iris dataset from sklearn.datasets.\n",
    "# • Split the dataset into features and labels.\n",
    "# • Implement a Grid Search with 5-fold Cross-Validation to find the best hyperparameters for the SVM classifier. \n",
    "# Consider the following hyperparameters: C: [0.1, 1, 10, 100] kernel: ['linear', 'rbf'] gamma: ['scale', 'auto']\n",
    "# • Print the best hyperparameters found by the Grid Search.\n",
    "# • Evaluate the best model on the entire dataset and print the accuracy.\n",
    "# Generate and visualize the confusion matrix for the best model.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target  # Features and Labels\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Perform Grid Search with 5-Fold Cross-Validation\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X)\n",
    "\n",
    "# Evaluate and print accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(f\"Accuracy of Best Model: {accuracy:.4f}\")\n",
    "\n",
    "# Generate and print classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))\n",
    "\n",
    "# Generate and visualize confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27\n",
    "# Write a Python program to apply Principal Component Analysis (PCA) on the Iris dataset \n",
    "# and reduce its dimensionality to 2 dimensions. \n",
    "# You should use sklearn's PCA implementation. \n",
    "# After reducing the dimensions, visualize the transformed data in a 2D scatter plot, \n",
    "# coloring the points based on their respective classes.\n",
    "# Dataset Description:\n",
    "# • The Iris dataset consists of 150 samples with 4 features.\n",
    "# • The target variable is categorical with three classes (Setosa, Versicolor, Virginica).\n",
    "# Instructions:\n",
    "# • Import the necessary libraries.\n",
    "# • Load the Iris dataset from sklearn.datasets.\n",
    "# • Separate the features and the target variable.\n",
    "# • Standardize the features (mean = 0, variance = 1).\n",
    "# • Apply PCA to reduce the dimensionality of the dataset to 2 dimensions.\n",
    "# Visualize the transformed data in a 2D scatter plot, using different colors for each class.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['species'] = y\n",
    "# print(pca_df)\n",
    "# Create a DataFrame for the reduced data\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df['species'] = y\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "for species in np.unique(y):\n",
    "    plt.scatter(pca_df[pca_df['species'] == species]['PC1'],\n",
    "                pca_df[pca_df['species'] == species]['PC2'],\n",
    "                label=iris.target_names[species])\n",
    "\n",
    "plt.title('PCA of Iris Dataset (2D)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28\n",
    "# Write a Python program that demonstrates the curse of dimensionality \n",
    "# using the k-Nearest Neighbors (k-NN) classifier with k=5. \n",
    "# The program should create a synthetic dataset with varying dimensions and \n",
    "# evaluate the performance of the k-NN classifier as the number of dimensions increases. Specifically, you should:\n",
    "# • Generate a synthetic dataset with a fixed number of samples (e.g., 1000) and varying dimensions (e.g., 2, 5, 10, 20, 50, 100).\n",
    "# • Use the k-NN classifier to classify the data.\n",
    "# • Measure and print the accuracy of the classifier for each dimensionality.\n",
    "# Visualize the accuracy as a function of dimensionality.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fixed number of samples\n",
    "n_samples = 1000\n",
    "\n",
    "# Varying number of dimensions\n",
    "dimensions = [2, 5, 10, 20, 50, 100]\n",
    "accuracies = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    # Ensure n_informative, n_redundant, and n_repeated are valid\n",
    "    n_informative = max(2, dim // 2)  # At least 2 informative features\n",
    "    n_redundant = max(0, dim - n_informative - 1)  # Adjust redundant features\n",
    "    n_repeated = 0  # No repeated features\n",
    "\n",
    "    # Generate synthetic dataset with specified dimensions\n",
    "    X, y = make_classification(n_samples=n_samples, n_features=dim, \n",
    "                               n_informative=n_informative, n_redundant=n_redundant, \n",
    "                               n_repeated=n_repeated, n_classes=2, random_state=42)\n",
    "    \n",
    "    # Split into training and testing sets (80-20 split)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Apply k-NN classifier with k=5\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Print accuracy for each dimensionality\n",
    "    print(f\"Dimensionality: {dim}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualize the accuracy as a function of dimensionality\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(dimensions, accuracies, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel(\"Number of Dimensions\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Impact of Dimensionality on k-NN Performance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29\n",
    "# You are using a Support Vector Machine (SVM) classifier for a dataset that has 3 features and 500 data points.\n",
    "# If you increase the dimensionality from 3 features to 12 features, explain how the volume of the feature space changes. \n",
    "# What are the potential challenges that arise from this increase in dimensionality?\n",
    "# Instructions:\n",
    "# • Provide detailed answers to both questions, incorporating concepts from machine learning and geometry.\n",
    "# • Use diagrams or examples where appropriate to illustrate your points.\n",
    "# Discuss the implications of high-dimensional spaces on \n",
    "# model performance, including overfitting, computational complexity, and the curse of dimensionality.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define number of samples and dimensions to test\n",
    "num_samples = 500\n",
    "dimensions = [3, 5, 10, 20, 50, 100]  # Different feature space sizes\n",
    "accuracy_scores = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    # Ensure n_informative is at least 2 to avoid ValueError\n",
    "    n_informative = max(2, min(dim - 1, int(np.log2(2 * 2))))\n",
    "\n",
    "    # Generate dataset with proper constraints\n",
    "    X, y = make_classification(n_samples=num_samples, n_features=dim, \n",
    "                               n_informative=n_informative, \n",
    "                               n_redundant=dim - n_informative - 1,  # Some redundant features\n",
    "                               n_clusters_per_class=1,  # Avoid excessive clusters\n",
    "                               n_classes=2, random_state=42)\n",
    "\n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Train an SVM classifier\n",
    "    clf = SVC(kernel='linear', random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and compute accuracy\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(acc)\n",
    "\n",
    "    print(f\"Dim: {dim}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Plot accuracy vs. dimensions\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(dimensions, accuracy_scores, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Number of Dimensions')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Curse of Dimensionality: SVM Accuracy vs. Feature Space Size')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30\n",
    "# Create a synthetic dataset using make_blobs with the following specifications:\n",
    "# • The dataset should have 4 features and 4 centers (clusters).\n",
    "# • Initialize the PCA object with n_components=3 to reduce the dataset to 3 dimensions.\n",
    "# • Use Matplotlib to plot the original 4D dataset in a 3D scatter plot and \n",
    "# the PCA-reduced 3D dataset in another 3D scatter plot side by side.\n",
    "# Instructions:\n",
    "# • Import the necessary libraries (numpy, matplotlib, and sklearn).\n",
    "# • Generate the synthetic dataset using make_blobs.\n",
    "# • Apply PCA to reduce the dataset to 3 dimensions.\n",
    "# • Create a figure with two subplots:\n",
    "# • The first subplot should display the original 3D representation of the dataset \n",
    "# (you can use any three of the four features for this plot).\n",
    "# • The second subplot should display the PCA-reduced 3D dataset.\n",
    "# • Ensure that the points in both plots are color-coded based on their respective cluster labels.\n",
    "# Add appropriate titles and labels to the plots for clarity.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Generate a synthetic dataset with 4 features and 4 clusters\n",
    "n_samples = 500\n",
    "n_features = 4\n",
    "n_clusters = 4\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=42)\n",
    "\n",
    "# Step 2: Apply PCA to reduce the dataset from 4D to 3D\n",
    "pca = PCA(n_components=3)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Step 3: Create a figure with two subplots\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the original 4D dataset (using first 3 features)\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='viridis', edgecolor='k')\n",
    "ax1.set_title(\"Original 4D Dataset (First 3 Features)\")\n",
    "ax1.set_xlabel(\"Feature 1\")\n",
    "ax1.set_ylabel(\"Feature 2\")\n",
    "ax1.set_zlabel(\"Feature 3\")\n",
    "\n",
    "# Plot the PCA-reduced 3D dataset\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap='viridis', edgecolor='k')\n",
    "ax2.set_title(\"PCA-Reduced 3D Dataset\")\n",
    "ax2.set_xlabel(\"Principal Component 1\")\n",
    "ax2.set_ylabel(\"Principal Component 2\")\n",
    "ax2.set_zlabel(\"Principal Component 3\")\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31\n",
    "# Use the Breast Cancer dataset to train a random forest classifier. \n",
    "# The model will be trained to accurately classify tumors as benign or malignant. \n",
    "# The dataset is imbalanced, with a significant majority of benign samples. \n",
    "# Evaluate the model's performance using accuracy, precision, recall, and F1-score. generate complete problem statement\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load the Breast Cancer dataset\n",
    "cancer_data = load_breast_cancer()\n",
    "X = cancer_data.data  # Features\n",
    "y = cancer_data.target  # Target (0 = malignant, 1 = benign)\n",
    "\n",
    "# Convert to DataFrame for better understanding\n",
    "df = pd.DataFrame(X, columns=cancer_data.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Step 2: Split the dataset (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Standardize the features (mean = 0, variance = 1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Train the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Visualizing the Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Malignant', 'Benign'], yticklabels=['Malignant', 'Benign'])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Step 8: Feature Importance Visualization\n",
    "feature_importances = rf_model.feature_importances_\n",
    "feature_names = cancer_data.feature_names\n",
    "sorted_idx = np.argsort(feature_importances)[::-1][:10]  # Top 10 features\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=feature_importances[sorted_idx], y=np.array(feature_names)[sorted_idx], palette=\"viridis\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Top 10 Important Features in Breast Cancer Classification\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32\n",
    "# Use the Wine dataset, to develop a k-nearest neighbors (k-NN) classifier to classify different \n",
    "# types of wine based on their chemical properties. \n",
    "# You will evaluate the model's performance using various metrics, including accuracy, precision, recall, and F1-score.\n",
    "# • Load the Wine dataset from a CSV file\n",
    "# • checking for missing values, understanding the distribution of classes, and visualizing the features.\n",
    "# • Normalize or standardize the features if necessary to improve the performance of the k-NN classifier\n",
    "# • split the dataset into training and testing sets (e.g., 80% training and 20% testing)\n",
    "# • Train the classifier on the training dataset.\n",
    "# • Use the trained model to make predictions on the test dataset.\n",
    "# • Calculate and print the following performance metrics:\n",
    "# o Accuracy\n",
    "# o Precision (for each class)\n",
    "# o Recall (for each class)\n",
    "# o F1-score (for each class)\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Step 1: Load the Wine dataset from CSV\n",
    "file_path = \"wine.csv\"  # Change this if necessary\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Explore the dataset\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['Wine'].value_counts())\n",
    "\n",
    "# Step 3: Separate features and target\n",
    "X = df.drop(columns=['Wine'])  # Assuming 'target' column contains class labels\n",
    "y = df['Wine']\n",
    "\n",
    "# Step 4: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 5: Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 6: Train the k-NN classifier (using k=5 as default)\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Make predictions on the test dataset\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average=None)  # Per class\n",
    "recall = recall_score(y_test, y_pred, average=None)  # Per class\n",
    "f1 = f1_score(y_test, y_pred, average=None)  # Per class\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 9: Visualizing the Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33\n",
    "# Apply Principal Component Analysis (PCA) to the Iris dataset to reduce its dimensionality from 4 to 2 dimensions. \n",
    "# You will visualize the PCA-reduced dataset in a 2D scatter plot using Matplotlib.\n",
    "# instructions:\n",
    "# Load the Iris dataset\n",
    "# Briefly explore the dataset to understand its structure. Print the shape of the dataset and \n",
    "# the first few rows to get an overview of the features and target variable.\n",
    "# Initialize the PCA object to reduce the dataset to 2 dimensions.\n",
    "# Fit the PCA model to the Iris dataset and transform the dataset\n",
    "# Use Matplotlib to create a scatter plot of the PCA-reduced dataset.\n",
    "# Color the points based on their respective species (Setosa, Versicolor, Virginica) to \n",
    "# visualize how well the PCA has separated the different classes.\n",
    "# Add appropriate titles and labels to the axes of the plot.\n",
    "# Include a legend to indicate which colors correspond to which species.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "target_names = iris.target_names  # Class names\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"First 5 rows:\\n\", pd.DataFrame(X, columns=iris.feature_names).head())\n",
    "\n",
    "# Standardize the dataset (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a scatter plot of PCA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i, target in enumerate(np.unique(y)):\n",
    "    plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1], \n",
    "                color=colors[i], label=target_names[i], alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA of Iris Dataset\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34\n",
    "# Apply Linear Discriminant Analysis (LDA) to the Iris dataset to reduce its dimensionality from 4 to 2 dimensions. \n",
    "# You will visualize the LDA-reduced dataset in a 2D scatter plot using Matplotlib.\n",
    "# instructions:\n",
    "# Load the Iris dataset\n",
    "# Briefly explore the dataset to understand its structure. Print the shape of the dataset and \n",
    "# the first few rows to get an overview of the features and target variable.\n",
    "# Initialize the PCA object to reduce the dataset to 2 dimensions.\n",
    "# Fit the LDA model to the Iris dataset and transform the dataset\n",
    "# Use Matplotlib to create a scatter plot of the LDA-reduced dataset.\n",
    "# Color the points based on their respective species (Setosa, Versicolor, Virginica) to \n",
    "# visualize how well the LDA has separated the different classes.\n",
    "# Add appropriate titles and labels to the axes of the plot.\n",
    "# Include a legend to indicate which colors correspond to which species.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target labels\n",
    "target_names = iris.target_names  # Class names\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"First 5 rows:\\n\", pd.DataFrame(X, columns=iris.feature_names).head())\n",
    "\n",
    "# Apply LDA to reduce dimensions to 2\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "\n",
    "# Create a scatter plot of LDA results\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i, target in enumerate(np.unique(y)):\n",
    "    plt.scatter(X_lda[y == target, 0], X_lda[y == target, 1], \n",
    "                color=colors[i], label=target_names[i], alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel(\"LDA Component 1\")\n",
    "plt.ylabel(\"LDA Component 2\")\n",
    "plt.title(\"LDA of Iris Dataset\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35\n",
    "# Apply Linear Discriminant Analysis (LDA) to the Wine dataset to reduce its dimensionality and classify the types of wine based on their chemical properties. \n",
    "# You will visualize the LDA-reduced dataset in a 2D scatter plot and evaluate the classification performance.\n",
    "# Dataset: You will use the Wine dataset, which consists of 178 samples of wine, \n",
    "#     each described by 13 features representing different chemical properties. \n",
    "#     The target variable indicates the type of wine, which can take on one of three classes (1, 2, or 3).\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data  # Features (13 chemical properties)\n",
    "y = wine.target  # Target labels (wine classes: 0, 1, 2)\n",
    "target_names = wine.target_names  # Class names\n",
    "\n",
    "# Explore dataset\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"First 5 rows:\\n\", pd.DataFrame(X, columns=wine.feature_names).head())\n",
    "\n",
    "# Standardize the features (LDA is sensitive to scale)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA to reduce dimensions to 2\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_lda, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Visualizing LDA-reduced data\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i, target in enumerate(np.unique(y)):\n",
    "    plt.scatter(X_lda[y == target, 0], X_lda[y == target, 1], \n",
    "                color=colors[i], label=target_names[i], alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel(\"LDA Component 1\")\n",
    "plt.ylabel(\"LDA Component 2\")\n",
    "plt.title(\"LDA of Wine Dataset\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36\n",
    "# apply Linear Discriminant Analysis (LDA) to the Iris dataset using the Scikit-learn library. \n",
    "# You will preprocess the data using label encoding, perform LDA to reduce the dimensionality of the dataset, \n",
    "# and visualize the results in a 2D scatter plot.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features (4D)\n",
    "y = iris.target  # Target labels (0, 1, 2)\n",
    "target_names = iris.target_names  # Class names\n",
    "\n",
    "# Standardize the features (LDA is affected by scale)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA to reduce dimensionality to 2D\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Visualizing LDA-transformed data in a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "for i, target in enumerate(np.unique(y)):\n",
    "    plt.scatter(X_lda[y == target, 0], X_lda[y == target, 1], \n",
    "                color=colors[i], label=target_names[i], alpha=0.7, edgecolors='k')\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel(\"LDA Component 1\")\n",
    "plt.ylabel(\"LDA Component 2\")\n",
    "plt.title(\"LDA of Iris Dataset\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37\n",
    "# Objective: The goal of this practical exam is to apply Linear Discriminant Analysis (LDA) to \n",
    "#     the Breast Cancer dataset using the Scikit-learn library. \n",
    "#     You will reduce the dimensionality of the dataset from multiple features to 1 dimension and \n",
    "#     visualize the results using Matplotlib.\n",
    "# • load the Breast Cancer dataset\n",
    "# • initialize the LDA to reduce the dataset to 1 dimension.\n",
    "# • Fit the LDA model to the Breast Cancer dataset and transform the dataset to obtain the LDA-reduced representation.\n",
    "# Visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA with 1 component\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_lda = lda.fit_transform(X_scaled,y)\n",
    "\n",
    "# Plot the LDA-reduced dataset\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_lda, np.zeros_like(X_lda), c=y, cmap='coolwarm', alpha=0.7, edgecolors='k')\n",
    "\n",
    "#The x-coordinates come from X_lda (likely a 1D projection of your data, such as after applying LDA).\n",
    "#The y-coordinates are all 0 (effectively placing the points along a horizontal line).\n",
    "#The color of each point corresponds to its class label (y), with the colors chosen according to the coolwarm colormap.\n",
    "#The transparency of the points is set to 70% (alpha=0.7), and the points have black borders.\n",
    "\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.title('LDA Reduction of Breast Cancer Dataset')\n",
    "plt.yticks([])\n",
    "plt.colorbar(label='Class Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38\n",
    "# Dataset: You will use the Iris dataset, which consists of 150 samples of iris flowers, \n",
    "#     each described by 4 features (sepal length, sepal width, petal length, and petal width). \n",
    "#     The target variable indicates the species of the iris flower (Setosa, Versicolor, or Virginica). \n",
    "#     apply both Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) to the Iris dataset. \n",
    "#     You will reduce the dimensionality of the dataset using both techniques and visualize the results in 2D scatter plots. \n",
    "#     You will then compare the effectiveness of LDA and PCA in terms of class separability.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features (4D)\n",
    "y = iris.target  # Target labels (0, 1, 2)\n",
    "target_names = iris.target_names  # Class names\n",
    "colors = ['red', 'blue', 'green']\n",
    "\n",
    "# Standardize the features (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA (4D → 2D)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Apply LDA (4D → 2D)\n",
    "lda = LDA(n_components=2)\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# PCA Plot\n",
    "axes[0].scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color=colors[0], label=target_names[0], alpha=0.7, edgecolors='k')\n",
    "axes[0].scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color=colors[1], label=target_names[1], alpha=0.7, edgecolors='k')\n",
    "axes[0].scatter(X_pca[y == 2, 0], X_pca[y == 2, 1], color=colors[2], label=target_names[2], alpha=0.7, edgecolors='k')\n",
    "axes[0].set_title(\"PCA of Iris Dataset\")\n",
    "axes[0].set_xlabel(\"Principal Component 1\")\n",
    "axes[0].set_ylabel(\"Principal Component 2\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# LDA Plot\n",
    "axes[1].scatter(X_lda[y == 0, 0], X_lda[y == 0, 1], color=colors[0], label=target_names[0], alpha=0.7, edgecolors='k')\n",
    "axes[1].scatter(X_lda[y == 1, 0], X_lda[y == 1, 1], color=colors[1], label=target_names[1], alpha=0.7, edgecolors='k')\n",
    "axes[1].scatter(X_lda[y == 2, 0], X_lda[y == 2, 1], color=colors[2], label=target_names[2], alpha=0.7, edgecolors='k')\n",
    "axes[1].set_title(\"LDA of Iris Dataset\")\n",
    "axes[1].set_xlabel(\"Linear Discriminant 1\")\n",
    "axes[1].set_ylabel(\"Linear Discriminant 2\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 39\n",
    "# develop a linear regression model to predict house prices based on various features. \n",
    "# You will use a dataset that contains information about houses, including features such as the \n",
    "# number of bedrooms, square footage, and location. You will evaluate the model's performance using metrics such as \n",
    "# Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared. \n",
    "# Show performance of same model when PCA reduced data set is used.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('HousePrices.csv')\n",
    "\n",
    "# Drop unnecessary column\n",
    "df = df.drop(columns=['Home'])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['Brick', 'Neighborhood'], drop_first=True)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=['Price'])\n",
    "y = df['Price']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = lr.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Linear Regression without PCA:\")\n",
    "print(f\"MAE: {mae:.2f}, MSE: {mse:.2f}, R²: {r2:.4f}\")\n",
    "\n",
    "# Apply PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Train linear regression with PCA-transformed data\n",
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = lr_pca.predict(X_test_pca)\n",
    "\n",
    "# Evaluate PCA model\n",
    "mae_pca = mean_absolute_error(y_test, y_pred_pca)\n",
    "mse_pca = mean_squared_error(y_test, y_pred_pca)\n",
    "r2_pca = r2_score(y_test, y_pred_pca)\n",
    "\n",
    "print(\"\\nLinear Regression with PCA:\")\n",
    "print(f\"MAE: {mae_pca:.2f}, MSE: {mse_pca:.2f}, R²: {r2_pca:.4f}\")\n",
    "print(f\"PCA reduced features to {pca.n_components_} components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40\n",
    "# apply Principal Component Analysis (PCA) to a house price prediction dataset to reduce its dimensionality and \n",
    "# then use a regression model to predict house prices. You will evaluate the model's \n",
    "# performance using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R²).\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('HousePrices.csv')\n",
    "\n",
    "# Drop unnecessary column\n",
    "df = df.drop(columns=['Home'])\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df = pd.get_dummies(df, columns=['Brick', 'Neighborhood'], drop_first=True)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(columns=['Price'])\n",
    "y = df['Price']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = lr.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(\"Linear Regression with PCA:\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"PCA reduced features to {pca.n_components_} components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41\n",
    "# Consider Social_Netwok_Ads.csv dataset - (UserID, Gender, Age, EstimatedSalary, Purchased). \n",
    "# Use Age and EstimatedSalary as input features and Purchased as target feature. \n",
    "# Split test data set 30% of complete dataset. Build two models of support vector classifier in \n",
    "# python using sklearn library, one for linear and another for RBF kernel with C and gamma parameters set.\n",
    "# Predict test labels and print test accuracy.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"Social_Network_Ads.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select features and target variable\n",
    "X = df[['Age', 'EstimatedSalary']]\n",
    "y = df['Purchased']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA (retain 95% variance)\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train SVM with linear kernel\n",
    "svc_linear = SVC(kernel='linear', C=1.0)\n",
    "svc_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svc_linear.predict(X_test)\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "\n",
    "# Train SVM with RBF kernel\n",
    "svc_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svc_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svc_rbf.predict(X_test)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "# Print results\n",
    "print(\"SVM with Linear Kernel:\")\n",
    "print(f\"Accuracy: {accuracy_linear:.2f}\")\n",
    "\n",
    "print(\"\\nSVM with RBF Kernel:\")\n",
    "print(f\"Accuracy: {accuracy_rbf:.2f}\")\n",
    "\n",
    "print(f\"PCA reduced features to {pca.n_components_} components\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42\n",
    "# Develop a Support Vector Classifier to predict whether a tumor is malignant or benign based on \n",
    "# 30 features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. \n",
    "# The dataset contains 569 samples with binary labels indicating tumor type. (breast_cancer dataset of sklearn) \n",
    "# Load the Dataset from sklearn.Split the dataset into training and testing sets.Train the SVC Model using the training data. \n",
    "# Visualize the Results.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Support Vector Classifier\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Benign\", \"Malignant\"], yticklabels=[\"Benign\", \"Malignant\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43\n",
    "# Write a Python program to implement SVM classification for breast cancerprediction with the following requirements: \n",
    "#     Data preprocessing using StandardScaler use kernel parameters for tuning.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVM model with kernel parameters\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = svc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44\n",
    "# The breast cancer dataset is available in the sklearn.datasets module and can be loaded using load_breast_cancer(). \n",
    "# implement a Naive Bayes Classifier on the Breast Cancer Dataset using python's sklearn library. \n",
    "# Assume that all features of datasets are continuous variables and must be used for building model. \n",
    "# Perform following task. load data set, Split data for training and testing, Build a model and Predict labels of test data.\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45\n",
    "# Implement a Gaussian Naive Bayes classifier to predict whether a patient has diabetes based on various health \n",
    "# metrics of PIMA.csv dataset. The dataset consists of information from 768 female Pima Indians aged 21 and older, \n",
    "# initially gathered by the National Institute of Diabetes and Digestive and Kidney Diseases. \n",
    "# Target variable: Diabetes (binary, 0 or 1) Attributes: Pregnancies, OGTT (Oral Glucose Tolerance Test), \n",
    "# Blood pressure, Skin thickness, Insulin, BMI (Body Mass Index), Age, Pedigree diabetes function.\n",
    "# Load the Dataset from a CSV file. Provide summary statistics for the dataset. \n",
    "# Split the data into training and testing sets (80% train, 20% test). \n",
    "# Instantiate a Gaussian Naive Bayes model and fit it on the training data. \n",
    "# Predict diabetes status for the test set. Discuss any biases in the dataset and how they may affect model performance\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('PIMA.csv') # Ensure PIMA.csv is in your working directory\n",
    "\n",
    "# Display summary statistics\n",
    "print(data.describe())\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=['Outcome'])\n",
    "y = data['Outcome']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 46\n",
    "# Build a Naive Bayes classifier to analyze the sentiment of movie reviews (positive or negative). \n",
    "# The IMDB_Dataset.csv typically contains two main columns: Review: The text of the movie review. Sentiment: \n",
    "# A label indicating the sentiment of the review, usually categorized as 'positive' or 'negative'. \n",
    "# Load the dataset and inspect the structure. Clean the text data by removing punctuation, converting to lowercase, \n",
    "# and tokenizing the reviews.Use CountVectorizer or TfidfVectorizer to convert the text data into a \n",
    "# numerical format suitable for model training. Split the dataset and Create a Multinomial Naive Bayes model and \n",
    "# fit it on the training data. Predict the sentiment of the test reviews.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "\n",
    "# Preprocess text data\n",
    "data['Review'] = data['Review'].str.replace('[^\\w\\s]', '').str.lower()\n",
    "\n",
    "# Convert text to numerical format\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(data['Review'])\n",
    "y = data['Sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and evaluate the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47\n",
    "# Use sklearn.datasets.fetch_20newsgroups() dataset (all sets) to classify newsgroup documents into their \n",
    "# respective categories using a Multinomial Naive Bayes classifier. Load the dataset and explore the categories available. \n",
    "# Preprocess the text data using CountVectorizer to convert text documents into a matrix of token counts. \n",
    "# Split the dataset and Create a Multinomial Naive Bayes model. Fit the model on the training data and print accuracy.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "\n",
    "# Convert text to numerical format\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "# Build and evaluate the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48\n",
    "# Imagine a telecommunications company that wants to predict whether a customer will churn (leave the service) \n",
    "# based on various features such as age, account length, and monthly charges. \n",
    "# The company has historical data on customers, including whether they churned or not.\n",
    "# Age: (in years), Account_Length: (in months), Monthly_Charges: (in dollars)\n",
    "# Churn: Target variable (1 if the customer churned, 0 otherwise)\n",
    "# 'Age': [25, 34, 45, 29, 50, 38, 42, 35, 48, 55],\n",
    "# 'Account_Length': [12, 24, 36, 18, 48, 30, 42, 24, 36, 60],\n",
    "# 'Monthly_Charges': [70, 90, 80, 100, 60, 80, 90, 70, 80, 100],\n",
    "# 'Churn': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "# build a logistic regression model using python that can predict the probability of a customer churning \n",
    "# by spliting 80% of given data for training. Predict targets on trained model of test data. Print accuracy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Data\n",
    "data = {\n",
    "'Age': [25, 34, 45, 29, 50, 38, 42, 35, 48, 55],\n",
    "'Account_Length': [12, 24, 36, 18, 48, 30, 42, 24, 36, 60],\n",
    "'Monthly_Charges': [70, 90, 80, 100, 60, 80, 90, 70, 80, 100],\n",
    "'Churn': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df[['Age', 'Account_Length', 'Monthly_Charges']]\n",
    "y = df['Churn']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49\n",
    "# build a logistic regression model that can predict whether a customer is likely to churn based on the \n",
    "# features in file named customer_churn.csv.\n",
    "# customer_id: Unique customer ID, age: Customer age,\n",
    "# gender: Customer gender (male/female)\n",
    "# account_length: Length of the customer's account (in months)\n",
    "# international_plan: Whether the customer has an international plan (yes/no)\n",
    "# voice_mail_plan: Whether the customer has a voice mail plan (yes/no)\n",
    "# number_vmail_messages: Number of voice mail messages\n",
    "# total_day_calls: Total day calls\n",
    "# total_night_calls: Total night calls\n",
    "# total_intl_calls: Total international calls\n",
    "# • churn: Whether the customer churned (yes/no)\n",
    "# Load data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv(\"customer_churn.csv\")\n",
    "\n",
    "# Preprocess data\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50\n",
    "# Use iris dataset for creating a binary classification problem that predicts whether a flower is of \n",
    "# the species \"Iris-Virginica\" or not. Dataset Features: sepal_length, sepal_width, petal_length, petal_width (in cm) \n",
    "# species: Species of the iris flower (Iris-setosa, Iris-versicolor, Iris-virginica). \n",
    "# Make Predictions and Print Probabilities with Alter Threshold to 0.6.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = (data.target == 2).astype(int)  # Binary classification: 1 if Iris-Virginica, else 0\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Apply threshold 0.6\n",
    "y_pred = (y_prob >= 0.6).astype(int)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "print(\"Predicted Probabilities:\", y_prob)\n",
    "print(\"Predicted Labels with 0.6 Threshold:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51\n",
    "# Consider Salary.csv , with years_of_experience and salary. Write a python code for fitting best fit \n",
    "# simple linear regression with independent variable years_of_experience and dependent variable salary. \n",
    "# Use complete dataset to train model. Plot :fitted model along with trained data points , residual plot.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"Salary.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.dropna()  # Remove rows with NaN values\n",
    "\n",
    "\n",
    "# Extract independent and dependent variables\n",
    "X = df[['Years of Experience']]\n",
    "y = df['Salary']\n",
    "\n",
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot fitted model with trained data points\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(X, y_pred, color='red', label='Fitted Line')\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.title(\"Linear Regression - Salary vs Experience\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y - y_pred\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.residplot(x=X.values.flatten(), y=residuals, lowess=True, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52\n",
    "# Consider a dataset HousePrices.csv with columns square_feet and price.\n",
    "# Write a Python code to fit a polynomial regression model with square_feet as the independent variable and \n",
    "# price as the dependent variable. Use the complete dataset to train the model.\n",
    "# Plot the fitted polynomial model along with the trained data points and create a residual plot.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"HousePrices.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract independent and dependent variables\n",
    "X = df[['SqFt']]\n",
    "y = df['Price']\n",
    "\n",
    "# Transform features to polynomial degree 2\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Train Polynomial Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_poly)\n",
    "\n",
    "# Plot fitted polynomial model with trained data points\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(np.sort(X.values, axis=0), model.predict(poly.transform(np.sort(X.values, axis=0))), color='red', \n",
    "         label='Fitted Polynomial Curve')\n",
    "plt.xlabel(\"Square Feet\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Polynomial Regression - House Prices\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y - y_pred\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.residplot(x=X.values.flatten(), y=residuals, lowess=True, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(\"Square Feet\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 53\n",
    "# Consider a dataset CarPrices.csv with columns age, mileage, and price. \n",
    "# Write a Python code to fit a multiple linear regression model with age and mileage as independent variables \n",
    "# and price as the dependent variable. Use the complete dataset to train the model. \n",
    "# Plot the actual prices against the predicted prices and create a residual plot.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"CarPrices.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract independent and dependent variables\n",
    "X = df[['age', 'mileage']]\n",
    "y = df['price']\n",
    "\n",
    "# Train Multiple Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot actual vs predicted prices\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y, y_pred, color='blue', label='Predicted vs Actual')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], color='red', linestyle='dashed', label='Perfect Fit')\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Actual vs Predicted Prices\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y - y_pred\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(\"Predicted Price\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54\n",
    "# Consider a dataset HousePrices.csv with features such as size (in square feet) and \n",
    "# a target column price (in dollars). Write a Python code to implement Linear Regression using \n",
    "# Gradient Descent to predict price based on size. Use the complete dataset to train the model. \n",
    "# Plot the regression line along with the training data points.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"HousePrices.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract independent and dependent variables\n",
    "X = df[['SqFt']]  # Feature (square feet)\n",
    "y = df['Price']   # Target (price)\n",
    "\n",
    "# Standardize the feature\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train Linear Regression using Gradient Descent\n",
    "model = SGDRegressor(max_iter=1000, learning_rate='optimal', eta0=0.01)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_scaled)\n",
    "\n",
    "# Plot regression line with training data points\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(df['SqFt'], y, color='blue', label='Actual Data')\n",
    "plt.plot(df['SqFt'], y_pred, color='red', label='Regression Line')\n",
    "plt.xlabel(\"Size (Square Feet)\")\n",
    "plt.ylabel(\"Price (Dollars)\")\n",
    "plt.title(\"Linear Regression using SGDRegressor\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 55\n",
    "# Consider a dataset CarPrices.csv with features such as horsepower, age, and a target column price. \n",
    "# Write a Python code to fit a linear regression model to predict price based on the features and \n",
    "# evaluate the model using MSE, R², and MAE.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"CarPrices.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Extract independent and dependent variables\n",
    "X = df[['horsepower', 'months_old']]\n",
    "y = df['price']\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 56\n",
    "# Consider a dataset HouseData.csv with features such as num_rooms, square_feet, and location, and \n",
    "# a target column house_price. Write a Python code to fit a linear regression model to predict\n",
    "# house_price based on the features and evaluate the model using Mean Squared Error (MSE), R², and Mean Absolute Error (MAE).\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"HouseData.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "X = df[['Bedrooms', 'SqFt','Neighborhood']]\n",
    "y = df['Price']\n",
    "\n",
    "X=pd.get_dummies(X,drop_first=True)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 57\n",
    "# Consider a dataset EmployeeData.csv with features such as years_of_experience, education_level, \n",
    "# and a target column salary. Write a Python code to fit a linear regression model to predict salary based on \n",
    "# the features and evaluate the model using Mean Squared Error (MSE), R-squared (R²), and Mean Absolute Error (MAE)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"Salary.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)  # Converts categorical columns\n",
    "\n",
    "# Extract independent and dependent variables\n",
    "X = df_encoded.drop(columns=['Salary'])  # Assuming 'salary' is the target column\n",
    "y = df_encoded['Salary']\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 58\n",
    "# Consider a dataset HousingData.csv with columns num_rooms, area, age, and a target column price. \n",
    "# Write a Python code to fit a Lasso regression model using num_rooms, area, and age as independent \n",
    "# variables to predict price. Use the complete dataset to train the model. \n",
    "# Plot the coefficients of the features and create a residual plot.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"HousingData.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define independent variables (features) and dependent variable (target)\n",
    "X = df[['Bedrooms', 'SqFt', 'age']]\n",
    "y = df['Price']\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Lasso Regression model\n",
    "lasso = Lasso(alpha=0.1)  # You can tune alpha for regularization strength\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature coefficients\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(X.columns, lasso.coef_)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient Values\")\n",
    "plt.title(\"Lasso Regression Coefficients\")\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(\"Predicted Price\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 59\n",
    "# Consider a dataset Diabetes.csv with various medical attributes and a target column diabetes_progression. \n",
    "# Write a Python code to fit a Ridge regression model using all available features to predict diabetes_progression. \n",
    "# Use the complete dataset to train the model. Plot the coefficients of the features and create a residual plot.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"Diabetes.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define independent variables (features) and dependent variable (target)\n",
    "X = df.drop(columns=['DiabetesPedigreeFunction'])  # Assuming 'diabetes_progression' is the target column\n",
    "y = df['DiabetesPedigreeFunction']\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Ridge Regression model\n",
    "ridge = Ridge(alpha=1.0)  # Regularization strength\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = ridge.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature coefficients\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(X.columns, ridge.coef_)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient Values\")\n",
    "plt.title(\"Ridge Regression Coefficients\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(\"Predicted Diabetes Progression\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60\n",
    "# Consider a dataset WineQuality.csv with various chemical properties of wine and a target column quality. \n",
    "# Write a Python code to fit a Lasso regression model using all available features to predict quality. \n",
    "# Use the complete dataset to train the model. Plot the coefficients of the features and create a residual plot.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"WineQuality.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define independent variables (features) and dependent variable (target)\n",
    "X = df.drop(columns=['quality'])  # Assuming 'quality' is the target column\n",
    "y = df['quality']\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Lasso Regression model\n",
    "lasso = Lasso(alpha=0.1)  # Adjust alpha for regularization strength\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature coefficients\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(X.columns, lasso.coef_)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient Values\")\n",
    "plt.title(\"Lasso Regression Coefficients\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(\"Predicted Quality\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 61\n",
    "# Consider a dataset HealthData.csv with features such as age, bmi, blood_pressure, and a target column health_score. \n",
    "# Write a Python code to fit a Lasso regression model using all available features to predict health_score. \n",
    "# Use the complete dataset to train the model. Plot the coefficients of the features and create a residual plot.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"HealthData.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Define independent variables (features) and dependent variable (target)\n",
    "X = df.drop(columns=['health_score'])  # Assuming 'health_score' is the target column\n",
    "y = df['health_score']\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Lasso Regression model\n",
    "lasso = Lasso(alpha=0.1)  # Adjust alpha for regularization strength\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Plot feature coefficients\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(X.columns, lasso.coef_)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Coefficient Values\")\n",
    "plt.title(\"Lasso Regression Coefficients\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, line_kws={\"color\": \"red\"})\n",
    "plt.xlabel(\"Predicted Health Score\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 62\n",
    "# Write a python snippet to demonstrate use of PCA on 100 samples with 3 features generated randomly.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate random data with 100 samples and 3 features\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 3)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 principal components\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Print explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Scatter plot of PCA components\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], color='blue', alpha=0.7)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA Transformation (2D Projection)\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63\n",
    "# Write a python snippet to demonstrate use of PCA on dataset of your choice. \n",
    "# Also print explained variance of all principle components.\n",
    "#Write a python snippet to demonstrate use of PCA on 100 samples with 3 features generated randomly\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random data: 100 samples, 3 features\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 3)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Print explained variance ratio to see how much variance is captured by each principal component\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Visualize the 2D data after PCA\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title(\"2D PCA of Random Data\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64\n",
    "# Create synthetic Dataset using make_blobs with 3 features and 3 centers initialize the PCA \n",
    "# object with n_components=2 to reduce the dataset to 2 dimensions. \n",
    "# use matplotlib to plot the original 3D dataset and the PCA-reduced 2D dataset side by side.\n",
    "#Create synthetic Dataset using make_blobs with 3 features and 3 centers. initialize the PCA object with n_components=2 to reduce the dataset to 2 dimensions.\n",
    "#use matplotlib to plot the original 3D dataset and the PCA-reduced 2D dataset side by side.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a synthetic dataset with 3 features and 3 centers\n",
    "X, y = make_blobs(n_samples=300, centers=3, n_features=3, random_state=42)\n",
    "\n",
    "# Initialize PCA and reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(y)\n",
    "\n",
    "# Set up the plot with two subplots (side by side)\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 3D plot of the original dataset\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='viridis', marker='o')\n",
    "ax1.set_title(\"Original 3D Dataset\")\n",
    "ax1.set_xlabel(\"Feature 1\")\n",
    "ax1.set_ylabel(\"Feature 2\")\n",
    "ax1.set_zlabel(\"Feature 3\")\n",
    "\n",
    "# 2D plot of the PCA-reduced dataset\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "scatter = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', marker='o')\n",
    "ax2.set_title(\"PCA Reduced 2D Dataset\")\n",
    "ax2.set_xlabel(\"Principal Component 1\")\n",
    "ax2.set_ylabel(\"Principal Component 2\")\n",
    "\n",
    "# Add colorbar to the 2D plot\n",
    "fig.colorbar(scatter, ax=ax2, label='Class')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 65\n",
    "# Use load_iris from sklearn.datasets to load the Iris dataset. initialize the PCA object \n",
    "# with n_components=2 to reduce the dataset to 2. use matplotlib to plot the PCA-reduced dataset in 2D.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i,label in enumerate(np.unique(y)):\n",
    "    plt.scatter(X_pca[y==label,0],X_pca[y==label,1],label=iris.target_names[label])\n",
    "\n",
    "plt.title('PCA of Iris Dataset (2D)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66\n",
    "# Use load_wine from sklearn.datasets to load the wine dataset. \n",
    "# initialize the LDA object with n_components=2 to reduce the dataset to 2. \n",
    "# use matplotlib to plot the LDA-reduced dataset in 2D represented by a different color.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data  # Features\n",
    "y = wine.target  # Target labels\n",
    "target_names = wine.target_names\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Plot the LDA-reduced dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, label in enumerate(np.unique(y)):\n",
    "    print(i)\n",
    "    plt.scatter(X_lda[y == label, 0], X_lda[y == label, 1], label=target_names[label], alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"LDA Component 1\")\n",
    "plt.ylabel(\"LDA Component 2\")\n",
    "plt.title(\"LDA Projection of Wine Dataset\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 67\n",
    "# Use breast cancer from sklearn.datasets to load the cancer dataset. \n",
    "# initialize the LDA object with one component to reduce the dataset to 1D. use matplotlib to plot the LDA-reduced dataset.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA with 1 component\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_lda = lda.fit_transform(X_scaled,y)\n",
    "# Plot the LDA-reduced dataset\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_lda, np.zeros_like(X_lda), c=y, cmap='coolwarm', alpha=0.7, edgecolors='k')\n",
    "\n",
    "#The x-coordinates come from X_lda (likely a 1D projection of your data, such as after applying LDA).\n",
    "#The y-coordinates are all 0 (effectively placing the points along a horizontal line).\n",
    "#The color of each point corresponds to its class label (y), with the colors chosen according to the coolwarm colormap.\n",
    "#The transparency of the points is set to 70% (alpha=0.7), and the points have black borders.\n",
    "\n",
    "plt.xlabel('LDA Component 1')\n",
    "plt.title('LDA Reduction of Breast Cancer Dataset')\n",
    "plt.yticks([])\n",
    "plt.colorbar(label='Class Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68\n",
    "# Use load_digits() from sklearn.datasets to load the hand written numbers dataset. \n",
    "# initialize the LDA object with two component to reduce the dataset. Use matplotlib to plot the LDA-reduced dataset.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data  # Features\n",
    "y = digits.target  # Target labels\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X_scaled, y)\n",
    "\n",
    "# Plot the LDA-reduced dataset\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1],c=y, cmap='jet', alpha=0.7)\n",
    "plt.colorbar(scatter, label=\"Digit Class\")\n",
    "plt.xlabel(\"LDA Component 1\")\n",
    "plt.ylabel(\"LDA Component 2\")\n",
    "plt.title(\"LDA Projection of Handwritten Digits Dataset\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 69\n",
    "# Use load_iris from sklearn.datasets to load the Iris dataset. \n",
    "# initialize the LDA object with n_components=2 to reduce the dataset to 2. \n",
    "# use matplotlib to plot the LDA-reduced dataset in 2D.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Standardize the data (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA to reduce dimensions to 2\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X_scaled,y)\n",
    "\n",
    "# print(X_lda)\n",
    "plt.scatter(X_lda[:,0],X_lda[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70\n",
    "# Implement an SVC model to classify iris flowers into three species (Setosa, Versicolor, and Virginica) \n",
    "# based on their sepal and petal dimensions. The dataset contains 150 samples with \n",
    "# four features: sepal length, sepal width, petal length, and petal width. \n",
    "# Load the Dataset from sklearn.Split the dataset into training and testing sets.\n",
    "# Train the SVC Model using the training data. Visualize the Results.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # Target: species (Setosa, Versicolor, Virginica)\n",
    "\n",
    "# Split dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train SVC model\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix using seaborn heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
