# ðŸš€ Huggingface-TogetherAI LangChain Wrapper

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/badge/python-3.11%2B-blue.svg)](https://www.python.org/)

A **LangChain** integration for **DeepSeek-R1** and **Meta Llama-3.3-70B-Instruct-Turbo** models via **Hugging Face's Inference API**, enabling seamless interaction with state-of-the-art language models.

## âœ¨ Features

- ðŸš€ **Custom LangChain Chat Model** â€“ Optimized for Hugging Face + Together AI.
- âš¡ **Sync & Async Support** â€“ Run queries in synchronous or asynchronous mode.
- ðŸŒŠ **Streaming Capabilities** â€“ Supports token streaming for real-time responses.
- ðŸ› ï¸ **Tool Calling & Structured Output** â€“ Enables function calling and JSON outputs.
- ðŸ”§ **Configurable Model Parameters** â€“ Fine-tune temperature, max tokens, etc.

## ðŸ“¦ Installation

```bash
pip install huggingface-togetherai 
```

## ðŸš€ Quick Start

```python
from huggingface_togetherai import ChatHuggingFaceTogetherAI

hf_token = "your_huggingface_token"
hf_llm = ChatHuggingFaceTogetherAI(
    model="deepseek-ai/DeepSeek-R1",
    hf_token=hf_token
)

response = hf_llm.invoke("Hi!")
print(response)
```

## ðŸ¤” Why Use Huggingface-TogetherAI?

In **LangChain**, the `HuggingFaceEndpoint` class is typically used for Hugging Face models:

```python
from langchain_huggingface import HuggingFaceEndpoint
from langchain_huggingface.chat_models import ChatHuggingFace

hf_endpoint = HuggingFaceEndpoint(
    repo_id="deepseek-ai/DeepSeek-R1",
    task="text-generation",
    huggingfacehub_api_token=hf_token
)

langchain_llm = ChatHuggingFace(llm=hf_endpoint)
langchain_llm.invoke("Hello")
```

However, this results in an error:

```
The model deepseek-ai/DeepSeek-R1 is too large to be loaded automatically (688GB > 10GB).
```

### âœ… The Better Alternative: Huggingface-TogetherAI

With **Huggingface-TogetherAI**, you can seamlessly use large models without running into memory issues:

```python
from huggingface_togetherai import ChatHuggingFaceTogetherAI

hf_llm = ChatHuggingFaceTogetherAI(
    model="deepseek-ai/DeepSeek-R1",
    hf_token=hf_token,
    other_params...
)

response = hf_llm.invoke("Hello")
print(response)  # Output: '<think>\n\n</think>\n\nHello! How can I assist you today? ðŸ˜Š'
```

### ðŸŽ‰ Good News!

âœ… You can leverage **all [Langchain](https://www.langchain.com/) functionalities** for standard LLMs with this package.

## ðŸ“œ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for more details.

