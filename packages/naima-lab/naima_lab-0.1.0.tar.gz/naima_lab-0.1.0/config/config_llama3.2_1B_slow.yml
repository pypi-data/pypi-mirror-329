model_name: unsloth/Llama-3.2-1B-Instruct-bnb-4bit # unsloth/Meta-Llama-3.1-8B "unsloth/Llama-3.2-3B-Instruct-bnb-4bit"
chat_template: "llama-3.1" # base or instruct
max_seq_length: 2048
load_in_4bit: true
test_size: 0.2
save_method: adapters # adapters vs full
model_save_model: "model/llama3.2_1B_chunk_selector"

unsloth:
  fast_language_model_config:
    r: 16
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
    lora_alpha: 16
    lora_dropout: 0
    bias: "none"
    use_gradient_checkpointing: "unsloth"
    random_state: 3407
    use_rslora: false
    loftq_config: null
    
trl:
  dataset_text_field: "text"
  dataset_num_proc: 2
  packing: false
  callbacks:
      use_callbaks: true # Add if you want to use early stopping 
      early_stopping_patience: 3
      early_stopping_threshold: 0
  args:
    run_name: "myrun"
    max_steps: 2 # Add null is you want to run for more time
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 4
    warmup_steps: 5
    num_train_epochs: 1
    do_eval: true  # add True to do a evalutation 
    eval_steps: 1
    learning_rate: 0.0002
    logging_steps: 1
    optim: "adamw_8bit"
    weight_decay: 0.01
    lr_scheduler_type: "linear"
    seed: 3407
    logging_first_step: true
    eval_strategy: "steps" # "steps"
    save_strategy: "steps"
    output_dir: "model/llama3.2_1B_chunk_selector/outputs"
    report_to: "none"
    load_best_model_at_end: true

train_on_responses_only:
  instruction_part: "<|start_header_id|>user<|end_header_id|>\n\n"
  response_part: "<|start_header_id|>assistant<|end_header_id|>\n\n"